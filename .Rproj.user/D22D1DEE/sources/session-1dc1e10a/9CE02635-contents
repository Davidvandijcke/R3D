\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}
\usepackage{bbold}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }

\title{Robust uniform inference for quantile treatment effects in regression discontinuity designs \({ }^{\star}\) }

\author{Harold D. Chiang \({ }^{\text {a }}\), Yu-Chin Hsu \({ }^{\text {b,c,d }}\), Yuya Sasaki \({ }^{\text {a,* }}\)\\
\({ }^{\text {a }}\) Department of Economics, Vanderbilt University, VU Station B \#351819, 2301 Vanderbilt Place, Nashville, TN 37235-1819, USA\\
\({ }^{\mathrm{b}}\) Institute of Economics, Academia Sinica, 128 Academia Road, Section 2, Nankang, Taipei, 115, Taiwan\\
\({ }^{\text {c }}\) Department of Finance, National Central University, NO. 300 Jung-da Road, Jung-li City, Taoyuan, 320, Taiwan\\
\({ }^{\text {d }}\) Department of Economics, National Chengchi University, 64, Chih-nan Road, Sec. 2, Wenshan, Taipei, 11623, Taiwan}
\date{}


%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}
\maketitle


\section*{ARTICLE INFO}
\section*{Article history:}
Received 17 April 2018\\
Received in revised form 1 March 2019\\
Accepted 18 March 2019\\
Available online 18 April 2019

\section*{JEL classification:}
C01, C14, C21

\section*{Keywords:}
Bias correction\\
Local Wald estimator\\
Multiplier bootstrap\\
Quantile\\
Regression discontinuity design\\
Regression kink design\\
Robustness

\begin{abstract}
The practical importance of inference with robustness against large bandwidth for causal effects in regression discontinuity and kink designs is widely recognized. Existing robust methods cover many cases, but do not handle uniform inference for CDF and quantile processes in fuzzy designs. In this light, this paper extends the literature by developing a unified framework of inference with robustness against large bandwidths that applies to uniform inference for quantile treatment effects in fuzzy designs, as well as all the other cases. We present Monte Carlo simulation studies and an empirical application for evaluations of the Oklahoma pre-K program.
\end{abstract}

Â© 2019 Elsevier B.V. All rights reserved.

\section*{1. Introduction}
Empirical researchers have used various versions of local Wald estimators. In widest use are the local Wald estimators for the regression discontinuity design (RDD). More recently, researchers have also used local Wald ratios of derivative estimators for the regression kink design (RKD). Furthermore, local Wald ratios of conditional cumulative distribution functions and their variants are used for estimation of quantile treatment effects. In all of these variants of local Wald estimators, researchers often choose large bandwidths by alternative data-driven selectors in practice. As such, ideal estimators and inference procedures need to be robust against large bandwidths.

\footnotetext{ty First arXiv version: February 24, 2017. Code files are available upon request. We would like to thank Bill Gormley for kindly approving our use of his data for our empirical application, Brigham Frandsen for kindly providing us with a working data set, and Fangzhu Yang for excellent research assistance. We benefited from useful comments by Matias Cattaneo, Yingying Dong, Oliver Linton (the co-editor), Patrick Richard, Pedro Sant'Anna, the associate editor, anonymous referees, seminar participants at UC Davis, UC Irvine, and University of Tsukuba, and conference participants at CESG 2018, ESEM 2017, IAAE 2017, IEAS Econometric Workshop: Theories and Applications 2017, LAMES 2017, New York Camp Econometrics XII, SETA 2017, Shanghai Econometrics Workshop 2017, and University of Tokyo Conference in Advances in Econometrics. All remaining errors are ours. Yu-Chin Hsu gratefully acknowledges the research support from Ministry of Science and Technology of Taiwan (MOST107-2410-H-001-034-MY3) and Career Development Award of Academia Sinica, Taiwan. This paper was previously circulated as "A Unified Robust Bootstrap Method for Sharp/Fuzzy Mean/Quantile Regression Discontinuity/Kink Designs.".

\begin{itemize}
  \item Corresponding author.
\end{itemize}

E-mail addresses: \href{mailto:harold.d.chiang@vanderbilt.edu}{harold.d.chiang@vanderbilt.edu} (H.D. Chiang), \href{mailto:ychsu@econ.sinica.edu.tw}{ychsu@econ.sinica.edu.tw} (Y.-C. Hsu), \href{mailto:yuya.sasaki@vanderbilt.edu}{yuya.sasaki@vanderbilt.edu} (Y. Sasaki).
}Proposal of inference methods which are robust against large bandwidths has occurred relatively recently in the literature, given the history of local Wald estimators. The framework of Calonico et al. (2014) covers robust point-wise inference for mean effects in such designs as the sharp mean RDD, fuzzy mean RDD, sharp mean RKD, and fuzzy mean RKD. The robust wild bootstrap method proposed by Bartalotti et al. (2017) covers the sharp mean RDD. The pivotal method proposed by Qu and Yoon (2018) covers robust uniform inference for the sharp quantile RDD, and is also extensible to the sharp quantile RKD (Chiang and Sasaki, 2019).

To our knowledge, these methods proposed in the existing literature do not cover another important case, namely robust uniform inference \({ }^{1}\) for quantile treatment effects in the fuzzy RDD (Frandsen et al., 2012), despite its frequent use in the recent literature in applied economics, e.g., Clark and Martorell (2014) and Deshpande (2016), to list a few. \({ }^{2}\) In this light, this paper proposes a new general robust inference method and construction of bias-corrected uniform confidence bands that cover the fuzzy quantile RDD in particular. Instead of proposing a robust inference method which specifically applies to the fuzzy quantile RDD, however, we propose one generic framework that uniformly applies to most, if not all, versions of the local Wald estimators including the sharp mean RDD, the fuzzy mean RDD, the sharp mean RKD, the fuzzy mean RKD, the sharp quantile RDD, the fuzzy quantile RDD, the sharp quantile RKD, and the fuzzy quantile RKD, to list a few most popular examples used in empirical research. We focus on the case of the fuzzy quantile RDD for most parts of this paper, as the applicability to this particular case is new in the literature.

After assessing the performance of the proposed method through Monte Carlo simulation studies, we apply it to real data and study causal effects on test outcomes of the Oklahoma pre-K program, following the earlier work by Gormley Jr. et al. (2005) and Frandsen et al. (2012). While Frandsen et al. (2012) provide point-wise confidence intervals for the quantile treatment effects in this application, we follow up and complement their earlier analysis by further providing uniform confidence bands with robustness against large data-driven bandwidths.

The rest of this paper is organized as follows. Section 2 discusses the related literature. Section 3 provides an overview of the method. Section 4 presents the main theoretical results. Section 5 demonstrates Monte Carlo simulation studies. Section 6 presents an empirical illustration. Section 7 presents extended results, including cluster robust inference (Section 7.1), inference with robustness against no or weak jumps (Section 7.2), and inference for models with covariates (Section 7.3). Section 8 concludes. All mathematical proofs and additional details are delegated to the appendix.

\section*{2. Relation to the literature}
In this section, we overview the most relevant parts of the existing literature. Because of the breadth of the related literature, what we write below is far from being exhaustive.\\
Literature on Local Designs: The idea of the RDD is introduced by Thistlethwaite and Campbell (1960). There is a vast literature on the RDD. Instead of enumerating all papers, we refer the readers to a seminal paper by Hahn et al. (2001) and a collection of surveys, including Cook (2008) contained in the special issue of Journal of Econometrics edited by Imbens and Lemieux (2008), Imbens and Wooldridge (2009, Section 6.4), Lee and Lemieux (2010), and Volume 38 of Advances in Econometrics edited by Cattaneo and Escanciano (2017), as well as the references cited therein. For technical matters, we mainly refer to Porter (2003) in deriving the general Bahadur representation for higher-order local polynomial mean regression. While it mostly evolved around the RDD, recent additions to this local design literature include the RKD (e.g., Nielsen et al., 2010; Chen and Fan, 2011; Landais, 2015; Simonsen et al., 2016; Card et al., 2015; Dong, 2016), quantile extensions (e.g., Frandsen et al., 2012; Qu and Yoon, 2018), and their combination (Chiang and Sasaki, 2019). While we focus on the fuzzy quantile RDD for most parts of this paper, we note that all these different frameworks are uniformly encompassed by the general framework developed in this paper.\\
Literature on Robust Inference: Calonico et al. (2014) introduce bias correction to achieve the robustness of asymptotic inference against large bandwidths. This innovation paves the way for empirical practitioners to obtain valid standard errors for their estimates under popular data-driven methods of bandwidths. \({ }^{3}\) Bartalotti et al. (2017) adapt this idea of bias correction to a wild bootstrap method of inference for the sharp mean RDD. Qu and Yoon (2018) adapt this idea of bias correction to a simulation method of uniform inference for the sharp quantile RDD.

Our approach is closely related to Calonico et al. (2014), Qu and Yoon (2015, 2018), and Bartalotti et al. (2017). Calonico et al. (2014) analytically develop the asymptotic distribution accounting for effects of bias estimation. We also analytically develop the limit processes for CDF and quantile processes accounting for effects of bias estimation, which can be seen as a uniform extension to the analytic asymptotic distribution of Calonico et al. (2014). Bartalotti et al. (2017) come up with the idea of approximating the asymptotic distribution of Calonico et al. (2014) via wild bootstrap. We propose to

\footnotetext{1 Throughout this paper, we use the phrase "uniform inference" to refer to inference based on weak convergence in the uniform normed linear space \(\ell^{\infty}\).\\
2 We also refer readers to Shigeoka (2014), Ito (2015), Deshpande (2016), and Bernal et al. (2017) for empirical applications with quantile treatment effects in the sharp RDD.\\
\({ }^{3}\) Examples include Imbens and Kalyanaraman (2012), Calonico et al. (2014), Arai and Ichimura (2016), Calonico et al. (2016, 2018a), and Arai and Ichimura (2018). Calonico et al. (2016, 2018a) propose a coverage-probability optimal bandwidth selector and provide a rule of thumb adjustment method to convert MSE-optimal bandwidths into the coverage-probability optimal ones.
}
approximate the limit process by the multiplier bootstrap, analogously to the wild bootstrap approximation of Bartalotti et al. \((2017)\). Qu and Yoon \((2015,2018)\) analytically develop the limit processes for local quantile processes via higherorder local polynomials, which effectively account for effects of bias estimation (Calonico et al., 2014, Remark 7). We also analytically develop the limit process for a more general classes of local Wald estimators, which can be seen as a generalization and an extension to Qu and Yoon \((2015,2018)\). While Qu and Yoon \((2015,2018)\) - Chiang and Sasaki (2019) likewise - propose a simulation method of approximating the limits of conditional quantile processes for sharp designs by exploiting a pivotal property of quantile regression, we propose to approximate the limit processes via the multiplier bootstrap because it applies to fuzzy designs as well where the joint process of the numerator and the denominator in the local Wald ratio are concerned. In summary, our contribution relies on the ideas developed in these three previous papers.\\
Literature on Uniform Bahadur Representation: The Bahadur representation is a key to asymptotic distributional results. To cover uniform inference for local polynomial estimators over a general index set with bias correction of any arbitrary order, uniform validity of Bahadur representation over the set is essential. For classes of nonparametric kernel regressions on which our method relies, Masry (1996), Kong et al. (2010) and Fan and Liu (2016) develop uniform Bahadur representations over regressors. Furthermore, Guerre and Sabbah (2012), Qu and Yoon (2015), Lee et al. (2015), Fan and Guerre (2016) develop uniform validity over quantiles as well. We take advantage of this existing idea. In order to deal with a general class of complexity, we use a new maximal inequality (van der Vaart and Wellner, 2011; Chernozhukov et al., 2014).\\
Literature on Multiplier Bootstrap: In the broad literature, the multiplier bootstrap for Donsker and other weakly convergent classes is first studied by Ledoux and Talagrand (1988) and GinÃ© and Zinn (1990). To our knowledge, use of the multiplier bootstrap in econometrics dates back to Hansen (1996). The multiplier bootstrap for different parametric models has been extensively studied in the literature - it is sometimes referred to as the score bootstrap. For nonparametric CDF estimators, Barrett and Donald (2003) and Donald et al. (2012) use the multiplier bootstrap for uniform inference on unconditional and conditional CDFs, respectively, using the exact solution of their estimators. Chernozhukov et al. (2014) demonstrate the validity of the multiplier bootstrap for inference on suprema of certain non-Donsker processes without using an extreme value limit distribution. Due to the unique nature of our local Wald estimators, our results are based on a multiplier central limit theorem developed more lately by Kosorok (2003, 2008), along with our uniform Bahadur representation.

\section*{3. An overview}
In this section, we present an overview of the main result, focusing on the case of the fuzzy quantile RDD, which has not been covered by the existing literature of robust inference yet despite its use in the recent literature on empirical microeconomics, such as Clark and Martorell (2014) and Deshpande (2016). A formal and general treatment will follow in Section 4.

Suppose that we observe a random sample of \(\left(Y^{*}, D^{*}, X\right)\), where \(X\) is the running variable or the forcing variable, \(D^{*}\) is the binary treatment indicator, and \(Y^{*}\) is the outcome of interest. A researcher faces a fuzzy regression discontinuity design where the cutoff location is normalized to \(X=0\) without loss of generality. Frandsen et al. (2012) identify the conditional CDF of the potential outcome \(Y_{i}^{d}\) under each treatment status \(d \in\{0,1\}\) given the event \(C\) of compliance locally at \(X=0\) by

\[
F_{Y^{d} \mid C}(y)=\frac{\lim _{x \downarrow 0} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]-\lim _{x \uparrow 0} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]}{\lim _{x \downarrow 0} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]-\lim _{x \uparrow 0} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]},
\]

where we omit the conditioning argument \(X=0\) from our notation and thus \(F_{Y^{d} \mid C}\) succinctly denotes \(F_{Y^{d} \mid C, X=0}\). Consequently, the local \(\theta\)-th quantile treatment effect is identified by

\[
\tau(\theta)=: Q_{Y^{1} \mid C}(\theta)-Q_{Y^{0} \mid C}(\theta) ; \quad \text { where } Q_{Y^{d} \mid C}(\theta):=\inf \left\{y: F_{Y^{d} \mid C}(y) \geq \theta\right\}
\]

where we again note that the conditioning argument \(X=0\) is omitted from our notations. Frandsen et al. (2012) develop methods of inference for \(\tau\) based on the exact solutions of local linear estimation of the components of the local Wald ratio (3.1).

In order to make an inference with the local linear estimation, one would need to choose an under-smoothing bandwidth parameter \(h_{n}\). However, commonly available procedures choose rather large bandwidths, e.g., \(h_{n} \propto n^{-1 / 5}\). To accommodate these common procedures in the framework of Frandsen et al. (2012), we need to estimate higher-order bias and to develop the limit process accounting for this bias estimation, as in Calonico et al. (2014). In this section, we present how to make uniform inference for \(\tau\) specifically based on local quadratic estimation of the components of the local Wald ratio (3.1), effectively accounting for the second-order bias estimation - see Remark 7 of Calonico et al. (2014). Consequently, the uniform inference turns robust against large bandwidths as in the commonly available procedures, e.g., \(h_{n} \propto n^{-1 / 5}\).

The right-hand limit, \(\lim _{x \downarrow 0} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\), in the numerator of the local Wald ratio (3.1) can be estimated by \(\hat{\mu}_{1}\left(0^{+}, y, d\right)\) in the local quadratic estimator

\[
\begin{aligned}
& \left(\hat{\mu}_{1}\left(0^{+}, y, d\right), \hat{\mu}_{1}^{\prime}\left(0^{+}, y, d\right), \hat{\mu}_{1}^{\prime \prime}\left(0^{+}, y, d\right)\right)= \\
& \arg \min _{\left(\mu, \mu^{\prime}, \mu^{\prime \prime}\right)} \sum_{i: X_{i}>0}\left(\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\}-\left\{\mu+\mu^{\prime} X_{i}+\frac{\mu^{\prime \prime}}{2!} X_{i}^{2}\right\}\right)^{2} \cdot K\left(\frac{X_{i}}{h_{n}}\right),
\end{aligned}
\]

where \(K\) is a kernel function and \(h_{n}\) is a bandwidth parameter. The left-hand limit, \(\lim _{x \uparrow 0} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\), in the numerator of the local Wald ratio (3.1) can be similarly estimated by \(\hat{\mu}_{1}\left(0^{-}, y, d\right)\) using the observations \(\left\{i: X_{i}<0\right\}\). Likewise, the right-hand limit, \(\lim _{x \downarrow 0} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\), in the denominator of (3.1) can be estimated by \(\hat{\mu}_{2}\left(0^{+}, d\right)\) in the local quadratic estimator

\[
\begin{aligned}
& \left(\hat{\mu}_{2}\left(0^{+}, d\right), \hat{\mu}_{2}^{\prime}\left(0^{+}, d\right), \hat{\mu}_{2}^{\prime \prime}\left(0^{+}, d\right)\right)= \\
& \arg \min _{\left(\mu, \mu^{\prime}, \mu^{\prime \prime}\right)} \sum_{i: X_{i}>0}\left(\mathbb{1}\left\{D_{i}^{*}=d\right\}-\left\{\mu+\mu^{\prime} X_{i}+\frac{\mu^{\prime \prime}}{2!} X_{i}^{2}\right\}\right)^{2} \cdot K\left(\frac{X_{i}}{h_{n}}\right),
\end{aligned}
\]

where we can use the same bandwidth \(h_{n}\) as above for simplicity here. The left-hand limit, \(\lim _{\chi \uparrow 0} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\), in the denominator of the local Wald ratio (3.1) can be similarly estimated by \(\hat{\mu}_{2}\left(0^{-}, d\right)\) using the observations \(\left\{i: X_{i}<0\right\}\). With these component estimates, the estimand (3.2) for the identified local quantile treatment effect may be estimated by

\[
\hat{\tau}(\theta)=\hat{Q}_{Y^{1} \mid C}(\theta)-\hat{Q}_{Y^{0} \mid C}(\theta)
\]

where

\[
\hat{Q}_{Y^{d} \mid C}(\theta)=\inf \left\{y: \frac{\hat{\mu}_{1}\left(0^{+}, y, d\right)-\hat{\mu}_{1}\left(0^{-}, y, d\right)}{\hat{\mu}_{2}\left(0^{+}, d\right)-\hat{\mu}_{2}\left(0^{-}, d\right)} \geq \theta\right\} \quad \text { for each } d \in\{0,1\}
\]

Under suitable conditions, there exists a zero mean Gaussian process \(\mathbb{G}^{\prime}\) such that

\[
\sqrt{n h_{n}}[\hat{\tau}(\cdot)-\tau(\cdot)] \rightsquigarrow \mathbb{G}^{\prime}(\cdot) \quad \text { as } n \rightarrow \infty
\]

See Corollary 1 (i) ahead for formal and general arguments. This result establishes the asymptotic distribution result for the fuzzy quantile RDD process with robustness against large bandwidth, e.g., \(h_{n} \propto n^{-1 / 5}\). In practice, it will be somewhat easier to approximate the limit process \(\mathbb{G}^{\prime}\) by the multiplier bootstrap procedure outlined below.

Let \(\left\{\xi_{i}\right\}_{i=1}^{n}\) be a random sample drawn from the standard normal distribution independently from the data \(\left\{\left(Y_{i}^{*}, D_{i}^{*}, X_{i}\right)\right\}_{i=1}^{n}\). Letting \(\Gamma_{2}^{ \pm}=\int_{\mathbb{R}_{ \pm}}\left(1 u u^{2}\right)^{\prime} \cdot K(u) \cdot\left(1 u u^{2}\right) d u\) be a \(3 \times 3\) matrix, we define the estimated multiplier processes for \(\hat{\mu}_{1}\left(0^{+}, y, d_{1}\right)\) and \(\hat{\mu}_{2}\left(0^{+}, d_{2}\right)\) by

\[
\begin{aligned}
& \hat{v}_{\xi, n}^{+}(y, d, 1)=\sum_{i: X_{i}>0} \xi_{i} \frac{(100) \cdot\left(\Gamma_{2}^{ \pm}\right)^{-1} \cdot\left(1 \frac{X_{i}}{h_{n}} \frac{X_{i}^{2}}{h_{n}^{2}}\right)^{\prime}\left[\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d\right\}-\tilde{\mu}_{1}\left(X_{i}, y, d\right)\right] K\left(\frac{X_{i}}{h_{n}}\right)}{\sqrt{n h_{n}} \hat{f}_{X}(0)} \\
& \text { and } \quad \hat{v}_{\xi, n}^{+}(d, 2)=\sum_{i: X_{i}>0} \xi_{i} \frac{(100) \cdot\left(\Gamma_{2}^{ \pm}\right)^{-1} \cdot\left(1 \frac{X_{i}}{h_{n}} \frac{x_{i}^{2}}{h_{n}^{2}}\right)^{\prime}\left[\mathbb{1}\left\{D_{i}^{*}=d\right\}-\tilde{\mu}_{2}\left(X_{i}, d\right)\right] K\left(\frac{X_{i}}{h_{n}}\right)}{\sqrt{n h_{n}} \hat{f}_{X}(0)}
\end{aligned}
\]

respectively, where \(\hat{f}_{X}(0)\) estimates \(f_{X}(0), \tilde{\mu}_{1}\left(X_{i}, y, d\right)\) estimates \(E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}\right]\), and \(\tilde{\mu}_{2}\left(X_{i}, d\right)\) estimates \(E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}\right]\). Specifically, one can choose any consistent kernel density estimator for \(\hat{f}_{X}(0)\), and concrete examples of the estimators, \(\tilde{\mu}_{1}\left(X_{i}, y, d\right)\) and \(\tilde{\mu}_{2}\left(X_{i}, d\right)\), are provided in Appendix A.6. The estimated multiplier processes for \(\hat{\mu}_{1}\left(0^{-}, y, d_{1}\right)\) and \(\hat{\mu}_{2}\left(0^{-}, d_{2}\right)\) are similarly defined by \(\hat{\nu}_{\xi, n}^{-}(y, d, 1)\) and \(\hat{\nu}_{\xi, n}^{-}(d, 2)\) using the observations \(\left\{i: X_{i}<0\right\}\). By the Hadamard derivative, we construct the approximate estimated multiplier process

\[
\begin{aligned}
& \widehat{\mathbb{G}}_{n}^{\prime}(\theta)=\frac{\hat{\mu}_{2}\left(0^{+}, 1\right)-\hat{\mu}_{2}\left(0^{-}, 1\right)}{\hat{f}_{Y^{1} \mid C}\left(\hat{Q}_{Y^{1} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 1\right)-\hat{\mu}_{2}\left(0^{-}, 1\right)\right]^{2}} \cdot\left\{\hat{\nu}_{\xi, n}^{+}\left(\hat{Q}_{Y^{1} \mid C}(\theta), 1,1\right)-\hat{\nu}_{\xi, n}^{-}\left(\hat{Q}_{Y^{1} \mid C}(\theta), 1,1\right)\right\} \\
& -\frac{\hat{\mu}_{1}\left(0^{+}, \hat{Q}_{Y^{1} \mid}(\theta), 1\right)-\hat{\mu}_{1}\left(0^{-}, \hat{Q}_{Y^{1} \mid C}(\theta), 1\right)}{\hat{f}_{Y^{1} \mid C}\left(\hat{Q}_{Y^{1} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 1\right)-\hat{\mu}_{2}\left(0^{-}, 1\right)\right]^{2}} \cdot\left\{\hat{\nu}_{\xi, n}^{+}(1,2)-\hat{\nu}_{\xi, n}^{-}(1,2)\right\} \\
& \left.-\frac{\hat{\mu}_{2}\left(0^{+}, 0\right)-\hat{\mu}_{2}\left(0^{-}, 0\right)}{\hat{f}_{Y^{0} \mid C}\left(\hat{Q}_{\mathrm{Y}^{0} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 0\right)-\hat{\mu}_{2}\left(0^{-}, 0\right)\right]^{2}} \cdot\left\{\hat{\nu}_{\xi, n}^{+}{ }_{n}\left(\hat{Q}_{Y^{0} \mid C}(\theta), 0,1\right)-\hat{\nu}_{\xi, n}^{-}{ }^{( } \hat{Q}_{\mathrm{Y}^{0} \mid C}(\theta), 0,1\right)\right\} \\
& +\frac{\hat{\mu}_{1}\left(0^{+}, \hat{Q}_{Y^{0} \mid C}(\theta), 0\right)-\hat{\mu}_{1}\left(0^{-}, \hat{Q}_{Y^{0} \mid C}(\theta), 0\right)}{\hat{f}_{Y_{0} \mid C}\left(\hat{Q}_{Y{ }^{0} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 0\right)-\hat{\mu}_{2}\left(0^{-}, 0\right)\right]^{2}} \cdot\left\{\hat{v}_{\xi, n}^{+}(0,2)-\hat{v}_{\xi, n}^{-}(0,2)\right\} \text {. }
\end{aligned}
\]

Under suitable conditions, with probability approaching one, this process \(\widehat{\mathbb{G}}_{n}^{\prime}\) weakly converges to the limit process, \(\mathbb{G}^{\prime}\), of interest conditionally on the data \(\left\{\left(Y_{i}^{*}, D_{i}^{*}, X_{i}\right)\right\}_{i=1}^{n}\), i.e.,

\[
\widehat{\mathbb{G}}_{n}^{\prime} \underset{\xi}{p} \mathbb{G}^{\prime} \text { as } n \rightarrow \infty .
\]

See Corollary 1 (ii) ahead for formal and general arguments. From (3.3) and (3.5), therefore, we may use \(\widehat{\mathbb{G}}_{n}^{\prime}\) to approximate the limit process of \(\sqrt{n h_{n}}[\hat{\tau}-\tau]\).

One of the most relevant practical applications of this result is to test the null hypothesis of uniform treatment nullity:

\[
H_{0}: \tau(\theta)=0 \quad \text { for all } \theta \in[a, 1-a]
\]

for some \(\alpha \in(0,1 / 2)\). To test this hypothesis, we can use \(\sup _{\theta \in[a, 1-a]} \sqrt{n h_{n}}|\hat{\tau}(\theta)|\) as the test statistic, and use

\[
\sup _{\theta \in[a, 1-a]}\left|\widehat{\mathbb{G}}_{n}^{\prime}(\theta)\right|
\]

to simulate its asymptotic distribution.\\
Another of the most relevant practical applications of the above corollary is to test the null hypothesis of treatment homogeneity across quantiles:

\[
H_{0}: \tau(\theta)=\tau\left(\theta^{\prime}\right) \text { for all } \theta, \theta^{\prime} \in[a, 1-a]
\]

To test this hypothesis, we can use \(\sup _{\theta \in[a, 1-a]} \sqrt{n h_{n}}\left|\hat{\tau}(\theta)-(1-2 a)^{-1} \int_{[a, 1-a]} \hat{\tau}(\vartheta) d \vartheta\right|\) as the test statistic, and use

\[
\sup _{\theta \in[a, 1-a]}\left|\widehat{\mathbb{G}}_{n}^{\prime}(\theta)-\frac{1}{1-2 a} \int_{[a, 1-a]} \widehat{\mathbb{G}}_{n}^{\prime}(\vartheta) d \vartheta\right|
\]

to simulate its asymptotic distribution.\\
Finally, we can use the approximate estimated multiplier process to construct uniform confidence bands for the quantile treatment effects. To this end, we compute

\[
\hat{\mathcal{C}}_{n}(a, 1-a ; \lambda)=\text { the }(1-\lambda) \text {-th quantile of } \sup _{\theta \in[a, 1-a]}\left|\widehat{\mathbb{G}}_{n}^{\prime}(\theta)\right| .
\]

The band of the form

\[
\left[\hat{\tau}(\theta) \pm \frac{1}{\sqrt{n h_{n}}} \hat{\mathcal{C}}_{n}(a, 1-a ; \lambda): \theta \in[a, 1-a]\right]
\]

constitutes a \(100(1-\lambda)\) percent uniform confidence band for the quantile treatment effects \(\tau\). In summary, we provide a step-by-step procedure below.

\section*{Algorithm 1 (Practical Guideline on Constructing Uniform Confidence Bands).}
Step 1. Pick a finite set \(\mathcal{Y}^{*} \subset \mathcal{Y}\) of grid points of outcome values and a finite set \(\mathcal{T}^{*} \subset[a, 1-a]\) of grid points of quantiles.\\
Estimate \(\hat{\mu}_{1}\left(0^{ \pm}, y, d\right)\) and \(\hat{\mu}_{2}\left(0^{ \pm}, d\right)\) for all \(y \in \mathcal{Y}^{*}, d \in\{0,1\}\).\\
Step 2. Calculate \(\hat{Q}_{Y^{d} \mid C}(\theta)\) for each \(\theta \in \mathcal{T}^{*}\) by

\[
\hat{Q}_{Y^{d} \mid C}(\theta)=\inf \left\{y \in \mathcal{Y}^{*}: \frac{\hat{\mu}_{1}\left(0^{+}, y, d\right)-\hat{\mu}_{1}\left(0^{-}, y, d\right)}{\hat{\mu}_{2}\left(0^{+}, d\right)-\hat{\mu}_{2}\left(0^{-}, d\right)} \geq \theta\right\}
\]

for \(d \in\{0,1\}\), and then compute \(\hat{\tau}=\hat{Q}_{Y^{1} \mid C}(\theta)-\hat{Q}_{Y^{1} \mid C}(\theta)\) for each \(\theta \in \mathcal{T}^{*}\).\\
Step 3. Compute \(\hat{f}_{X}(0)\) and \(\hat{f}_{Y^{d} \mid C}\left(\hat{Q}_{Y^{d} \mid C}(\theta)\right)\) - see Appendix A. 6 for formulas.\\
Step 4. For each bootstrap iteration \(b=1, \ldots, B\), generate independent standard normal \(\xi^{b}=\left\{\xi_{i}^{b}\right\}_{i=1}^{n}\) independently from data, and compute \(\hat{\nu}_{\xi^{b}, n}(\theta, d, 1)\) and \(\hat{\nu}_{\xi^{b}, n}(d, 2)\) for each \(\theta \in \mathcal{T}^{*}, d \in\{0,1\}\). (Note in each iteration \(b\), we compute these values for different \(\theta\) and \(d\) using the same \(\xi^{b}\).)\\
Step 5. Construct \(\hat{\mathbb{G}}_{n, b}^{\prime}\) for each \(\theta \in \mathcal{T}^{*}\) :

\[
\begin{aligned}
\widehat{\mathbb{G}}_{n, b}^{\prime}(\theta) & =\frac{\hat{\mu}_{2}\left(0^{+}, 1\right)-\hat{\mu}_{2}\left(0^{-}, 1\right)}{\hat{f}_{Y^{1} \mid C}\left(\hat{Q}_{Y^{1} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 1\right)-\hat{\mu}_{2}\left(0^{-}, 1\right)\right]^{2}} \cdot\left\{\hat{v}_{\xi^{b}, n}^{+}\left(\hat{Q}_{Y^{1} \mid C}(\theta), 1,1\right)-\hat{v}_{\xi^{b}, n}^{-}\left(\hat{Q}_{Y^{1} \mid C}(\theta), 1,1\right)\right\} \\
& -\frac{\hat{\mu}_{1}\left(0^{+}, \hat{Q}_{Y^{1} \mid C}(\theta), 1\right)-\hat{\mu}_{1}\left(0^{-}, \hat{Q}_{Y^{1} \mid C}(\theta), 1\right)}{\hat{f}_{Y^{1} \mid C}\left(\hat{Q}_{Y^{1} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 1\right)-\hat{\mu}_{2}\left(0^{-}, 1\right)\right]^{2}} \cdot\left\{\hat{v}_{\xi^{b}, n}^{+}(1,2)-\hat{v}_{\xi^{b}, n}^{-}(1,2)\right\}
\end{aligned}
\]

\[
\begin{aligned}
& -\frac{\hat{\mu}_{2}\left(0^{+}, 0\right)-\hat{\mu}_{2}\left(0^{-}, 0\right)}{\hat{f}_{Y^{0} \mid C}\left(\hat{Q}_{Y^{0} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 0\right)-\hat{\mu}_{2}\left(0^{-}, 0\right)\right]^{2}} \cdot\left\{\hat{v}_{\xi^{b}, n}^{+}\left(\hat{Q}_{Y^{0} \mid C}(\theta), 0,1\right)-\hat{v}_{\xi^{b}, n}^{-}\left(\hat{Q}_{Y^{0} \mid C}(\theta), 0,1\right)\right\} \\
& +\frac{\hat{\mu}_{1}\left(0^{+}, \hat{Q}_{Y^{0} \mid C}(\theta), 0\right)-\hat{\mu}_{1}\left(0^{-}, \hat{Q}_{Y^{0} \mid C}(\theta), 0\right)}{\hat{f}_{Y^{0} \mid C}\left(\hat{Q}_{Y^{0} \mid C}(\theta)\right)\left[\hat{\mu}_{2}\left(0^{+}, 0\right)-\hat{\mu}_{2}\left(0^{-}, 0\right)\right]^{2}} \cdot\left\{\hat{v}_{\xi^{b}, n}^{+}(0,2)-\hat{v}_{\xi^{b}, n}^{-}(0,2)\right\} .
\end{aligned}
\]

Step 6. Set \(\hat{\mathcal{C}}_{n}^{B}(a, 1-a ; \lambda)=\) the \((1-\lambda)\)-th quantile of \(\left\{\max _{\theta \in \mathcal{T}^{*}}\left|\widehat{\mathbb{G}}_{n, b}^{\prime}(\theta)\right|\right\}_{b=1}^{B}\), and construct an asymptotically valid \(100(1-\lambda)\) percent uniform confidence band over \([a, 1-a]\) by

\[
\left[\hat{\tau}(\theta) \pm \frac{1}{\sqrt{n h_{n}}} \hat{\mathcal{C}}_{n}^{B}(a, 1-a ; \lambda): \theta \in \mathcal{T}^{*}\right]
\]

Remark 1. Steps \(1-5\) also give the bootstrapped distribution \(\left\{\hat{\mathbb{G}}_{n, b}^{\prime}\right\}_{b=1}^{B}\), which can be used to construct critical values for tests of uniform treatment nullity and treatment homogeneity as well. For the null hypothesis of uniform treatment nullity, use \(\max _{\theta \in[a, 1-a]} \sqrt{n h_{n}}|\hat{\tau}(\theta)|\) as the test statistic, and use \((1-\lambda)\)-th quantile of \(\left\{\max _{\theta \in[a, 1-a]}\left|\widehat{\mathbb{G}}_{n, b}^{\prime}(\theta)\right|\right\}_{b=1}^{B}\) as the critical value. For the null hypothesis of treatment homogeneity, let \(\max _{\theta \in[a, 1-a]} \sqrt{n h_{n}}\left|\hat{\tau}(\theta)-(1-2 a)^{-1} \int_{[a, 1-a]} \hat{\tau}(\vartheta) d \vartheta\right|\) be the test statistic, and the \((1-\lambda)\)-th quantile of \(\left\{\max _{\theta \in[a, 1-a]}\left|\widehat{\mathbb{G}}_{n, b}^{\prime}(\theta)-\frac{1}{1-2 a} \int_{[a, 1-a]} \widehat{\mathbb{G}}_{n, b}^{\prime}(\vartheta) d \vartheta\right|\right\}_{b=1}^{B}\) be the critical value for the test.

Remark 2. In practice we may set \(\left|\mathcal{T}^{*}\right|=50\) and \(\left|\mathcal{Y}^{*}\right|=5000\). We tried a local polynomial mean regression for each of the 5000 grid points of \(y \in \mathcal{Y}^{*}\), and found that it is not computationally burdensome in general, since local polynomial mean regressions are smooth convex problems and gradient decent algorithms can solve them efficiently.

\section*{4. A unified framework}
In this section, we present a generalized framework for a broad class of local Wald estimands that encompasses not only the case of the fuzzy quantile RDD, but also the cases of the sharp mean RDD, the fuzzy mean RDD, the sharp mean RKD, the fuzzy mean RKD, the sharp CDF discontinuity design, the fuzzy CDF discontinuity design, the sharp quantile RDD, the sharp quantile RKD, and the fuzzy quantile RKD. We revisit the case of the fuzzy quantile RDD in Section 4.5 to provide a formal justification of the overview in Section 3. All the other examples are relegated to Appendices B and C.

\subsection*{4.1. The general framework}
Let \((Y, D, X)\) be a random vector defined on a probability space \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\), where \(Y\) is a random vector containing an outcome and possibly other variables, \(D\) is a random vector containing a treatment indicator and possibly others, and \(X\) is a running variable or an assignment variable. We denote their supports as \(\mathscr{Y}, \mathscr{D}\) and \(\mathscr{X}\), respectively. Suppose that a researcher observes \(n\) i.i.d. copies \(\left\{\left(Y_{i}, D_{i}, X_{i}\right)\right\}_{i=1}^{n}\) of \((Y, D, X)\). Consider some subsets of some finite dimensional Euclidean spaces \(\Theta_{1}, \Theta_{2}, \Theta_{1}^{\prime}\), \(\Theta_{2}^{\prime}\), and \(\Theta^{\prime \prime}\). We will use them to denote sets of indices. Let \(\Theta=\Theta_{1} \times \Theta_{2}\), and let \(g_{1}: \mathscr{Y} \times \Theta_{1} \rightarrow \mathbb{R}\) and \(g_{2}: \mathscr{D} \times \Theta_{2} \rightarrow \mathbb{R}\) be functions to be defined in various contexts of empirical research designs - concrete examples are suggested in the subsections below and in the Supplementary Appendix. When we discuss the continuity of \(g_{k}\) in \(\theta_{k}\), \(k \in\{1,2\}\), we consider \(\Theta_{1}\) and \(\Theta_{2}\) with the topologies they inherit from the finite dimensional Euclidean spaces they reside in. We write \(\mu_{1}\left(x, \theta_{1}\right)=E\left[g_{1}\left(Y_{i}, \theta_{1}\right) \mid X_{i}=x\right]\) and \(\mu_{2}\left(x, \theta_{2}\right)=E\left[g_{2}\left(D_{i}, \theta_{2}\right) \mid X_{i}=x\right]\). Their \(v\) th order partial derivatives with respect to \(x\) are denoted by \(\mu_{1}^{(v)}=\frac{\partial^{v}}{\partial x^{v}} \mu_{1}\) and \(\mu_{2}^{(v)}=\frac{\partial^{v}}{\partial x^{v}} \mu_{2}\). For a set \(T\), we denote \(\mathcal{C}^{1}(T)\) as the collection of all real-valued functions on \(T\) that are continuously differentiable, and \(\ell^{\infty}(T)\) is the collection of all bounded real-valued functions on \(T\). With suitable operators \(\phi: \ell^{\infty}\left(\Theta_{1}\right) \rightarrow \ell^{\infty}\left(\Theta_{1}^{\prime}\right), \psi: \ell^{\infty}\left(\Theta_{2}\right) \rightarrow \ell^{\infty}\left(\Theta_{2}^{\prime}\right)\), and \(\Upsilon: \ell^{\infty}\left(\Theta_{1}^{\prime} \times \Theta_{2}^{\prime}\right) \rightarrow \ell^{\infty}\left(\Theta^{\prime \prime}\right)\), a general class of local Wald estimands can be expressed in the form of

\[
\tau\left(\theta^{\prime \prime}\right)=\Upsilon\left(\frac{\phi\left(\lim _{x \downarrow 0} \mu_{1}^{(v)}(x, \cdot)\right)(\cdot)-\phi\left(\lim _{x \uparrow 0} \mu_{1}^{(v)}(x, \cdot)\right)(\cdot)}{\psi\left(\lim _{x \downarrow 0} \mu_{2}^{(v)}(x, \cdot)\right)(\cdot)-\psi\left(\lim _{x \uparrow 0} \mu_{2}^{(v)}(x, \cdot)\right)(\cdot)}\right)\left(\theta^{\prime \prime}\right) .
\]

for all \(\theta^{\prime \prime} \in \Theta^{\prime \prime}\). This class of local Wald estimands encompasses a wide array of design-based estimands used by empirical practitioners. In all examples, \(v\) is either 0 (for RDD) or 1 (for RKD), and setting the order of local polynomial estimator \(p=v+2\) would generally suffice in practice. We list two examples below: Example 1 illustrates the case of the fuzzy mean RDD; Example 2 illustrates the case of the fuzzy quantile RDD for which the existing literature has proposed no robust uniform inference methods - also overviewed in Section 3. See Appendix C for additional examples. For convenience of writing, we introduce the notation for the intermediate local Wald estimand:

\[
W=\frac{\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)}{\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)}
\]

Example 1 (Fuzzy Mean RDD). We do not need index sets for fuzzy mean RDD, so let \(\Theta_{1}=\Theta_{2}=\Theta_{1}^{\prime}=\Theta_{2}^{\prime}=\Theta^{\prime \prime}=\{0\}\) for simplicity. Set \(g_{1}\left(Y_{i}, \theta_{1}\right)=Y_{i}\) and \(g_{2}\left(D_{i}, \theta_{2}\right)=D_{i}\). Note that \(\mu_{1}\left(x, \theta_{1}\right)=\mathrm{E}\left[g_{1}\left(Y_{i}, \theta_{1}\right) \mid X_{i}=x\right]=\mathrm{E}\left[Y_{i} \mid X_{i}=x\right]\) and \(\mu_{2}\left(x, \theta_{2}\right)=\mathrm{E}\left[g_{2}\left(D_{i}, \theta_{2}\right) \mid X_{i}=x\right]=\mathrm{E}\left[D_{i} \mid X_{i}=x\right]\). Let \(\phi\) and \(\psi\) be the identity operators, and for \(W \in \ell^{\infty}\left(\Theta_{1}^{\prime} \times \Theta_{2}^{\prime}\right)\) let the operator \(\Upsilon\) be \(\Upsilon(W)\left(\theta^{\prime \prime}\right)=W\left(\theta^{\prime \prime}, \theta^{\prime \prime}\right) \forall \theta^{\prime \prime} \in \Theta^{\prime \prime}\). The local Wald estimand (4.1) with \(v=0\) in this setting becomes

\[
\tau\left(\theta^{\prime \prime}\right)=\frac{\lim _{x \downarrow 0} \mathrm{E}\left[Y_{i} \mid X_{i}=x\right]-\lim _{x \uparrow 0} \mathrm{E}\left[Y_{i} \mid X_{i}=x\right]}{\lim _{x \downarrow 0} \mathrm{E}\left[D_{i} \mid X_{i}=x\right]-\lim _{x \uparrow 0} \mathrm{E}\left[D_{i} \mid X_{i}=x\right]}
\]

for all \(\theta^{\prime \prime} \in \Theta^{\prime \prime}=\{0\}\). This estimand \(\tau(0)\) will be denoted by \(\tau_{F M R D}\) for Fuzzy Mean RD design.\\
Example 2 (Fuzzy Quantile RDD). Consider the estimand (3.1) proposed by Frandsen et al. (2012) to identify the conditional CDF of potential outcome \(Y_{i}^{d}\) under each treatment status \(d \in\{0,1\}\) given the event \(C\) of compliance, and the quantile treatment effect (3.2) given this event \(C\). These estimands also fit in the general framework (4.1). We first fix an \(a \in(0,1 / 2), \varepsilon>0\) and let \(\mathscr{Y}_{1}=\left[Q_{Y^{1} \mid C}(a)-\varepsilon, Q_{Y^{1} \mid C}(1-a)+\varepsilon\right] \cup\left[Q_{Y^{0} \mid C}(a)-\varepsilon, Q_{Y^{0} \mid C}(1-a)+\varepsilon\right]\). Let \(\Theta_{1}=\Theta_{1}^{\prime}=\mathscr{Y}_{1} \times \mathscr{D}\) and \(\Theta_{2}=\Theta_{2}^{\prime}=\mathscr{D}\) for \(\mathscr{D}=\{0,1\}\), and let \(\Theta^{\prime \prime}=[a, 1-a]\) for a constant \(a \in(0,1 / 2)\). Let \(Y_{i}=\left(Y_{i}^{*}, D_{i}^{*}\right)\) and \(D_{i}=D_{i}^{*}\). Set \(g_{1}\left(\left(Y_{i}^{*}, D_{i}^{*}\right),(y, d)\right)=\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\}\) and \(g_{2}\left(D_{i}^{*}, d\right)=\mathbb{1}\left\{D_{i}^{*}=d\right\}\). Note that \(\mu_{1}(x, y, d)=\mathrm{E}\left[g_{1}\left(\left(Y_{i}^{*}, D_{i}^{*}\right), y, d\right) \mid X_{i}=\right.\) \(x]=\mathrm{E}\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\) and \(\mu_{2}(x, d)=\mathrm{E}\left[g_{2}\left(D_{i}^{*}, d\right) \mid X_{i}=x\right]=\mathrm{E}\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\). Let \(\phi\) and \(\psi\) be the identity operators, and define \(\Upsilon\) for each \(W \in \ell^{\infty}\left(\Theta_{1}^{\prime} \times \Theta_{2}^{\prime}\right)=\ell^{\infty}\left(\mathscr{Y} \times \mathscr{D}^{2}\right)\) by \(\Upsilon(W)\left(\theta^{\prime \prime}\right)=\inf \{y \in \mathscr{Y}: W(y, 1,1) \geq\) \(\left.\theta^{\prime \prime}\right\}-\inf \left\{y \in \mathscr{Y}: W(y, 0,0) \geq \theta^{\prime \prime}\right\}\). The local Wald estimand (4.1) with \(v=0\) in this setting becomes (3.2), i.e.,

\[
\tau\left(\theta^{\prime \prime}\right)=Q_{Y^{1} \mid C}\left(\theta^{\prime \prime}\right)-Q_{Y^{0} \mid C}\left(\theta^{\prime \prime}\right)
\]

for all \(\theta^{\prime \prime} \in \Theta^{\prime \prime}=[a, 1-a]\), where \(Q_{Y^{d} \mid C}\left(\theta^{\prime \prime}\right):=\inf \left\{y \in \mathscr{Y}: F_{Y^{d} \mid C}(y) \geq \theta^{\prime \prime}\right\}\) for a short-hand notation, and \(F_{Y^{d} \mid C}(y)\) is given in (3.1) for all \((y, d) \in \mathscr{Y} \times \mathscr{D}\). This estimand \(\tau\) will be denoted by \(\tau_{\mathrm{FQRD}}\) for Fuzzy Quantile RD design.

We introduce some short-hand notations for conservation of space. Let \(\mathcal{E}_{k}(y, d, x, \theta)=g_{k}\left(y, \theta_{k}\right)-\mu_{k}\left(x, \theta_{k}\right)\) for \(\left(\theta_{1}, \theta_{2}\right) \in \Theta, k \in\{1,2\}, y \in \mathscr{Y}, d \in \mathscr{D}\), and \(x \in \mathscr{X}\). Let \(\sigma_{k l}(\theta, \vartheta \mid x)=E\left[\mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \cdot \mathcal{E}_{l}\left(Y_{i}, D_{i}, X_{i}, \vartheta\right) \mid X_{i}=x\right]\) denote the conditional covariance of residuals for each \(\theta=\left(\theta_{1}, \theta_{2}\right), \vartheta=\left(\vartheta_{1}, \vartheta_{2}\right) \in \Theta\), and \(k, l \in\{1,2\}\). Also define the product space \(\mathbb{T}=\Theta \times\{1,2\}=\left(\Theta_{1} \times \Theta_{2}\right) \times\{1,2\}\). We will also use the following short-hand notations for functions at rightand left-hand limits: \(\mu_{k}^{(v)}\left(0^{+}, \theta\right)=\lim _{x \downarrow 0} \mu_{k}^{(v)}(x, \theta)\) and \(\mu_{k}^{(v)}\left(0^{-}, \theta\right)=\lim _{x \uparrow 0} \mu_{k}^{(v)}(x, \theta)\) for \(k=\{1,2\}\). The composite notation \(\mu_{k}^{(v)}\left(0^{ \pm}, \theta\right)\) is used to collectively refer to \(\mu_{k}^{(v)}\left(0^{+}, \theta\right)\) and \(\mu_{k}^{(v)}\left(0^{-}, \theta\right)\). Let \(r_{p}(x)=\left(1, x, \ldots, x^{p}\right)^{\prime}\). Let \(K\) denote a kernel function, and let \(\left(h_{1, n}\left(\theta_{1}\right), h_{2, n}\left(\theta_{2}\right)\right)\) denote bandwidth parameters that depend on \(\theta=\left(\theta_{1}, \theta_{2}\right) \in \Theta\) and the sample size \(n \in \mathbb{N}\). For \(p \in \mathbb{N}\), let \(e_{v}\) denote the \(v\) th standard basis element of \(\mathbb{R}^{p}\). We write \(\Gamma_{p}^{ \pm}=\int_{\mathbb{R}_{ \pm}} K(u) r_{p}(u) r_{p}^{\prime}(u) d u\) and \(\Lambda_{p, q}^{ \pm}=\int_{\mathbb{R}_{ \pm}} u^{q} K(u) r_{p}(u) d u\). We use the notation \(\rightsquigarrow\) to denote weak convergence, and the notation \(\underset{\xi}{\stackrel{p}{\xi}}\) for conditional weak convergence as defined in Section 2.2.3 of Kosorok (2008) - see Appendix A. 2 for more details. Let \(v, p, q \in \mathbb{N}_{+}\)with \(v \leq p\). We will use \(v\) for the order of derivative of interest as in (4.1), and \(p\) stands for the order of local polynomial fitting to estimate (4.1).

\subsection*{4.2. The local wald estimator}
In this section, we develop an estimator for the nonparametric components \(\mu_{k}^{(v)}\left(0^{ \pm}, \cdot\right), k \in\{1,2\}\), of the local Wald estimand (4.1) based on local polynomial fitting with the bias correction approach proposed by Calonico et al. (2014) - for a comprehensive treatment for local polynomial models, see Fan and Gijbels (1996). Under proper smoothness assumptions to be formally stated below, the \(p\) th order approximations

\[
\begin{array}{ll}
\mu_{k}\left(x, \theta_{k}\right) \approx \mu_{k}\left(0^{+}, \theta_{k}\right)+\mu_{k}^{(1)}\left(0^{+}, \theta_{k}\right) x+\cdots+\frac{\mu_{k}^{(p)}\left(0^{+}, \theta_{k}\right)}{p!} x^{p}=r_{p}(x / h)^{\prime} \alpha_{k+, p}\left(\theta_{k}\right) & x>0 \\
\mu_{k}\left(x, \theta_{k}\right) \approx \mu_{k}\left(0^{-}, \theta_{k}\right)+\mu_{k}^{(1)}\left(0^{-}, \theta_{k}\right) x+\cdots+\frac{\mu_{k}^{(p)}\left(0^{-}, \theta_{k}\right)}{p!} x^{p}=r_{p}(x / h)^{\prime} \alpha_{k-, p}\left(\theta_{k}\right) & x<0
\end{array}
\]

hold for each \(k \in\{1,2\}\), where \(\alpha_{k \pm, p}(\theta)=\left[\mu_{k}\left(0^{ \pm}, \theta_{k}\right) / 0!, \mu_{k}^{(1)}\left(0^{ \pm}, \theta_{k}\right) h / 1!, \ldots, \mu_{k}^{(p)}\left(0^{ \pm}, \theta_{k}\right) h^{p} / p!\right]\) and \(h>0\). To estimate \(\alpha_{k \pm, p}\left(\theta_{k}\right)\), we solve the one-sided local weighted least squares problems

\[
\begin{aligned}
& \hat{\alpha}_{1 \pm, p}\left(\theta_{1}\right)=\underset{\alpha \in \mathbb{R}^{p+1}}{\arg \min } \sum_{i=1}^{n} \delta_{i}^{ \pm}\left(g_{1}\left(Y_{i}, \theta_{1}\right)-r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{\prime} \alpha\right)^{2} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \\
& \hat{\alpha}_{2 \pm, p}\left(\theta_{2}\right)=\underset{\alpha \in \mathbb{R}^{p+1}}{\arg \min } \sum_{i=1}^{n} \delta_{i}^{ \pm}\left(g_{2}\left(D_{i}, \theta_{2}\right)-r_{p}\left(\frac{X_{i}}{h_{2, n}\left(\theta_{2}\right)}\right)^{\prime} \alpha\right)^{2} K\left(\frac{X_{i}}{h_{2, n}\left(\theta_{2}\right)}\right),
\end{aligned}
\]

where \(\delta_{i}^{+}=\mathbb{1}\left\{X_{i} \geq 0\right\}\) and \(\delta_{i}^{-}=\mathbb{1}\left\{X_{i} \leq 0\right\}\). We let the coordinates of these estimates be written by

\[
\hat{\alpha}_{k \pm, p}\left(\theta_{k}\right)=\left[\hat{\mu}_{k, p}\left(0^{ \pm}, \theta_{k}\right) / 0!, \hat{\mu}_{k, p}^{(1)}\left(0^{ \pm}, \theta_{k}\right) h_{k, n}\left(\theta_{k}\right) / 1!, \ldots, \hat{\mu}_{k, p}^{(p)}\left(0^{ \pm}, \theta\right) h_{k, n}^{p}\left(\theta_{k}\right) / p!\right]^{\prime}
\]

With these component estimates, the local Wald estimand (4.1) is in turn estimated by the plug-in estimator.

\[
\hat{\tau}\left(\theta^{\prime \prime}\right)=\Upsilon\left(\frac{\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)-\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{-}, \cdot\right)\right)(\cdot)}{\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)-\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{-}, \cdot\right)\right)(\cdot)}\right)\left(\theta^{\prime \prime}\right) \quad \text { for each } \theta^{\prime \prime} \in \Theta^{\prime \prime}
\]

\subsection*{4.3. Weak convergence}
In this section, we establish the weak convergence result for the process \(\sqrt{n h_{n}^{1+2 v}}[\hat{\tau}-\tau]\) for some bandwidth \(h_{n}\). To this end we state the following set of assumptions.

Assumption 1 (Uniform Bahadur Representation). Let \(\underline{x}<0<\bar{x}\). (i) (a) \(\left\{\left(Y_{i}, D_{i}, X_{i}\right)\right\}_{i=1}^{n}\) are \(n\) i.i.d. copies of random vector ( \(Y, D, X\) ) defined on a probability space \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\); (b) \(X\) has a density function \(f_{X}\) which is continuously differentiable on \([\underline{x}, \bar{x}]\), and \(0<f_{X}(0)<\infty\). (ii) For each \(k=1,2\), (a) the collections of real-valued functions, \(\left\{x \mapsto \mu_{k}\left(x, \theta_{k}\right): \theta_{k} \in \Theta_{k}\right\}\), \(\left\{y \mapsto g_{1}\left(y, \theta_{1}\right): \theta_{1} \in \Theta_{1}\right\}\), and \(\left\{d \mapsto g_{2}\left(d, \theta_{2}\right): \theta_{2} \in \Theta_{2}\right\}\), are of VC type with a common integrable envelope \(F_{\mathcal{E}}\) such that \(\int_{\mathscr{Y} \times \mathscr{D} \times[\underline{x}, \overline{]}]}\left|F_{\mathcal{E}}(y, d, x)\right|^{2+\epsilon} d \mathbb{P}^{x}(y, d, x)<\infty\) for some \(\epsilon>0\); (b) \(\mu_{k}^{(j)}\) is Lipschitz on \([\underline{x}, 0) \times \Theta_{k}\) and \((0, \bar{x}] \times \Theta_{k}\) for \(j=0,1,2, \ldots, p+1\); (c) For any \((\theta, k),(\vartheta, l) \in \mathbb{T}\), we have \(\sigma_{k l}(\theta, \vartheta \mid \cdot) \in \mathcal{C}^{1}([\underline{x}, \bar{x}] \backslash\{0\})\) with bounded derivatives in \(x\) and \(\sigma_{k l}\left(\theta, \vartheta \mid 0^{ \pm}\right)<\infty\); (d) For each \(y \in \mathscr{Y}, g_{1}(y, \cdot)\) is left- or right-continuous in each dimension. Similarly, for each \(d \in \mathscr{D}, g_{2}(d, \cdot)\) is left- or right-continuous in each dimension. (iii) There exist bounded Lipschitz functions \(c_{1}: \Theta_{1} \rightarrow[\underline{c}, \bar{c}] \subset(0, \infty)\) and \(c_{2}: \Theta_{2} \rightarrow[\underline{c}, \bar{c}] \subset(0, \infty)\) such that \(h_{1, n}\left(\theta_{1}\right)=c_{1}\left(\theta_{1}\right) h_{n}\) and \(h_{2, n}\left(\theta_{2}\right)=c_{2}\left(\theta_{2}\right) h_{n}\) hold for baseline bandwidth \(h_{n}\) satisfying \(h_{n} \rightarrow 0, n h_{n}^{2} \rightarrow \infty\) and \(n h_{n}^{2 p+3} \rightarrow 0\) for some \(h_{0}<\infty\). (iv) (a) \(K:[-1,1] \rightarrow \mathbb{R}^{+}\)is bounded and continuous; (b) \(\{K(\cdot / h): h>0\}\) is of VC type. (c) \(\Gamma_{p}^{ \pm}\)is positive definite.

Condition (i) requires a random sampling of \((Y, D, X)\) and sufficient data around \(X=0\). The i.i.d. condition (i) (a) is shared by most of the prior work on regression discontinuity and kink designs. Exceptions are Bartalotti and Brummet (2017) and Calonico et al. (2018b), which relax the assumption of identical distribution and study clusterrobust inference - we will also present a method of cluster-robust inference in Section 7.1 by extending our baseline results. Versions of the smoothness and nonzero requirements in condition (i) (b) are shared by the assumptions made in the prior work, and relate to the absence of endogenous sorting. This assumption is analogous to Assumption 1.1. in the closely related benchmark paper by Bartalotti et al. (2017). Regarding condition (ii), a sufficient condition for \(\left\{x \mapsto \mu_{k}\left(x, \theta_{k}\right): \theta_{\underline{k}} \in \Theta_{k}\right\}\) to be of VC type class is, for example, the existence of some non-negative function \(M_{k}: \mathscr{X} \rightarrow \mathbb{R}_{+}\) such that \(\left|\mu_{k}\left(x, \bar{\theta}_{k}\right)-\mu_{k}\left(x, \theta_{k}\right)\right| \leq M_{k}(x)\left|\bar{\theta}_{k}-\theta_{k}\right|\) for all \(\bar{\theta}_{k}, \theta_{k} \in \Theta_{k}\) for each \(k=1\), 2 . Analogous remarks apply to \(\left\{y \mapsto g_{1}\left(y, \theta_{1}\right): \theta_{1} \in \Theta_{1}\right\}\) and \(\left\{d \mapsto g_{2}\left(d, \theta_{2}\right): \theta_{2} \in \Theta_{2}\right\}\) as well. Another sufficient condition is the case when a class of functions is of variations bounded by one, e.g. in the case of CDF estimation, \(\left\{y \mapsto \mathbb{1}\left\{y \leq y^{\prime}\right\}: y^{\prime} \in \mathscr{Y}\right\}\) satisfies the VC type condition. Also notice that the common integrable envelope \(F_{\mathcal{E}}\) in condition (ii) is satisfied if all the classes of functions are uniformly bounded, but does not rule out some cases that some of these classes of functions are unbounded. In Appendix C, we will check these high-level assumptions with primitive sufficient assumptions for each of the ten specific examples presented in Examples 1-2 and Appendix B.1-B.8. Parts (ii) (b) and (ii) (c) are analogous to Assumption 1.3. and Assumption 1.4., respectively, in Bartalotti et al. (2017). Condition (iii) specifies admissible rates of bandwidths, which are consistent with common choice rules (e.g., Imbens and Kalyanaraman, 2012; Calonico et al., 2014; Arai and Ichimura, 2016; Calonico et al., 2016, 2018a; Arai and Ichimura, 2018; for regression discontinuity designs) see Appendix F. This condition is analogous to Assumption 2 in Bartalotti et al. (2017) for the special case of \(h \sim b\) in their notation. Condition (iv) is satisfied by common kernel functions, such as uniform, triangular, biweight, triweight, and Epanechnikov kernels to list a few examples, while the normal kernel is obviously ruled out.

We will show below the weak convergence in \(\ell^{\infty}(\mathbb{T})\) of the Bahadur Representation (BR)

\[
v_{n}^{ \pm}(\theta, k)=v!\sum_{i=1}^{n} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) r_{p}\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) K\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{k, n}\left(\theta_{k}\right)} f_{X}(0)}
\]

We also write \(v_{n}(\cdot)=v_{n}^{+}(\cdot)-v_{n}^{-}(\cdot)\). By the functional delta method, the weak convergence translate into the asymptotic distribution of the process \(\sqrt{n h_{n}^{1+2 v}}[\hat{\tau}(\cdot)-\tau(\cdot)]\) for the local Wald estimator (4.5). We set the following additional assumption. Primitive conditions for it will be discussed specifically for the fuzzy quantile RDD in Section 4.5, and for each of the nine examples in Appendix \(C\).

Assumption 2 (Conditional Weak Convergence). (i) \(\psi, \phi\) and \(\Upsilon\) are Hadamard differentiable at \(\mu_{1}^{(v)}\left(0^{ \pm}, \cdot\right), \mu_{2}^{(v)}\left(0^{ \pm}\right.\), \(\left.\cdot\right)\), and \(W\), respectively, tangentially to some subspaces of their domains, with their Hadamard derivatives denoted by \(\phi_{\mu_{1}^{\prime}\left(0^{ \pm}, \cdot\right)}\), \(\psi_{\mu_{2}^{(v)}\left(0^{ \pm}, \cdot\right)}^{\prime}\), and \(\Upsilon_{W}^{\prime}\), respectively. (ii) \(\inf _{\theta_{2}^{\prime} \in \Theta_{2}^{\prime}}\left|\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)\left(\theta_{2}^{\prime}\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\left(\theta_{2}^{\prime}\right)\right|>0\). (iii) \(n h_{n}^{1+2 v} \rightarrow \infty\) as \(n \rightarrow \infty\).

Condition (ii) requires the existence of a jump or a kink, which is assumed in most of the prior work as the key identification condition. Exceptions are Otsu et al. (2015) and Feir et al. (2016), which provide weak-identification-robust\\
methods of inference. We later use this idea to relax condition (ii) of this assumption in Section 7.2. Condition (iii) restricts admissible rates of bandwidths, which are consistent with common choice rules (e.g., Imbens and Kalyanaraman, 2012; Calonico, Cattaneo and Titiunik, 2014; Arai and Ichimura, 2016; Calonico, Cattaeneo and Farrell, 2016, 2018; Arai and Ichimura, 2018; for regression discontinuity designs) - see Appendix F. The next theorem states the weak convergence result for the process \(\sqrt{n h_{n}^{1+2 v}}[\hat{\tau}-\tau]\).

Theorem 1 (Weak Convergence). Under Assumptions 1 and 2, we have \(v_{n}^{ \pm} \rightsquigarrow \mathbb{G}_{H^{ \pm}}\), where \(\mathbb{G}_{H^{ \pm}}\)are zero mean Gaussian processes \(\mathbb{G}_{H^{ \pm}}: \Omega^{x} \mapsto \ell^{\infty}(\mathbb{T})\) with covariance function

\[
H^{ \pm}((\theta, k),(\vartheta, l))=\frac{\sigma_{k l}\left(\theta, \vartheta \mid 0^{ \pm}\right) e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \Psi_{p}^{ \pm}((\theta, k),(\vartheta, l))\left(\Gamma_{p}^{ \pm}\right)^{-1} e_{v}}{\sqrt{c_{k}\left(\theta_{k}\right) c_{l}\left(\vartheta_{l}\right)} f_{X}(0)}
\]

where \(\Psi_{p}^{ \pm}((\theta, k),(\vartheta, l))=\int_{\mathbb{R}_{ \pm}} r_{p}\left(u / c_{k}\left(\theta_{k}\right)\right) r_{p}^{\prime}\left(u / c_{l}\left(\vartheta_{l}\right)\right) K\left(\frac{u}{c_{k}\left(\theta_{1}\right)}\right) K\left(\frac{u}{c_{l}\left(\vartheta_{l}\right)}\right) d u\) for each \(\theta=\left(\theta_{1}, \theta_{2}\right), \vartheta=\left(\vartheta_{1}, \vartheta_{2}\right) \in \Theta\). Therefore,

\[
\sqrt{n h_{n}^{1+2 v}}[\hat{\tau}-\tau] \rightsquigarrow \Upsilon_{W}^{\prime}\left(\frac{\left[\psi\left(\mu_{2}^{(v)}(0+, \cdot)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \mathbb{G}^{\prime}(\cdot, 1)-\left[\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \mathbb{G}^{\prime}(\cdot, 2)}{\left[\psi\left(\mu_{2}^{(v)}(0+, \cdot)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right]^{2}}\right),
\]

where \(\mathbb{G}^{\prime}: \Omega^{x} \mapsto \ell^{\infty}(\mathbb{T})\) is defined as

\[
\left[\begin{array}{l}
\mathbb{G}^{\prime}(\cdot, 1) \\
\mathbb{G}^{\prime}(\cdot, 2)
\end{array}\right]=\left[\begin{array}{l}
\phi_{\mu_{1}^{(v)}\left(0^{+}, \cdot\right)}^{\prime}\left(\mathbb{G}_{H+}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot)-\phi_{\mu_{1}^{(v)}\left(0^{-}, \cdot\right)}^{\prime}\left(\mathbb{G}_{H-}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot) \\
\psi_{\mu_{2}^{\prime(v)}\left(0^{+}, \cdot\right)}\left(\mathbb{G}_{H+}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)-\psi_{\mu_{2}^{\prime}\left(0^{-}, \cdot\right)}^{\prime}\left(\mathbb{G}_{H-}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)
\end{array}\right]
\]

See Appendix A. 3 for a proof. For the sharp mean RDD, the fuzzy mean RDD, the sharp mean RKD, and the fuzzy mean RKD, computation of the limit process is straightforward in practice. On the other hand, for CDF and the quantile process, it is somewhat easier to approximate the limit process through the multiplier bootstrap. The following subsection presents this additional practical consideration.

\subsection*{4.4. Multiplier bootstrap}
To simulate the limiting process of the BR, i.e., Theorem 1, we use the pseudo random sample \(\left\{\xi_{i}\right\}_{i=1}^{n}\) drawn from the standard normal distribution, independently from the data \(\left\{\left(Y_{i}, D_{i}, X_{i}\right)\right\}_{i=1}^{n}\). Precisely, \(\left\{\xi_{i}\right\}_{i=1}^{n}\) is defined on \(\left(\Omega^{\xi}, \mathcal{F}^{\xi}, \mathbb{P}^{\xi}\right)\), a probability space that is independent of \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\) - this condition will be formally stated in Assumption 3 . With this pseudo random sample, define the multiplier processes (MP)

\[
v_{\xi, n}^{ \pm}(\theta, k)=v!\sum_{i=1}^{n} \xi_{i} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) r_{p}\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) K\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{k, n}\left(\theta_{k}\right)} f_{X}(0)}
\]

We also write \(v_{\xi, n}(\cdot)=v_{\xi, n}^{+}(\cdot)-v_{\xi, n}^{-}(\cdot)\).\\
In practice, we need to replace \(\mathcal{E}_{k}\) and \(f_{X}\) with their estimates. Let \(\hat{f}_{X}\) be an estimate of \(f_{X}\). For estimation of \(\mathcal{E}_{k}\), since every component in the BR is multiplied by the kernel \(K\) supported on [ \(-1,1\) ], we only need to consider \(\mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mathbb{1}\left\{\left|X_{i} / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\). Write its estimate by \(\hat{\mathcal{E}}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mathbb{1}\left\{\left|X_{i} / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\), which has \(\mu_{k, p}\) replaced by some estimate \(\tilde{\mu}_{k, p}\) of \(\mu_{k, p}\). Appendix A. 6 discusses the effects of these first-stage estimates. Substituting these estimated components in the MP, we define the estimated multiplier processes (EMP)

\[
\hat{\mathcal{v}}_{\xi, n}^{ \pm}(\theta, k)=v!\sum_{i=1}^{n} \xi_{i} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \hat{\mathcal{E}}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) r_{p}\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) K\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{k, n}\left(\theta_{k}\right)} \hat{f}_{X}(0)}
\]

We also write \(\hat{v}_{\xi, n}(\cdot)=\hat{v}_{\xi, n}^{+}(\cdot)-\hat{v}_{\xi, n}^{-}(\cdot)\).\\
In order to establish the uniform validity of the multiplier bootstrap, we invoke the following two sets of assumptions.\\
Assumption 3 (Multiplier). \(\left\{\xi_{i}\right\}_{i=1}^{n}\) is an independent standard normal random sample defined on \(\left(\Omega^{\xi}, \mathcal{F}^{\xi}, \mathbb{P}^{\xi}\right)\), a probability space that is independent of \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\).

Assumption 4 (First Stage Estimation). \(\tilde{\mu}_{k, p}\left(x, \theta_{k}\right) \mathbb{1}\left\{\left|x / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\) is uniformly consistent for \(\mu_{k}\left(x, \theta_{k}\right) \mathbb{1}\left\{\left|x / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\) on \(([\underline{x}, \bar{x}] \backslash\{0\}) \times \mathbb{T} . \hat{f}_{X}(0)\) is consistent for \(f_{X}(0)\).

We remark that Assumption 3 is the standard assumption for multiplier, score, and wild bootstrap methods, cf. Kosorok (2003, 2008). At the current level of generality, we state these high-level conditions for the first stage estimation in Assumption 4. However, we will propose a concrete \(\tilde{\mu}_{k}\) in Appendix A. 6 that satisfies Assumption 4 under Assumptions 1 and 2. The following theorem, together with Theorem 1, establishes the uniform validity of the multiplier bootstrap.

Theorem 2 (Conditional Weak Convergence). Under Assumption 1, 2, 3, and 4, we have \(\hat{v}_{\xi, n}^{ \pm} \underset{\xi}{p} \mathbb{G}_{H^{ \pm}}\), and therefore

\[
\begin{aligned}
& \Upsilon_{W}^{\prime}\left(\frac{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 1)-\left[\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 2)}{\left(\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right)^{2}}\right) \\
& \underset{\xi}{p} \Upsilon_{W}^{\prime}\left(\frac{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \mathbb{G}^{\prime}(\cdot,, 1)-\left[\phi\left(\mu_{(v)}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)\right] \mathbb{G}^{\prime}(\cdot, 2)}{\left(\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right)^{2}}\right),
\end{aligned}
\]

where

\[
\left[\begin{array}{c}
\widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 1) \\
\widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 2)
\end{array}\right]=\left[\begin{array}{c}
\phi_{\mu_{1}^{(v)}\left(0^{+}, \cdot\right)}^{\prime}\left(\hat{v}_{\xi, n}^{+}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot)-\phi_{\mu_{1}^{(v)}\left(0^{-}, \cdot\right)}^{\prime}\left(\hat{v}_{\xi, n}^{-}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot) \\
\left.\psi_{\mu_{2}^{\prime(v)}\left(0^{+}, \cdot\right)}\left(\hat{v}_{\xi, n}^{+}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)-\psi_{\mu_{2}^{\prime}\left(0^{-}, \cdot\right)}^{\prime\left(\hat{v}_{\xi, n}^{-}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)}\right]
\end{array}\right]
\]

See Appendix A. 4 for a proof. Theorems 1 and 2 show that the estimated multiplier process

\[
\Upsilon_{W}^{\prime}\left(\frac{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 1)-\left[\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 2)}{\left(\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right)^{2}}\right)
\]

can be used to approximate the limit process of \(\sqrt{n h_{n}^{1+2 v}}[\hat{\tau}-\tau]\) in practice.

\subsection*{4.5. Fuzzy quantile RDD (Example 2) revisited}
In this section, we apply the main general results, namely Theorems 1 and 2, to the fuzzy quantile RDD introduced in Example 2. What we present in this section provides a formal justification of the overview in Section 3. We present our assumptions for the case of \(p=2\) as we did in the overview. We remark that, however, using a different order \(p\) of local polynomial fitting is also possible by similar arguments.

Consider \(\Theta_{1}, \Theta_{2}, \Theta_{1}^{\prime}, \Theta_{2}^{\prime}, \Theta^{\prime \prime}, g_{1}, g_{2}, \phi, \psi\), and \(\Upsilon\) defined in Example 2. Recall that we denote the local Wald estimand (4.1) with \(v=0\) in this setting by \(\tau_{\text {FQRD }}\). We also denote the analog estimator (4.5) with \(v=0\) in this setting by

\[
\hat{\tau}_{F Q R D}\left(\theta^{\prime \prime}\right)=\Upsilon\left(\hat{F}_{Y \cdot \mid C}\right)\left(\theta^{\prime \prime}\right)
\]

where

\[
\hat{F}_{Y^{1} \mid C}(y)=\frac{\hat{\mu}_{1,2}\left(0^{+}, y, 1\right)-\hat{\mu}_{1,2}\left(0^{-}, y, 1\right)}{\hat{\mu}_{2,2}\left(0^{+}, 1\right)-\hat{\mu}_{2,2}\left(0^{-}, 1\right)} \quad \text { and } \quad \hat{F}_{Y^{0} \mid C}(y)=\frac{\hat{\mu}_{1,2}\left(0^{+}, y, 0\right)-\hat{\mu}_{1,2}\left(0^{-}, y, 0\right)}{\hat{\mu}_{2,2}\left(0^{+}, 0\right)-\hat{\mu}_{2,2}\left(0^{-}, 0\right)}
\]

By van der Vaart and Wellner (1996; Lemma 3.9.23) and van der Vaart (1998; Theorem 20.9), \(\Upsilon\) is Hadamard differentiable at \(\left(F_{Y^{\cdot} \mid C}\right)\) tangentially to \(C\left(\mathscr{Y}_{1} \times \mathscr{D}^{2}\right)\), and the Hadamard derivative is a map that takes each \(g \in C\left(\mathscr{Y}_{1} \times \mathscr{D}^{2}\right)\) to

\[
\Upsilon_{W}^{\prime}(g)(\cdot)=: \Upsilon_{F_{Y} \mid C}^{\prime}(g)(\cdot)=-\frac{g\left(Q_{Y^{1} \mid C}(\cdot), 1,1\right)}{f_{Y^{1} \mid C}\left(Q_{Y^{1} \mid C}(\cdot)\right)}+\frac{g\left(Q_{Y^{0} \mid C}(\cdot), 0,0\right)}{f_{Y^{0} \mid C}\left(Q_{Y^{0} \mid C}(\cdot)\right)}
\]

under the assumptions to be stated below. Sufficient conditions for the assumptions required for the general result, tailored to the current example, are stated as follows.

Assumption \(\mathbf{S}\). (a) \(\left\{\left(Y_{i}, D_{i}, X_{i}\right)\right\}_{i=1}^{n}\) are \(n\) i.i.d. copies of the random vector \((Y, D, X)\) defined on a probability space ( \(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\) ). (b) \(X\) has a density function \(f_{X}\) which is continuously differentiable on \([\underline{x}, \bar{x}]\) that contains 0 in its interior, and \(0<f_{X}(0)<\infty\).

Assumption K. (a) \(K:[-1,1] \rightarrow \mathbb{R}^{+}\)is bounded and continuous. (b) \(\{K(\cdot / h): h>0\}\) is of VC type. (c) \(\Gamma_{p}^{ \pm}\)is positive definite.

Assumption M. \(\left\{\xi_{i}\right\}_{i=1}^{n}\) are independent standard normal random variables defined on \(\left(\Omega^{\xi}, \mathcal{F}^{\xi}, \mathbb{P}^{\xi}\right)\), a probability space that is independent of \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\).

Assumption FQRD. (i) \((x, y, d) \mapsto \frac{\partial^{j}}{\partial x^{j}} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d\right\} \mid X_{i}=x\right]\) is Lipschitz in \(x\) on \([\underline{x}, 0) \times \Theta_{1}\) and \((0, \bar{x}] \times \Theta_{1}\) for \(j=0,1,2,3\), and \((x, d) \mapsto \frac{\partial^{j}}{\partial x^{j}} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]\) is Lipschitz in \(x\) on \([\underline{x}, 0) \times \Theta_{2}\) and \((0, \bar{x}] \times \Theta_{2}\) for \(j=0,1,2,3\). (ii) The baseline bandwidth \(h_{n}\) satisfies \(h_{n} \rightarrow 0\) and \(n h_{n}^{2} \rightarrow \infty, n h_{n}^{7} \rightarrow 0\). There exist bounded constants \(0<c_{1}\), \(c_{2}<\infty\) such that \(h_{1, n}=c_{1} h_{n}\) and \(h_{2, n}=c_{2} h_{n}\). (iii) \(\left|\mathbb{P}^{x}\left(D_{i}=1 \mid X_{i}=0^{+}\right)-\mathbb{P}^{x}\left(D_{i}=1 \mid X_{i}=0^{-}\right)\right|>0\). (iv) \(F_{Y^{1} \mid C}, F_{Y^{0} \mid C} \in \mathcal{C}^{1}\left(\mathscr{Y}_{1}\right)\), and \(f_{Y^{1} \mid C}\) and \(f_{Y^{0} \mid C}\) are bounded away from 0 on \(\mathscr{Y}_{1}\). (v) There exists \(\hat{f}_{Y \mid X D^{*}}\left(y \mid 0^{ \pm}, d\right)\) such that \(\sup _{(y, d) \in \mathscr{Y}_{1 \times \mathscr{D}}}\left|\hat{f}_{Y \mid X D^{*}}\left(y \mid 0^{ \pm}, d\right)-f_{Y \mid X D^{*}}\left(y \mid 0^{ \pm}, d\right)\right|=o_{p}^{x}(1)\).

Assumption \(S(\mathrm{a})\) is assumed in most of the prior work, including the closely related benchmark by Frandsen et al. (2012). As emphasized after "Assumptions 1 (i) (a)", a part of the literature has relaxed the assumption of identical distribution and studies cluster-robust inference - we will also present a method of cluster-robust inference in Section 7.1 by extending our baseline results. Assumption S (b) is analogous to Assumption E3 of Frandsen et al. (2012). Assumption K is analogous to Assumption E4 of Frandsen et al. (2012). Assumption M is new to our paper due to our use of the multiplier bootstrap, which is not used by prior work such as Frandsen et al. (2012). Assumption FQRD (i), (ii), (iii), and (iv) are analogous to Assumptions E1, E5, E2, and Q respectively, of Frandsen et al. (2012). We state Assumptions FQRD (v) at this high level in order to accommodate a number of alternative estimators. In Lemma 14 in Appendix E.1.2, however, we propose one such concrete estimator which satisfies part (v). All the other parts of this assumption are immediately interpretable.

Define the EMP by

\[
\begin{aligned}
& \hat{v}_{\xi, n}^{ \pm}\left(y, d_{1}, d_{2}, 1\right)=\sum_{i=1}^{n} \xi_{i} \frac{e_{0}^{\prime}\left(\Gamma_{2}^{ \pm}\right)^{-1}\left[\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d_{1}\right\}-\tilde{\mu}_{1,2}\left(X_{i}, y, d_{1}\right)\right] r_{2}\left(\frac{X_{i}}{h_{n}}\right) K\left(\frac{X_{i}}{h_{n}}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{n}} \hat{f}_{X}(0)} \\
& \hat{v}_{\xi, n}^{ \pm}\left(y, d_{1}, d_{2}, 2\right)=\sum_{i=1}^{n} \xi_{i} \frac{e_{0}^{\prime}\left(\Gamma_{2}^{ \pm}\right)^{-1}\left[\mathbb{1}\left\{D_{i}^{*}=d_{2}\right\}-\tilde{\mu}_{2,2}\left(X_{i}, d_{2}\right)\right] r_{2}\left(\frac{X_{i}}{h_{n}}\right) K\left(\frac{X_{i}}{h_{n}}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{n}} \hat{f}_{X}(0)} \\
& \widehat{\mathbb{X}}_{n}^{\prime}\left(y, d_{1}, d_{2}, k\right)=\hat{v}_{\xi, n}^{+}\left(y, d_{1}, d_{2}, k\right) / \sqrt{c_{k}}-\hat{v}_{\xi, n}^{-}\left(y, d_{1}, d_{2}, k\right) / \sqrt{c_{k}}
\end{aligned}
\]

for each \(\left(y, d_{1}, d_{2}, k\right) \in \mathbb{T}=\mathscr{Y}_{1} \times \mathscr{D} \times \mathscr{D} \times\{1,2\}\). Define the following estimated process through the Hadamard derivative.

\[
\begin{aligned}
& \widehat{r}_{W}^{\prime}\left(\widehat{\mathbb{Y}}_{\mathrm{Y}}\right)\left(\theta^{\prime \prime}\right) \\
= & \frac{\left[\hat{\mu}_{2,2}\left(0^{+}, 1\right)-\hat{\mu}_{2,2}\left(0^{-}, 1\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}\left(\hat{Q}_{Y^{1} \mid C}\left(\theta^{\prime \prime}\right), 1,1,1\right)-\left[\hat{\mu}_{1,2}\left(0^{+}, \hat{Q}_{Y^{1} \mid c}\left(\theta^{\prime \prime}\right), 1\right)-\hat{\mu}_{1,2}\left(0^{-}, \hat{Q}_{Y^{1} \mid C}\left(\theta^{\prime \prime}\right), 1\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}\left(\hat{Q}_{Y^{1} \mid C}\left(\theta^{\prime \prime}\right), 1,1,2\right)}{\hat{f}_{Y^{1} \mid}\left(\hat{Q}_{Y^{1} \mid C}\left(\theta^{\prime \prime}\right)\right)\left[\hat{\mu}_{2,2}\left(0^{+}, 1\right)-\hat{\mu}_{2,2}\left(0^{-}, 1\right)\right]^{2}} \\
- & \frac{\left[\hat{\mu}_{2,2}\left(0^{+}, 0\right)-\hat{\mu}_{2,2}\left(0^{-}, 0\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}\left(\hat{Q}_{Y^{0} \mid C}\left(\theta^{\prime \prime}\right), 0,0,1\right)-\left[\hat{\mu}_{1,2}\left(0^{+}, \hat{Q}_{Y^{0} \mid C}\left(\theta^{\prime \prime}\right), 0\right)-\hat{\mu}_{1,2}\left(0^{-}, \hat{Q}_{Y^{0} \mid C}\left(\theta^{\prime \prime}\right), 0\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}\left(\hat{Q}_{Y^{0} \mid C}\left(\theta^{\prime \prime}\right), 0,0,2\right)}{\hat{f}_{Y^{0} \mid C}\left(\hat{Q}_{Y^{0} \mid C}\left(\theta^{\prime \prime}\right)\right)\left[\hat{\mu}_{2,2}\left(0^{+}, 0\right)-\hat{\mu}_{2,2}\left(0^{-}, 0\right)\right]^{2}}
\end{aligned}
\]

where

\[
\widehat{\mathbb{Y}}_{n}\left(y, d_{1}, d_{2}\right)=\frac{\left[\hat{\mu}_{2,2}\left(0^{+}, d_{2}\right)-\hat{\mu}_{2,2}\left(0^{-}, d_{2}\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}\left(y, d_{1}, d_{2}, 1\right)-\left[\hat{\mu}_{1,2}\left(0^{+}, y, d_{1}\right)-\hat{\mu}_{1,2}\left(0^{-}, y, d_{1}\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}\left(y, d_{1}, d_{2}, 2\right)}{\left[\hat{\mu}_{2,2}\left(0^{+}, d_{2}\right)-\hat{\mu}_{2,2}\left(0^{-}, d_{2}\right)\right]^{2}}
\]

for \(\left(y, d_{1}, d_{2}\right) \in \mathscr{V}_{1} \times \mathscr{D}^{2}\). With these preparations, our general result applied to the current case yields the following corollary.

Corollary 1 (Example: Fuzzy Quantile RDD). Suppose that Assumptions K, M, S and FQRD hold. (i) There exists a zero mean Gaussian process \(\mathbb{G}_{\text {FRRD }}^{\prime}: \Omega^{x} \mapsto \ell^{\infty}([a, 1-a])\) such that \(\sqrt{n h_{n}}\left[\hat{\tau}_{\text {FQRD }}-\tau_{\text {FRRD }}\right] \rightsquigarrow \mathbb{G}_{\text {FRRD }}^{\prime}\). (ii) Furthermore, with probability approaching one, \(\widehat{\Upsilon}_{W}^{\prime}\left(\widehat{\mathbb{Y}}_{n}\right) \underset{\xi}{p} \mathbb{G}_{\text {FRRD }}^{\prime}\).

A proof is provided in Appendix E.2.1. This result justifies the overview in Section 3. Specifically, the estimated multiplier process \(\widehat{\Upsilon}_{W}^{\prime}\left(\widehat{\mathbb{Y}}_{n}\right)\) - denoted by \(\widehat{\mathbb{G}}_{F Q R D, n}^{\prime}\) in Section 3 for simplicity - can be used to approximate the limit process of \(\sqrt{n h_{n}}\left[\hat{\tau}_{F Q R D}-\tau_{F Q R D}\right]\). We could consider a set of functions defined on \((0,1)\) instead of \([a, 1-a]\) by making a stronger assumption that the potential outcomes are compactly supported with their conditional density functions bounded away from zero, although we may not want to make such a strong assumption in general for typical applications. This tradeoff between the globalization of the domain and the strength of the assumption is even true for simple unconditional quantile processes, e.g., Lemma 21.4 (i) versus Lemma 21.4 (ii) in van der Vaart (1998).

Remark 3. Our bias correction is conducted on the mean regression estimates \(\hat{\mu}_{1,2}\) and \(\hat{\mu}_{2,2}\) in light of Remark 7 of Calonico et al. (2014). Some calculation shows the biases of local Wald-ratios \(\hat{F}_{Y^{d} \mid C}\) also have the same bias order as the regression estimates above. Finally, since the quantile estimate based on a left inverse of a CDF has the same order of bias as the CDF estimate, we can achieve bias correction for fuzzy quantile RDD estimate.

Remark 4. We emphasize that our one-step bias correction as well as Remark 7 of Calonico et al. (2014) relies on choosing the same bandwidth for both main estimators and higher order bias estimators. According to the simulation studies in Calonico et al. (2014), such bandwidth choice works rather well in their various DGPs. While studies of optimal coverage probability for CDF and quantile estimation would be both useful and important, it is out of the scope of the current paper.

It is also worth noting that our framework can accommodate the case when two bandwidths are different as well, since we can acquire the uniform Bahadur representations for both the main estimator and the bias estimator via our uniform Bahadur representation in Appendix A.1. The multiplier bootstrap can be applied to the difference of these two processes, which would be similar to Lemma 3 and Proposition 2 in Qu and Yoon (2018).

\section*{5. Simulation studies}
We conduct simulation studies for the fuzzy quantile RDD - see Sections 3 and 4.5. We also conduct simulation studies for the other cases - see Appendix D. We follow the procedure outlined in Appendix F for choices of bandwidths in finite samples. The kernel function that we use is the Epanechnikov kernel. All the other procedures exactly follow the guideline in Section 4.5.

To systematically evaluate the hypothesis testing for the hypotheses of uniform treatment nullity and treatment homogeneity presented in Section 3, we consider the following data generating process. We generate an i.i.d. sample \(\left\{\left(Y_{i}, D_{i}, X_{i}\right)\right\}_{i=1}^{n}\) through the following data generating process:

\[
\left\{\begin{array}{l}
Y_{i}=\mu\left(X_{i}\right)+\beta_{1} D_{i}+\left(1+\gamma_{1} D_{i}\right) \cdot U_{i}, \\
D_{i}=\mathbb{1}\left\{2 \cdot \mathbb{1}\left\{X_{i} \geq 0\right\}-1 \geq V_{i}\right\},
\end{array} \quad\left(X_{i}, U_{i}, V_{i}\right)^{\prime} \sim N(0, \Sigma)\right.
\]

where \(\beta_{1}\) and \(\gamma_{1}\) are to be varied across simulation sets, \(\Sigma_{11}=\sigma_{X}^{2}=0.1781742^{2}, \Sigma_{22}=\sigma_{U}^{2}=0.1295^{2}, \Sigma_{33}=\sigma_{V}^{2}=0.5^{2}\), \(\Sigma_{12}=\rho_{X U} \cdot \sigma_{X} \cdot \sigma_{U}=0.25 \cdot 1.0^{2}, \Sigma_{13}=\rho_{X V} \cdot \sigma_{X} \cdot \sigma_{V}=0.0 \cdot 1.0 \cdot 0.5\), and \(\Sigma_{23}=\rho_{U V} \cdot \sigma_{U} \cdot \sigma_{V}=0.25 \cdot 1.0 \cdot 0.5\). We choose the numbers, \(\sigma_{X}^{2}=0.1781742^{2}\) and \(\sigma_{U}^{2}=0.1295^{2}\), so that these variances match the corresponding variances in the data generating processes in the simulation studies by Calonico et al. (2014). Furthermore, the polynomial part \(\mu\left(X_{i}\right)\) of the outcome equation is defined due to Lee (2008):

\[
\mu(x)= \begin{cases}1.27 x+7.18 x^{2}+20.21 x^{3}+21.54 x^{4}+7.33 x^{5} & \text { if } x<0 \\ 0.84 x-3.00 x^{2}+7.99 x^{3}-9.01 x^{4}+3.56 x^{5} & \text { if } x \geq 0\end{cases}
\]

also following the simulation studies by Calonico et al. (2014). Observe under the above data generating process that

\[
\theta \text {-th conditional quantile treatment effect at } x=0 \text { is } \beta_{1}+\gamma_{1} F_{U \mid X}^{-1}(\theta \mid 0) \text {. }
\]

We set \(\Theta^{\prime \prime}=[a, 1-a]=[0.20,0.80]\) as the set of quantiles on which we conduct uniform inference. We use a grid with the interval size of 0.02 to approximate the continuum \(\Theta^{\prime \prime}\) for numerical evaluation of functions defined on \(\Theta^{\prime \prime}\). Similarly, we use a grid with interval size of 0.02 to approximate the continuum \(\mathscr{Y}\) for numerical evaluation of functions defined on \(\mathscr{Y}\). First, we simulate the \(95 \%\) test for the null hypothesis \(H_{0}: \tau_{F Q R D}\left(\theta^{\prime \prime}\right)=0 \forall \theta^{\prime \prime} \in[a, 1-a]\) of uniform treatment nullity using the procedure described in Section 4.5. Second, we simulate the \(95 \%\) test for the null hypothesis \(H_{0}: \tau_{\text {FQRD }}\left(\theta^{\prime \prime}\right)=\tau_{\mathrm{FQRD}}\left(\theta^{\prime \prime \prime}\right) \forall \theta^{\prime \prime}, \theta^{\prime \prime \prime} \in[a, 1-a]\) of treatment homogeneity using the procedure described in Section 4.5. Third, we simulate the uniform coverage probability of the true quantile treatment effects \(\tau_{F Q R D}(\cdot)\) using the uniform confidence band constructed in Section 4.5.

Table 1 shows simulated acceptance and coverage probabilities based on 2,500 multiplier bootstrap replications for 2,500 Monte Carlo replications for each of the sample sizes \(n=500,1,000,1,500\), and 2,000. Panel (A) reports results for the test of uniform treatment nullity, panel (B) shows results for the test of treatment homogeneity, and panel (C) shows results for the uniform coverage probability. The left column groups (I) present results across alternative values of \(\beta_{1} \in\{0.00,0.05,0.10,0.15,0.20\}\) while fixing \(\gamma_{1}=0\). The case of \(\beta_{1}=0.00\) evaluates the size in (A), whereas the cases of \(\beta_{1} \in\{0.05,0.10,0.15,0.20\}\) evaluate the power in (A). All of the cases of \(\beta_{1} \in\) \(\{0.00,0.05,0.10,0.15,0.20\}\) evaluate the coverage in (B) and (C). The right column groups (II) present results across alternative values of \(\gamma_{1} \in\{0.00,0.25,0.50,0.75,1.00\}\) while fixing \(\beta_{1}=0\). The case of \(\gamma_{1}=0.00\) evaluates the coverage in (A) and (B), whereas the cases of \(\gamma_{1} \in\{0.25,0.50,0.75,1.00\}\) evaluate the power in (A) and (B). All of the cases of \(\beta_{1} \in\{0.00,0.25,0.50,0.75,1.00\}\) evaluate the coverage in (C). The nominal acceptance probability is \(95 \%\).

In view of the columns for \(\beta_{1}=0.00\) and \(\gamma_{1}=0.00\), we confirm the correct size. Furthermore, the simulated and coverage probabilities under \(\beta_{1}=0\) and \(\gamma_{1}=0\) approach the nominal probability as the sample size increases. The acceptance probability in panel (A) decreases as \(\beta_{1}\) or \(\gamma_{1}\) deviates away from zero, which is consistent with the fact that the joint treatment nullity is violated by \(\beta_{1} \neq 0\) or \(\gamma_{1} \neq 0\). The acceptance probability in panel (B) stays roughly constant as \(\beta_{1}\) deviates away from zero, but it decreases as \(\gamma_{1}\) deviates away from zero. This result is consistent with the fact that the treatment homogeneity is retained for any value of \(\beta_{1}\), but is violated for \(\gamma_{1} \neq 0\). The uniform coverage probability in panel (C) keeps the nominal size across all the values of \(\beta_{1}\) and \(\gamma_{1}\), which evidences the effectiveness of the uniform confidence bands across alternative data generating processes in the presence of nontrivial and/or heterogeneous treatment effects.

To assess the sensitivity of the proposed inference procedure to weak jumps, we conduct additional simulations with a sequence of values of \(\Sigma_{33} \in\left\{2^{0}, 2^{1}, 2^{2}, 2^{3}, 2^{4}, 2^{5}, 2^{6}\right\}\) while fixing \(\beta_{1}=\gamma=1=0\). Table 2 shows simulated acceptance and coverage probabilities based on 2,500 multiplier bootstrap replications for 2,500 Monte Carlo replications for each of the sample sizes \(n=500,1,000,1,500\), and 2,000. Panel (A) reports results for the test of uniform treatment nullity, and panel (B) shows results for the test of treatment homogeneity. Results for (C) the uniform coverage probability are omitted because they are the same as the results of (A) under \(\beta_{1}=\gamma_{1}=0\). Notice that the size becomes smaller and deviates away from the nominal size as the jump becomes weak (i.e., as \(\Sigma_{33}\) increases). This observation of size distortions under weak jumps is consistent with prior study, e.g., Feir et al. (2016). Motivated by these results, we present an extended theory of inference with robustness against the possibility of no or weak jumps in Section 7.2.

Table 1\\
(A) Simulated acceptance probabilities for uniform treatment nullity, (B) simulated acceptance probabilities for treatment homogeneity under the fuzzy quantile RDD, and (C) uniform coverage probability of the true quantile treatment effects by the uniform confidence bands. The left column groups (I) present results across alternative values of \(\beta_{1} \in\{0.00,0.05,0.10,0.15,0.20\}\) while fixing \(\gamma_{1}=0\). The right column groups (II) present results across alternative values of \(\gamma_{1} \in\{0.00,0.25,0.50,0.75,1.00\}\) while fixing \(\beta_{1}=0\). The nominal acceptance probability under the null hypothesis is \(95 \%\).

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|l|}{(I) (A) Joint Treatment Nullity} & \multicolumn{6}{|l|}{(II) (A) Joint Treatment Nullity} \\
\hline
\multirow[t]{2}{*}{\(n\)} & \multicolumn{5}{|l|}{\(\beta_{1}\)} & \multirow[t]{2}{*}{\(n\)} & \multicolumn{5}{|l|}{\(\gamma_{1}\)} \\
\hline
 & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 &  & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\
\hline
500 & 0.969 & 0.874 & 0.709 & 0.532 & 0.362 & 500 & 0.969 & 0.956 & 0.942 & 0.934 & 0.922 \\
\hline
1000 & 0.968 & 0.823 & 0.559 & 0.302 & 0.131 & 1000 & 0.968 & 0.941 & 0.916 & 0.867 & 0.800 \\
\hline
1500 & 0.957 & 0.784 & 0.422 & 0.149 & 0.045 & 1500 & 0.957 & 0.944 & 0.877 & 0.802 & 0.708 \\
\hline
2000 & 0.957 & 0.725 & 0.318 & 0.082 & 0.014 & 2000 & 0.957 & 0.927 & 0.852 & 0.747 & 0.617 \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{6}{|l|}{(I) (B) Treatment Homogeneity} & \multicolumn{6}{|l|}{(II) (B) Treatment Homogeneity} \\
\hline
\multirow[t]{2}{*}{\(n\)} & \multicolumn{5}{|l|}{\(\beta_{1}\)} & \multirow[t]{2}{*}{\(n\)} & \multicolumn{5}{|l|}{\(\gamma_{1}\)} \\
\hline
 & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 &  & 0.00 & 0.25 & 0.50 & 0.75 & 1.00 \\
\hline
500 & 0.989 & 0.984 & 0.988 & 0.986 & 0.987 & 500 & 0.989 & 0.966 & 0.928 & 0.873 & 0.828 \\
\hline
1000 & 0.980 & 0.979 & 0.979 & 0.979 & 0.985 & 1000 & 0.980 & 0.939 & 0.842 & 0.729 & 0.591 \\
\hline
1500 & 0.968 & 0.973 & 0.972 & 0.977 & 0.977 & 1500 & 0.968 & 0.913 & 0.770 & 0.606 & 0.438 \\
\hline
2000 & 0.963 & 0.962 & 0.966 & 0.970 & 0.967 & 2000 & 0.963 & 0.884 & 0.705 & 0.503 & 0.311 \\
\hline
\end{tabular}
\end{center}

(I) (C) Uniform Coverage Probability

\begin{center}
\begin{tabular}{llllll}
\hline
\(n\) & \(\beta_{1}\) &  &  &  &  \\
\cline { 2 - 6 }
 & 0.00 & 0.05 & 0.10 & 0.15 & 0.20 \\
\hline
500 & 0.969 & 0.966 & 0.969 & 0.966 & 0.970 \\
1000 & 0.968 & 0.965 & 0.965 & 0.969 & 0.976 \\
1500 & 0.957 & 0.967 & 0.966 & 0.962 & 0.967 \\
2000 & 0.957 & 0.953 & 0.965 & 0.968 & 0.971 \\
\hline
\end{tabular}
\end{center}

Table 2\\
(A) Simulated acceptance probabilities for uniform treatment nullity, (B) simulated acceptance probabilities for treatment homogeneity under the fuzzy quantile RDD for a sequence of weakening jumps. The results are displayed for \(\Sigma_{33} \in\left\{2^{0}, 2^{1}, 2^{2}, 2^{3}, 2^{4}, 2^{5}, 2^{6}\right\}\). The parameter values are fixed at \(\beta_{1}=\gamma_{1}=0\). The nominal acceptance probability under the null hypothesis is \(95 \%\). The results for ( C ) uniform coverage probability of the true quantile treatment effects by the uniform confidence bands are omitted because they produce the same numbers as (A) under \(\beta_{1}=\gamma_{1}=0\).\\
(A) Joint Treatment Nullity

\begin{center}
\begin{tabular}{llllllll}
\hline
\(n\) & \(\Sigma_{33}\) &  &  &  &  &  \\
\cline { 2 - 7 }
 & \(2^{0}\) & \(2^{1}\) & \(2^{2}\) & \(2^{3}\) & \(2^{4}\) & \(2^{5}\) \\
\hline
500 & 0.968 & 0.984 & 0.984 & 0.985 & 0.986 & 0.989 \\
1000 & 0.962 & 0.969 & 0.979 & 0.984 & 0.984 & 0.983 \\
1500 & 0.956 & 0.962 & 0.967 & 0.976 & 0.980 & 0.982 \\
2000 & 0.957 & 0.955 & 0.957 & 0.972 & 0.977 & 0.979 \\
\hline
\end{tabular}
\end{center}

(B) Treatment Homogeneity

\begin{center}
\begin{tabular}{llllllll}
\hline
\(n\) & \(\Sigma_{33}\) &  &  &  &  &  \\
\cline { 2 - 7 }
 & \(2^{0}\) & \(2^{1}\) & \(2^{2}\) & \(2^{3}\) & \(2^{4}\) & 0.990 \\
\hline
500 & 0.990 & 0.996 & 0.992 & 0.990 & 0.988 & 0.992 \\
1000 & 0.982 & 0.991 & 0.995 & 0.993 & 0.992 & 0.991 \\
1500 & 0.974 & 0.992 & 0.994 & 0.992 & 0.991 & 0.993 \\
2000 & 0.977 & 0.986 & 0.991 & 0.993 & 0.993 & 0.991 \\
\hline
\end{tabular}
\end{center}

\section*{6. Empirical illustration}
In this section, we apply our method of robust uniform inference for the fuzzy quantile RDD (Sections 3 and 4.5). Using RDD with a birth-day cutoff eligibility rule for the Oklahoma pre-K program, Gormley Jr. et al. (2005) find significant positive effects of cognitive development on average test scores. They also find that the average effects are positive among sub-samples of students in lower socio-economic status. Following up the latter finding, Frandsen et al. (2012) provide additional evidence that these effects are positive among the lower end of the distribution via estimated quantile treatment effects. We apply our method of constructing robust uniform confidence bands to complement the findings by Frandsen et al. (2012).\\
\includegraphics[max width=\textwidth, center]{2025_02_11_5e26b331b95b59abcde1g-14(1)}

Applied Problems\\
\includegraphics[max width=\textwidth, center]{2025_02_11_5e26b331b95b59abcde1g-14}

Fig. 1. The estimated local quantile treatment effects of the pre-K programs on scores on the three sub-tests of the Woodcock-Johnson tests, and their \(90 \%\) uniform confidence bands.

The data consist of a sample of 4,710 incoming Tulsa Public Schools kindergartners and pre-K participants for the 2003-2004 school year. The main variables used in this data are the birth date ( \(X\) ), an indicator for participation in the pre-K program in the previous year ( \(D^{*}\) ), and scores on the Woodcock-Johnson sub-tests ( \(Y^{*}\) ): Letter-word, Spelling, and Applied Problems. The implementation procedure follows the guideline provided in Section 4.5 - or the overview in Section 3 - as well as the additional first-stage estimators suggested in Appendix A. 6 and the bandwidth selection procedure suggested in Appendix F.

Fig. 1 plots the estimated local quantile treatment effects of the pre-K programs on scores on the three sub-tests of the Woodcock-Johnson tests. The figure shows the point estimates for each quantile \(\theta\) by black curves. It also shows the \(90 \%\) uniform confidence bands based on our proposed procedure.

The qualitative patterns of our results resemble those of Frandsen et al. (2012) for each of the three sub-tests. Not surprisingly, our \(90 \%\) confidence bands are wider than the \(90 \%\) point-wise confidence intervals obtained by Frandsen et al. (2012), and statistical significance vanishes at some quantile indices. Nonetheless, despite the generally greater widths of confidence bands than confidence intervals, the statistical significance remains for relevant quantile indices. In particular, as in Frandsen et al. (2012), we continue to conclude that the program succeeded in significantly raising the lower end of the distribution of test scores, especially for the Applied Problems sub-test, which is consistent with Gormley Jr. et al.'s (2005) finding that estimated average effects are larger for children with disadvantaged socio-economic status.

\section*{7. Extensions}
In this section, we discuss three directions for extending the baseline model and method presented in Section 4. These extensions include cluster-robust inference (Section 7.1), inference with robustness against no or weak jumps or kinks (Section 7.2), and augmented models with observed covariates (Section 7.3).

\subsection*{7.1. Cluster robust inference}
In applications, researchers may encounter situations where data are cluster sampled and thus the i.i.d. assumption may be implausible. Cluster-robust standard errors for sharp/fuzzy mean RDD are studied by Bartalotti and Brummet (2017) and Calonico et al. (2018b). In this section, we show that Theorems 1 and 2 can be generalized to the cases of cluster sampled data. Suppose that a researcher observes \(\left\{\left(Y_{i}, D_{i}, X_{i}\right): i \in C_{g}, g \in\{1, \ldots, G\}\right\}\), where each \(C_{g} \subset\{1, \ldots, n\}\) is an index set with cardinality \(\left|C_{g}\right| \leq \bar{N}\) for an \(\bar{N} \in \mathbb{N}\) independent of \(G\) and \(g, \sum_{g=1}^{G}\left|C_{g}\right|=n\), and \(C_{g} \cap C_{g^{\prime}}=\emptyset\) whenever \(g \neq g^{\prime}\). Observations within the same cluster \(g\) can be arbitrarily dependent, while any two observations across different clusters are independent. Denote \(\bar{f}_{X}=\frac{1}{G} \sum_{g=1}^{G} \sum_{i \in C_{g}} f_{X_{i}}\), where \(f_{X_{i}}\) is the density function of \(X_{i}\). To keep our writing and analysis simple, we let \(h_{1, G}\left(\theta_{1}\right)=h_{2, G}\left(\theta_{2}\right)=h_{G}\). The local Wald estimate \(\hat{\tau}\) is calculated according to (4.4) and (4.5) in Section 4 with the bandwidth \(h_{G}\). Note that \(\frac{1}{n} \sum_{i=1}^{n}\) can also be written as \(\frac{1}{n} \sum_{g=1}^{G} \sum_{i \in C_{g}}\). Since \(\frac{c}{\sqrt{G}} \leq \frac{1}{\sqrt{n}} \leq \frac{1}{\sqrt{G}}\) for some \(c>0\), we can rescale estimates by \(\frac{n}{G}\) without loss of generality. We now define the cluster-robust EMP as

\[
\hat{v}_{\xi, G}^{ \pm}(\theta, k)=v!\sum_{g=1}^{G} \xi_{g} \sum_{i \in C_{g}} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \hat{\mathcal{E}}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) r_{p}\left(\frac{x_{i}}{h_{G}}\right) K\left(\frac{X_{i}}{h_{G}}\right) \delta_{i}^{ \pm}}{\sqrt{G h_{G}} \hat{\mathcal{f}}_{X}(0)}
\]

where \(\hat{\mathcal{E}}_{k}=g_{k}-\tilde{\mu}_{k}\) with \(\tilde{\mu}_{k}\) and \(\hat{\bar{f}}_{X}\) denoting first-stage estimators of \(\mu_{k}\) and \(\bar{f}_{X}\), respectively, satisfying the assumption below. Also denote \(\hat{v}_{\xi, G}(\cdot)=\hat{v}_{\xi, G}^{+}(\cdot)-\hat{v}_{\xi, G}^{-}(\cdot)\).

Assumption 5 (Cluster Sampling). Let \(\underline{x}<0<\bar{x}\). (i) (a) For each \(G \in \mathbb{N},\left\{\left(Y_{i}, D_{i}, X_{i}\right): i \in C_{1}\right\}, \ldots,\left\{\left(Y_{i}, D_{i}, X_{i}\right): i \in C_{G}\right\}\) are independent random vectors defined on a probability space ( \(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\) ); (b) For each \(G \in \mathbb{N}\) and for each \(i \in C_{1} \cup \ldots \cup C_{G}, X_{i}\) has a density function \(f_{X_{i}}\) which is continuously differentiable and 1-Lipschitz, and satisfies \(0<f \leq f_{X_{i}} \leq \bar{f}<\infty\) on \([\underline{x}, \bar{x}]\). (ii) For each \(k=1\), 2: (a) the collections of real-valued functions, \(\left\{x \mapsto \mu_{k}\left(x, \theta_{k}\right): \theta_{k} \in \Theta_{k}\right\},\left\{\bar{y} \mapsto g_{1}\left(y, \theta_{1}\right): \theta_{1} \in \Theta_{1}\right\}\), and \(\left\{d \mapsto g_{2}\left(d, \theta_{2}\right): \theta_{2} \in \Theta_{2}\right\}\), are of VC type and are uniformly bounded by \(\bar{M}<\infty\); (b) \(\mu_{k}^{(j)}\) is Lipschitz on \([\underline{x}, 0) \times \Theta_{k}\) and \((0, \bar{x}] \times \Theta_{k}\) for \(j=0,1,2, \ldots, p+1\); (c) for each \(y \in \mathscr{Y}, g_{1}(y, \cdot)\) is left- or right-continuous in each dimension; and similarly, for each \(d \in \mathscr{D}, g_{2}(d, \cdot)\) is left- or right-continuous in each dimension. (d) For each \(G \in \mathbb{N}, i \in\{1, \ldots, n\}\), \(\theta \in \Theta\), it holds that \(E\left[\mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mid\left(X_{j}: j \in C_{g}, i \in C_{g}\right)\right]=E\left[\mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mid X_{i}\right]=0\). (iii) \(h_{G} \rightarrow 0, G h_{G}^{2} \rightarrow \infty\) and \(G h_{G}^{2 p+3} \rightarrow 0\) for some \(h_{0}<\infty\). (iv) (a) \(K:[-1,1] \rightarrow \mathbb{R}^{+}\)is bounded and continuous; (b) \(\{K(\cdot / h): h>0\}\) is of VC type. (c) \(\Gamma_{p}^{ \pm}\)is positive definite. (v) There exist uniformly bounded functions \(\Sigma^{ \pm}:(\Theta \times\{1,2\})^{2} \mapsto \mathbb{R}\) such that \(\frac{1}{G} \sum_{g=1}^{G} E\left[\left(\sum_{i \in C_{g}} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} r_{p}\left(\frac{x_{i}}{h_{G}}\right) \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) K\left(\frac{X_{i}}{h_{G}}\right) \delta_{i}^{ \pm}}{\sqrt{h_{G} \bar{F}_{X}(0)}}\right)\left(\sum_{i \in C_{g}} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} r_{p}\left(\frac{x_{i}}{h_{G}}\right) \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \vartheta\right) K\left(\frac{x_{i}}{h_{G}}\right) \delta_{i}^{ \pm}}{\sqrt{h_{G}} \bar{f}_{X}(0)}\right)^{\prime}\right]=\Sigma^{ \pm}((\theta, k),(\vartheta, l))+O_{p}\left(h_{G}\right)\). (vi)(a) \(\phi\) and \(\psi\) are identity mappings while \(\Upsilon\) is Hadamard differentiable at \(W\) tangentially to a subspace of its domain, with its Hadamard derivative denoted by \(\Upsilon_{W}^{\prime}\). (vii) \(\left\{\xi_{g}\right\}_{g=1}^{G}\) is an independent standard normal random sample defined on \(\left(\Omega^{\xi}, \mathcal{F}^{\xi}, \mathbb{P}^{\xi}\right)\), a probability space that is independent of \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\). (b) \(\inf _{\theta_{2} \in \Theta_{2}}\left|\mu_{2}^{(v)}\left(0^{+}, \theta_{2}\right)-\mu_{2}^{(v)}\left(0^{-}, \theta_{2}\right)\right|>0\). (viii) There exist first stage estimators \(\tilde{\mu}_{k, p}\) and \(\hat{\bar{f}}_{X}\) such that \(\tilde{\mu}_{k, p}\left(x, \theta_{k}\right) \mathbb{1}\left\{\left|x / h_{G}\right| \leq 1\right\}\) is uniformly consistent for \(\mu_{k}\left(x, \theta_{k}\right) \mathbb{1}\left\{\left|x / h_{G}\right| \leq 1\right\}\) on \(([\underline{x}, \bar{x}] \backslash\{0\}) \times \mathbb{T} . \hat{\bar{f}}_{X}(0)\) is consistent for \(\bar{f}_{X}(0)\).

This Assumption 5 for cluster sampling corresponds to Assumption 1, 2, 3 and 4 for the i.i.d. setting. In part (i)(b) of Assumption 5, we restrict the density functions to be 1-Lipschitz, but this particular scale is imposed only for the sake of concise writings in proofs. This number can be replaced by any finite constant. Part (vi)(a) of Assumption 5 imposes more restrictions (namely, the identity \(\phi\) and the identity \(\psi\) ) than the baseline case, but this assumption is made only for the sake of simplicity and concise writing in proofs, and it can be relaxed. We make this simplifying assumption because it is already satisfied by most of the important cases anyway, including the case of FQRD, which is the main focus of this paper, as well as the cases of sharp/fuzzy mean RDD studied in prior work (Bartalotti and Brummet, 2017; Calonico et al., 2018b). Under this assumption for cluster sampling, we obtain the weak convergence and the conditional weak convergence results as stated in the following corollary.

Corollary 2. Suppose Assumption 5 holds, then there exists a zero mean Gaussian process \(\mathbb{G}: \Omega^{x} \mapsto \ell^{\infty}(\mathbb{T})\) such that

\[
\sqrt{G h_{G}^{1+2 v}}[\hat{\tau}(\cdot)-\tau(\cdot)] \rightsquigarrow \Upsilon_{W}^{\prime}\left(\frac{\left[\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right] \mathbb{G}(\cdot, 1)-\left[\mu_{1}^{(v)}\left(0^{+}, \cdot\right)-\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right] \mathbb{G}(\cdot, 2)}{\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)^{2}}\right)(\cdot) .
\]

Furthermore, for the EMP, we have

\[
\begin{aligned}
& \Upsilon_{W}^{\prime}\left(\frac{\left[\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right] \hat{\nu}_{\xi, G}(\cdot, 1)-\left[\mu_{1}^{(v)}\left(0^{+}, \cdot\right)-\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right] \hat{\nu}_{\xi, G}(\cdot, 2)}{\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)^{2}}\right)(\cdot) \\
& \underset{\underset{\xi}{p}}{{ }_{\xi}^{\prime}} \Upsilon_{W}^{\prime}\left(\frac{\left[\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right] \mathrm{G}(\cdot, 1)-\left[\mu_{1}^{(v)}\left(0^{+}, \cdot\right)-\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right] \mathrm{G}(\cdot, 2)}{\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)^{2}}\right)(\cdot) .
\end{aligned}
\]

A proof is found in Appendix A.7.

\subsection*{7.2. Inference with robustness against no or weak jump or kink}
Another practically relevant issue to consider as an extension to our baseline model is the issue of no and weak jumps in regression discontinuity designs. Feir et al. (2016) report significant size distortions in the event of weak jumps in the mean RDD, similarly to weak instrument problems. In addition, we also find size distortions in the event of weak jumps in the FQRD designs - see Section 5. Otsu et al. (2015) and Feir et al. (2016) provide weak-identification-robust methods of inference in regression discontinuity designs. In the spirit of Anderson and Rubin (1949) - also see Kleibergen (2002), Moreira (2003), and Andrews et al. (2006) - a null restriction can provide valid standard errors robustly against the possibility of weak and no jumps (Feir et al., 2016). Adopting this idea, we apply our baseline results of the weak convergence and the conditional weak convergence to non- and weak-identification-robust inference in this section.

Under the null hypothesis \(H_{0}: \tau=\tau^{*}\), we can rewrite (4.1) as

\[
\phi\left(\lim _{x \downarrow 0} \mu_{1}^{(v)}(x, \cdot)\right)(\cdot)-\phi\left(\lim _{x \uparrow 0} \mu_{1}^{(v)}(x, \cdot)\right)(\cdot)=\left(\Upsilon^{-1} \tau^{*}\right)(\cdot)\left[\psi\left(\lim _{x \downarrow 0} \mu_{2}^{(v)}(x, \cdot)\right)(\cdot)-\psi\left(\lim _{x \uparrow 0} \mu_{2}^{(v)}(x, \cdot)\right)(\cdot)\right]
\]

provided that the operator \(\Upsilon\) is invertible. Notice that this characterization allows for a statistical inference without assuming that a nonzero jump or kink exists. To see this, note that \(\psi\left(\lim _{x \downarrow 0} \mu_{2}^{(v)}(x, \cdot)\right)(\cdot)-\psi\left(\lim _{x \uparrow 0} \mu_{2}^{(v)}(x, \cdot)\right)(\cdot)\) no longer appears as a denominator, unlike the original expression (4.1). We use this equality restriction for a statistical inference. A sample-analog statistic can be defined as

\[
\hat{\Xi}_{\tau}(\cdot)=\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)-\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{-}, \cdot\right)\right)(\cdot)-\left(\Upsilon^{-1} \tau\right)(\cdot)\left[\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)-\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{-}, \cdot\right)\right)(\cdot)\right]
\]

Our identification-robust inference procedure is based on the idea that \(\hat{\Xi}_{\tau^{*}}\) should be uniformly close to zero under the null hypothesis \(H_{0}: \tau=\tau^{*}\).

Assumption \(2^{\prime}\) (Conditional Weak Convergence). (i) \(\psi\) and \(\phi\) are Hadamard differentiable at \(\mu_{1}^{(v)}\left(0^{ \pm}, \cdot\right)\) and \(\mu_{2}^{(v)}\left(0^{ \pm}, \cdot\right)\), respectively, tangentially to some subspaces of their domains, with their Hadamard derivatives denoted by \(\phi_{\mu_{1}^{\prime(v)}\left(0^{ \pm}, \cdot\right)}\) and \(\psi_{\mu_{2}^{(v)}\left(0^{ \pm}, \cdot\right)}^{\prime}\), respectively. \(\Upsilon\) is invertible. (ii) \(n h_{n}^{1+2 v} \rightarrow \infty\) as \(n \rightarrow \infty\).

Compared with Assumption 2, this assumption is weaker. Specifically, part (ii) of Assumption 2 has been dropped, and hence we do allow for a possibility of no jump or no kink. Similar lines of arguments to those of Theorems 1 and 2 yield the following weak and conditional weak convergence results robustly against possible non-identification due to a lack of jumps or kinks.

Corollary 3 (Weak and Conditional Weak Convergence). Under Assumptions 1 and \(2^{\prime}\) and the null hypothesis \(H_{0}: \tau=\tau^{*}\), we have \(\sqrt{n h_{n}^{1+2 v}} \hat{\Xi}_{\tau^{*}} \rightsquigarrow \mathbb{G}^{\prime}(\cdot, 1)-\left(\Upsilon^{-1} \tau^{*}\right)(\cdot) \mathbb{G}^{\prime}(\cdot, 2)\), where \(\mathbb{G}^{\prime}\) is defined in the statement of Theorem 1 . If Assumptions 3 and\\
\includegraphics[max width=\textwidth]{2025_02_11_5e26b331b95b59abcde1g-16} of Theorem 2.

\subsection*{7.3. Covariates}
Many applied researchers augment their RDD estimating equations with additional predetermined covariates. Formal justification and asymptotic properties of such a practice for mean sharp/fuzzy RDD/RKD are shown in Calonico et al. (2018b). In this section, we demonstrate that such a practice is also justified in the case of FQRD designs, and then provide a generalization of Corollary 1 for models with covariates.

Suppose that we have a d-dimensional vector of covariates denoted by \(Z_{i}\). Define

\[
\begin{aligned}
& \left(\check{\mu}_{1}\left(0^{+}, y, d\right), \check{\mu}_{1}^{\prime}\left(0^{+}, y, d\right), \check{\mu}_{1}^{\prime \prime}\left(0^{+}, y, d\right), \check{\gamma}_{1+}(y, d)\right)= \\
& \arg \min _{\left(\mu, \mu^{\prime}, \mu^{\prime \prime}, \gamma\right) \in \mathbb{R}^{3+d}} \sum_{i: X_{i}>0}\left(\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d\right\}-\left\{\mu+\mu^{\prime} X_{i}+\frac{\mu^{\prime \prime}}{2!} X_{i}^{2}+Z_{i}^{\prime} \gamma\right\}\right)^{2} \cdot K\left(\frac{X_{i}}{h_{n}}\right),
\end{aligned}
\]

\[
\begin{aligned}
& \left(\check{\mu}_{2}\left(0^{+}, d\right), \check{\mu}_{2}^{\prime}\left(0^{+}, d\right), \check{\mu}_{2}^{\prime \prime}\left(0^{+}, d\right), \check{\gamma}_{2+}(d)\right)= \\
& \arg \min _{\left(\mu, \mu^{\prime}, \mu^{\prime \prime}, \gamma\right) \in \mathbb{R}^{3+d}} \sum_{i: X_{i}>0}\left(\mathbb{1}\left\{D_{i}^{*}=d\right\}-\left\{\mu+\mu^{\prime} X_{i}+\frac{\mu^{\prime \prime}}{2!} X_{i}^{2}+Z_{i}^{\prime} \gamma\right\}\right)^{2} \cdot K\left(\frac{X_{i}}{h_{n}}\right), \text { and } \\
& \check{F}_{Y^{d} \mid C}(y)=\frac{\check{\mu}_{1+}(y, d)-\check{\mu}_{1-}(y, d)}{\check{\mu}_{2+}(d)-\check{\mu}_{2-}(d)}
\end{aligned}
\]

The left-hand-side counterparts of them are defined analogously. The FQRD estimator based on these covariate-augmented local linear estimators is defined by

\[
\check{\tau}_{F Q R D}(\theta)=\check{Q}_{Y^{1} \mid C}(\theta)-\check{Q}_{Y^{0} \mid C}(\theta),
\]

where

\[
\check{Q}_{Y^{d} \mid C}(\theta)=\inf \left\{y: \check{F}_{Y^{d} \mid C}(y) \geq \theta\right\} \quad \text { for each } d \in\{0,1\}
\]

We next introduce the short-hand notations: \(\gamma_{1+}(y, d)=\sigma_{Z+}^{-1} E\left[\left(Z_{i}-\mu_{Z+}\left(X_{i}\right)\right) \mathbb{1}\left\{Y_{i} \leq y, D_{i}=d\right\} \mid X_{i}=0^{+}\right], \gamma_{2+}(d)=\) \(\sigma_{Z+}^{-1} E\left[\left(Z_{i}-\mu_{Z+}\left(X_{i}\right)\right) \mathbb{1}\left\{D_{i}=d\right\} \mid X_{i}=0^{+}\right], \mu_{Z_{+}}(x)=\left[\mu_{Z_{1}+}(x), \ldots, \mu_{Z_{d}+}(x)\right]^{\prime}, \mu_{Z_{+}+}(x)=E\left[Z_{i l} \mid X_{i}=x\right]\), and \(\sigma_{Z+}^{2}=V\left(Z_{i} \mid X_{i}=0^{+}\right)\). Their left-hand-side counterparts are also defined analogously. Consider the following assumptions.

Assumption C. (i) \(\gamma_{1}=\gamma_{1+}=\gamma_{1-}\) and \(\gamma_{2}=\gamma_{2+}=\gamma_{2-}\) on \(\mathscr{Y}_{1} \times\{0,1\}\). (ii) \(\mu_{Z+}=\mu_{Z-}\) on ( \(\left.\underline{x}, 0\right]\) and [ \(0, \bar{x}\) ). (iii) For all \((y, d) \in \mathscr{Y}_{1} \times\{0,1\}, E\left[Z_{i}\left[\mathbb{1}\left\{Y_{i} \leq y, D_{i}=d\right\}, D_{i}\right]^{\prime} \mid X=\cdot\right]\) are continuously differentiable on \((\underline{x}, 0]\) and \([0, \bar{x})\). (iv) \(\mu_{Z+}(\cdot)\) and \(\mu_{Z-}(\cdot)\) are three-times continuously differentiable on \((\underline{x}, 0]\) and \([0, \bar{x}) .(\mathrm{v}) \sigma_{Z \pm}^{2}(\cdot)\) are continuous and invertible on ( \(\underline{x}, 0\) ] and \([0, \bar{x})\).

Assumption \(4^{\prime}\) (First Stage Estimation). \(\ddot{\mu}_{k, p}\left(x, \theta_{k}\right) \mathbb{1}\left\{\left|x / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\) is uniformly consistent for \(\mu_{k}\left(x, \theta_{k}\right) \mathbb{1}\left\{\left|x / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\) on \(([\underline{x}, \bar{x}] \backslash\{0\}) \times \mathbb{T} . \hat{f}_{X}(0)\) is consistent for \(f_{X}(0)\).

With a first-stage estimator satisfying this assumption, we define \(\check{U}_{1 i}=\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d\right\}-\check{\gamma}_{1+}^{\prime}(y, d) \hat{\mu}_{Z+}\left(X_{i}\right)-\) \(\ddot{\mu}_{1}\left(X_{i}, y, d\right)\) and \(\check{U}_{2 i}=\mathbb{1}\left\{D_{i}^{*}=d\right\}-\check{\gamma}_{2+}^{\prime}(d) \hat{\mu}_{Z+}\left(X_{i}\right)-\ddot{\mu}_{2}\left(X_{i}, d\right)\). Define the estimated multiplier processes

\[
\begin{aligned}
& \check{v}_{\xi, n}^{+}(y, d, 1)=\sum_{i: X_{i}>0} \xi_{i} \frac{(100) \cdot\left(\Gamma_{2}^{ \pm}\right)^{-1} \cdot\left(1 \frac{X_{i}}{h_{n}} \frac{X_{i}^{2}}{h_{n}^{2}}\right)^{\prime} \check{U}_{1 i} K\left(\frac{x_{i}}{h_{n}}\right)}{\sqrt{n h_{n}} \hat{f}_{X}(0)} \\
& \text { and } \quad \check{v}_{\xi, n}^{+}(d, 2)=\sum_{i: X_{i}>0} \xi_{i} \frac{(100) \cdot\left(\Gamma_{2}^{ \pm}\right)^{-1} \cdot\left(1 \frac{X_{i}}{h_{n}} \frac{x_{i}^{2}}{h_{n}^{2}}\right)^{\prime} \check{U}_{2 i} K\left(\frac{x_{i}}{h_{n}}\right)}{\sqrt{n h_{n}} \hat{f}_{X}(0)}
\end{aligned}
\]

We now define \(\check{X}_{n}^{\prime}, \check{\mathbb{Y}}_{n}\) and \(\check{\Upsilon}_{W}^{\prime}\) as \(\widehat{\mathbb{X}}_{n}^{\prime}, \widehat{\mathbb{Y}}_{n}\) and \(\widehat{\Upsilon}_{W}^{\prime}\), respectively, but with \(\check{v}_{\xi, n}^{ \pm}\)in place of \(\hat{v}_{\xi, n}^{ \pm}, \check{Q}_{Y^{d} \mid C}\) in place of \(\hat{Q}_{Y^{d} \mid C}\), and \(\check{\mu}_{k, 2}\) in place of \(\hat{\mu}_{k, 2}\).

Corollary 4. Suppose that Assumptions S, K, M, FQRD, C and \(4^{\prime}\) hold. Corollary 1 holds with \(\check{\tau}_{\text {FQRD }}\) and \(\check{\Upsilon}_{W}^{\prime}\left(\check{Y}_{n}\right)\) in place of \(\hat{\tau}_{F Q R D}\) and \(\widehat{\Upsilon}_{W}^{\prime}\left(\widehat{\mathbb{Y}}_{n}\right)\), respectively.

A proof is found in Appendix A.9.\\
Remark 5. While many cases covered in our general framework can also be extended to covariate-augmented estimating equations Ã  la Calonico et al. (2018b), not all the cases can be. The covariate extension is not compatible with the designs in which the operators, \(\phi(\cdot)\) and \(\psi(\cdot)\), are not linear, and hence we did not work with the general unified framework in this subsection. Since the cases of mean sharp/fuzzy RDD/RKD are covered in prior work (Calonico et al., 2018b), we focused on the case of the FQRD design, which is the main design of interest in this paper.

\section*{8. Summary}
The existing literature on robust inference for causal effects in RDD and RKD covers major important cases, including the sharp mean RDD, the fuzzy mean RDD, the sharp mean RKD, the sharp fuzzy RKD, the sharp quantile RDD, and the sharp quantile RKD. These existing methods, however, do not cover uniform inference for CDF and quantile cases in fuzzy designs. Particularly, the existing methods are not able to handle robust uniform inference for quantile treatment effects in the fuzzy RDD, despite its practical relevance, e.g., Clark and Martorell (2014) and Deshpande (2016). In this light, this paper proposes a new general robust inference method that covers the fuzzy quantile RDD in particular, but also all the\\
other cases covered by the existing methods of robust inference. We provide high-level statements for the general result, and also provide more primitive conditions and detailed discussions focusing on the fuzzy quantile RDD.

Monte Carlo simulation studies confirm the theoretical properties for data generating processes calibrated to match real applications. Applying the proposed method to real data, we study causal effects on test outcomes of the Oklahoma pre-K program, following the earlier work by Gormley Jr. et al. (2005) and Frandsen et al. (2012). Despite the generally larger lengths of uniform confidence bands than point-wise confidence intervals, we obtain results qualitatively similar to those of Frandsen et al. (2012).

Finally, we conclude this paper with a guide for practitioners. Our method applies to most, if not all, of the commonly used local Wald estimators, but practitioners may want to know which method they should consider in empirical research. We are not aware yet of theoretical benefits of the multiplier bootstrap method over the analytic method - which is left for future research - and hence analytic methods may be preferred for their computational advantage for applications where such an analytic method is applicable. As such, the analytic method proposed by Calonico et al. (2014) is probably a superior option if a practitioner is interested in mean-regression-based designs. As their method is limited to mean-regression-based designs, one may need to seek alternative methods for quantile-regression-based designs. The pivotal methods of Qu and Yoon (2018) and Chiang and Sasaki (2019) are probably superior to our method if a practitioner is interested in sharp quantile RDD and sharp quantile RKD, respectively. The bootstrap method proposed in this paper, to the best of our knowledge, is the only option if a practitioner is interested in the fuzzy quantile RDD and the fuzzy CDF discontinuity design.

\section*{Appendix A. Mathematical appendix}
The relation \(a \lesssim b\) means that there exists \(C, 0<C<\infty\), such that \(a \leq C b\). For the definitions and notations of what follows, we refer the reader to van der Vaart and Wellner (1996), Kosorok (2003), and GinÃ© and Nickl (2015). For an arbitrary semimetric space ( \(T, d\) ), define the covering number \(N(\epsilon, T, d\) ) to be the minimal number of closed \(d\)-balls of radius \(\epsilon\) required to cover \(T\). For any function \(f: T \times \Omega \mapsto \mathbb{R}\) and any \(w \in \Omega\), denote \(\|f(\cdot, w)\|_{T}=\sup _{t \in T}|f(t, w)|\). We further define the Uniform Entropy Integral \(J(\delta, \mathscr{F}, F)=\sup _{Q} \int_{0}^{\delta} \sqrt{1+\log N\left(\epsilon\|F\|_{Q, 2}, \mathscr{F},\|\cdot\|_{Q, 2}\right)} d \epsilon\), where \(\|\cdot\|_{Q, 2}\) is the \(L 2\) norm with respect to measure \(Q\) and the supremum is taken over all probability measure over ( \(\Omega^{x}, \mathcal{F}^{x}\) ). A class of measurable functions \(\mathscr{F}\) is called a VC (Vapnik-Chervonenkis) type with respect to a measurable envelope \(F\) of \(\mathscr{F}\) if there exist finite constants \(A \geq 1, V \geq 1\) such that for all probability measures \(Q\) on \(\left(\Omega^{x}, \mathcal{F}^{x}\right)\), we have \(N\left(\epsilon\|F\|, \mathscr{F},\|\cdot\|_{Q, 2}\right) \leq\left(\frac{A}{\epsilon}\right)^{V}\), for \(0<\epsilon \leq 1\). We use \(C, C_{1}, C_{2}, \ldots\), to denote constants that are positive and independent of \(n\). The values of \(C\) may change at each appearance but \(C_{1}, C_{2}, \ldots\), are fixed.

\section*{A.1. Uniform Bahadur representation}
We obtain the uniform BR of the local polynomial estimators \(\hat{\mu}_{1, p}^{(v)}\left(0^{ \pm}, \cdot\right)\) and \(\hat{\mu}_{2, p}^{(v)}\left(0^{ \pm}, \cdot\right)\) presented in the following lemma - see Appendix A. 1 for a proof.

Lemma 1 (Uniform Bahadur Representation). Under Assumption 1, we have:

\[
\begin{aligned}
& \sqrt{n h_{k, n}^{1+2 v}\left(\theta_{k}\right)}\left(\hat{\mu}_{k, p}^{(v)}\left(0^{ \pm}, \theta_{k}\right)-\mu_{k}^{(v)}\left(0^{ \pm}, \theta_{k}\right)-h_{k, n}^{p+1-v}\left(\theta_{k}\right) \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \Lambda_{p, p+1}^{ \pm}}{(p+1)!} \mu_{k}^{(p+1)}\left(0^{ \pm}, \theta_{k}\right)\right) \\
= & v!\sum_{i=1}^{n} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) r_{p}\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) K\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{k, n}\left(\theta_{k}\right)} f_{X}(0)}+o_{p}^{x}(1)
\end{aligned}
\]

uniformly for all \(\theta_{k} \in \Theta_{k}\) for each \(k \in\{1,2\}\).\\
Notice that the leading bias terms on the left-hand side of the equations in this lemma are of the \((p+1)\)-st order. Thus, the asymptotic distributions of the Bahadur representation take into account the pth order bias reduction. This property is the key to develop a method of inference which is robust against large bandwidth parameter choices.

Remark 6. It is worth remarking on the relation between this lemma and Theorems 2 and 3 in Frandsen et al. (2012). The proofs of Theorem 2 and 3 in Frandsen et al. (2012) rely on the exact solution of the local linear smoother (see their equation (11)), while ours makes use of the uniform Bahadur representation derived in our Lemma 1. As illustrated in Section 4.2 of Dony et al. (2006), if we let \(\tilde{f}_{n, h, j}=\frac{1}{n h} \sum_{i=1}^{n}\left(\frac{x_{i}-x}{h}\right)^{j} K\left(\frac{x-X_{i}}{h}\right)\) and \(\tilde{r}_{n, h, j}=\frac{1}{n h} \sum_{i=1}^{n} Y_{i}\left(\frac{X_{i}-x}{h}\right)^{j} K\left(\frac{x-X_{i}}{h}\right)\), then the exact solution for the local linear mean regression estimator \((p=1)\) has the form \(\hat{\mu}(x)=\frac{\tilde{f}_{n, h, 2} \tilde{r}_{n, h, 0}-\tilde{f}_{n, h, 1} \tilde{r}_{n, h, 1}}{\tilde{f}_{n, h, 0}, \tilde{f}_{n, h, 2}-\tilde{f}_{n, h, 1}^{2}}\). For the local\\
\includegraphics[max width=\textwidth]{2025_02_11_5e26b331b95b59abcde1g-18} exact solution. Therefore, for kernel estimators with the squared loss, even though it is in principle possible to establish results by working with the exact solution for any order of polynomial, the expression of exact solution becomes rather\\
complex as the order \(p\) increases due to the presence of an inverse factor, whose determinant gives the denominator terms of the above expressions. Further, this illustration is only for mean regressions, and it would add even more complications if we have an index \(\theta\) to keep track of in this inverse factor. By working with its limit \(\Gamma_{p}^{ \pm}\), our uniform Bahadur representation holds with the same unified general expression for any arbitrary order, with any index set that satisfies Assumption 1 (ii) (a). This helps us to allow for robust bias correction of any order in a unified manner, in comparison with the exact solution approach taken by Frandsen et al. (2012).

Proof. In this proof, we will show the first result for the case of \(k=1\) and \(\pm=+\). All the other results can be shown by similar lines of proof. As in Section 1.6 of Tsybakov (2008), the solution to (4.4) can be computed explicitly as \(\hat{\alpha}_{1+, p}\left(\theta_{1}\right)=\) \(\left[\hat{\mu}_{1, p}\left(0^{ \pm}, \theta_{1}\right), \hat{\mu}_{1, p}^{(1)}\left(0^{ \pm}, \theta_{1}\right) h_{1, n}\left(\theta_{1}\right), \ldots, \hat{\mu}_{1, p}^{(p)}\left(0^{ \pm}, \theta_{1}\right) h_{1, n}^{p}\left(\theta_{1}\right) / p!\right]^{\prime}=\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1}\). \(\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) g_{1}\left(Y_{i}, \theta_{1}\right)\right]\) For each data point \(X_{i}>0\), a mean value expansion by Assumption 1 (ii)(b) gives \(g_{1}\left(Y_{i}, \theta_{1}\right)=\mu_{1}\left(X_{i}, \theta\right)+\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)=\mu_{1}(0, \theta)+\mu_{1}^{(1)}\left(0^{+}, \theta_{1}\right) X_{i}+\cdots+\mu_{1}^{(p)}\left(0^{+}, \theta_{1}\right) \frac{X_{i}^{p}}{p!}+\mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) \frac{X_{i}^{(p+1)}}{(p+1)!}+\) \(\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)=r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \alpha_{1+, p}\left(\theta_{1}\right)+\mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{1_{1, n}\left(\theta_{1}\right)}\right)^{(p+1)}}{(p+1)!}+\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\) for an \(x_{n i}^{*} \in\left(0, X_{i}\right]\). Substituting this expansion in the equation above and multiplying both sides by \(\sqrt{n h_{1, n}\left(\theta_{1}\right)} e_{v}^{\prime}\), we obtain

\[
\sqrt{n h_{1, n}\left(\theta_{1}\right)} \hat{\mu}_{1}^{(v)}\left(0^{+}, \theta_{1}\right) h_{1, n}^{v}\left(\theta_{1}\right) / v!=\sqrt{n h_{1, n}\left(\theta_{1}\right)} \mu_{1}^{(v)}\left(0^{+}, \theta_{1}\right) h_{1, n}^{v}\left(\theta_{1}\right) / v!+(a)+(b)
\]

where

\[
\begin{aligned}
(a)= & e_{v}^{\prime}\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1} . \\
& \frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}
\end{aligned}
\]

and

\[
\begin{aligned}
(b)= & e_{v}^{\prime}\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1} . \\
& \frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)
\end{aligned}
\]

We will show stochastic limits of the (a) and (b) terms above.\\
Step 1 First, we consider their common inverse factor. Specifically, we show that

\[
\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1} \xrightarrow{p}\left(\Gamma_{p}^{+}\right)^{-1} / f_{X}(0)
\]

uniformly in \(\theta_{1}\). Note that by Minkowski's inequality

\[
\begin{aligned}
& \left|\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1}-\left(\Gamma_{p}^{+}\right)^{-1} / f_{X}(0)\right|_{\Theta_{1}} \\
& \leq \left\lvert\,\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1}\right. \\
& \quad-\left.E\left[\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1}\right]\right|_{\Theta_{1}} \\
& +\left|E\left[\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]^{-1}\right]-\left(\Gamma_{p}^{+}\right)^{-1} / f_{X}(0)\right|_{\Theta_{1}}
\end{aligned}
\]

where the first term on the right hand side is stochastic, while the second term is deterministic. First, regarding the deterministic part, \(E\left[\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}^{\prime}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\right]=f_{X}(0) \Gamma_{p}^{+}+O\left(h_{1, n}\left(\theta_{1}\right)\right)\) holds uniformly by Assumption 1(i), (iii), and (iv). For the stochastic part, we will show that each entry of such a matrix converges in probability with respect to \(\mathbb{P}^{x}\) uniformly. We may write \(\mathscr{F}_{r}=\left\{x \mapsto \mathbb{1}\{x \geq 0\} K(a x)(a x)^{r} \mathbb{1}\{a x \in[-1,1]\}: a>1 / h_{0}\right\}\) and \(\mathscr{F}_{n, r}=\left\{x \mapsto \mathbb{1}\{x \geq 0\} K\left(x / h_{1, n}\left(\theta_{1}\right)\right)\left(x / h_{1, n}\left(\theta_{1}\right)\right)^{r}: \theta_{1} \in \Theta_{1}\right\}\) for each integer \(r\) such that \(0 \leq r \leq 2 p\). By Lemma 6 , each \(\mathscr{F}_{r}\) is of VC type (Euclidean) with envelope \(F=\|K\|_{\infty}\) under Assumption 1 (iii) and (iv)(a) and (b), i.e., there exist constants \(k, v<\infty\) such that \(\sup _{Q} \log N\left(\epsilon\|F\|_{Q, 2}, \mathscr{F}_{r},\|\cdot\|_{Q, 2}\right) \leq\left(\frac{k}{\epsilon}\right)^{v}\) for \(0<\epsilon \leq 1\) and for all probability measures \(Q\) supported on\\
\([\underline{x}, \bar{x}]\). This implies \(J\left(1, \mathscr{F}_{r}, F\right)=\sup _{Q} \int_{0}^{1} \sqrt{1+\log N\left(\epsilon\|F\|_{Q, 2}, \mathscr{F}_{r},\|\cdot\|_{Q, 2}\right)} d \epsilon<\infty\). Since \(F \in L_{2}(P)\), we can apply Theorem 5.2 of Chernozhukov et al. (2014) to obtain \(E\left[\sup _{f \in \mathscr{F}_{r}}\left|\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\left(f\left(X_{i}\right)-E f\right)\right|\right] \leq C\left\{J\left(1, \mathscr{F}_{r}, F\right)\|F\|_{P, 2}+\frac{\|K\|_{\infty} J^{2}\left(1, \mathscr{F}_{r}, F\right)}{\delta^{2} \sqrt{n}}\right\}<\infty\) for a universal constant \(C>0\). Note that \(\mathscr{F}_{n, r} \subset \mathscr{F}_{r}\) for all \(n \in \mathbb{N}\). Multiplying both sides by \(\left[\sqrt{n} h_{1, n}\left(\theta_{1}\right)\right]^{-1}\) yields

\[
\begin{aligned}
& E\left[\sup _{\theta_{1} \in \Theta_{1}}\left|\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{s}-\frac{1}{n h_{1, n}\left(\theta_{1}\right)} \sum_{i=1}^{n} E\left[\delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{s}\right]\right|\right] \\
\leq & \frac{1}{\sqrt{n} h_{1, n}\left(\theta_{1}\right)} C\left\{J\left(1, \mathscr{F}_{r}, F\right)\|F\|_{P, 2}+\frac{B J^{2}\left(1, \mathscr{F}_{r}, F\right)}{\delta^{2} \sqrt{n}}\right\}=O\left(\frac{1}{\sqrt{n} h_{n}}\right)
\end{aligned}
\]

The last line goes to zero uniformly under Assumption 1(iii). Finally, Markov's inequality gives the uniform convergence of the stochastic part at the rate \(O_{p}^{x}\left(\frac{1}{\sqrt{n} h_{n}}\right)\). Consequently, we have the uniform convergence in probability for each \(r \in\{0, \ldots, 2 p\}\). Assumption 1 (iv)(c) and the continuous mapping theorem concludes (A.1).

Step 2 For term (a), we may again use Minkowski's inequality under the supremum norm as in Step 1 to decompose

\[
\begin{aligned}
& \left|\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}-0\right|_{\Theta_{1}} \\
& \leq \left\lvert\, \frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\right. \\
& \quad-\left.E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\right]\right|_{\Theta_{1}} \\
& +\left|E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\right]-0\right|_{\Theta_{1}}
\end{aligned}
\]

Under Assumption 1(i),(ii)(b),(iii),(iv)(a), standard calculations show that the deterministic part

\[
\begin{aligned}
& E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\right] \\
= & \frac{h_{1, n}^{p+1}\left(\theta_{1}\right) \Lambda_{p, p+1}^{+}}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}(p+1)!} f_{X}(0) \mu_{1}^{(p+1)}\left(0^{+}, \theta_{1}\right)+O\left(\frac{h_{n}^{p+2}}{\sqrt{n h_{1, n}}}\right)=O\left(\sqrt{\frac{h_{n}^{2 p+1}}{n}}\right)
\end{aligned}
\]

uniformly in \(\theta_{1}\).\\
As for the stochastic part, first note that under Assumption 1(ii), we know that for a Lipschitz constant \(L\) such that \(0 \leq L<\infty\), it holds uniformly in \(\theta_{1}\) that

\[
\begin{aligned}
& \left|\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\left[\mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right)-\mu_{1}^{(p+1)}\left(0^{+}, \theta_{1}\right)\right]\right| \\
& \lesssim n \max _{1 \leq i \leq n}\left|\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\left[\mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right)-\mu_{1}^{(p+1)}\left(0^{+}, \theta_{1}\right)\right]\right| \\
& \lesssim \frac{n \max _{1 \leq i \leq n}\left|\mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right)-\mu_{1}^{(p+1)}\left(0^{+}, \theta_{1}\right)\right| h_{1, n}^{p+1}\left(\theta_{1}\right)}{\sqrt{n h_{n}}} \leq \frac{n L h_{n}^{p+2}}{\sqrt{n h_{n}}}=O_{p}^{x}\left(\sqrt{n h_{n}^{2 p+3}}\right) .
\end{aligned}
\]

The second inequality holds since \(\delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \frac{\left(\frac{x_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\) is bounded under Assumption 1(iii) and (iv)(a), while the third one is due to Assumption 1(ii). It is then sufficient to consider the asymptotic behavior of \(\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\) \(\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(0^{+}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(\frac{x_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)^{p+1}}{(p+1)!}\). Note that \(r_{p}(x / h)(x / h)^{p+1}=\left[(x / h)^{p+1}, \ldots,(x / h)^{2 p+1}\right]^{\prime}\). So we may let

\[
\mathscr{F}_{s}=\left\{x \mapsto \mathbb{1}\{x \geq 0\} K(a x)(a x)^{s+p+1} \mu_{1}^{p+1}\left(0^{+}, \theta_{1}\right) \mathbb{1}\{a x \in[-1,1]\}: a \geq 1 / h_{0}, \theta_{1} \in \Theta_{1}\right\} \quad \text { and }
\]

\[
\mathscr{F}_{n, s}=\left\{x \mapsto \mathbb{1}\{x \geq 0\} K\left(x / h_{1, n}\left(\theta_{1}\right)\right)\left(x / h_{1, n}\left(\theta_{1}\right)\right)^{s+p+1} \mu_{1}^{p+1}\left(0^{+}, \theta_{1}\right): \theta_{1} \in \Theta_{1}\right\}
\]

for each integer \(s\) such that \(0 \leq s \leq p\). Since \((a x)^{s+p+1} \mathbb{1}\{a x \in[-1,1]\}\) is Lipschitz continuous for each \(a \geq 1 / h_{0}\) and bounded by 1 , it is of VC type by Lemma 5 . We then apply Lemma 6 to show that for each \(0 \leq s \leq p\), \(\mathscr{F}_{s}\) is a VC type class\\
with envelope \(F_{s}(x)=\|K\|_{\infty} \int_{\mathcal{Y} \times \mathcal{D}} F_{\epsilon}\left(y, d^{\prime}, x\right) d \mathbb{P}^{x}\left(y, D=d^{\prime} \mid x\right)\), which is integrable under Assumption 1(ii)(b) and (iv)(a). By Assumption 1 (iii) and (iv)(a), \(\mathscr{F}_{n, s} \subset \mathscr{F}_{s}\) for all \(n \in \mathbb{N}\), thus an argument similar to the one above with Theorem 5.2 of Chernozhukov et al. (2014) shows that for each \(0 \leq s \leq p\)

\[
E\left[\sup _{f \in \mathscr{F}_{S}}\left|\frac{1}{\sqrt{n}} \sum_{i=1}^{n}\left(f\left(X_{i}\right)-E f\left(X_{i}\right)\right)\right|\right]=O(1)
\]

To prove the uniform convergence of the part of (a) outside the inverse sign, it suffices to show that for each \(s\) \(\sup _{f \in \mathscr{F}_{s}}\left|\frac{1}{\sqrt{n h_{n}}} \sum_{i=1}^{n}\left(f\left(X_{i}\right) h_{n}^{p+1}-E\left[f\left(X_{i}\right) h_{n}^{p+1}\right]\right)\right| \xrightarrow[x]{p} 0\). Multiplying both sides of Eq. (A.2) by \(h_{n}^{p+1} / \sqrt{h_{n}}\) and applying Markov's inequality as in Step 1, we have

\[
\sup _{f \in \mathscr{F}_{s}}\left|\frac{1}{\sqrt{n h_{n}}} \sum_{i=1}^{n}\left(f\left(X_{i}\right) h_{n}^{p+1}-E f\left(X_{i}\right) h_{n}^{p+1}\right)\right|=O_{p}^{x}\left(\sqrt{\frac{h_{n}^{2 p+1}}{n}}\right)
\]

which converges to zero in probability \(\left(\mathbb{P}^{x}\right)\) under Assumption 1(iii). To conclude, we have shown

\[
\begin{aligned}
& \frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{1, n}^{p+1}\left(\theta_{1}\right) \frac{\left(X_{i} / h_{l, n}\left(\theta_{1}\right)\right)^{p+1}}{(p+1)!} \\
= & O\left(\sqrt{\frac{h_{n}^{2 p+1}}{n}}\right)+O_{p}^{X}\left(\sqrt{n h_{n}^{2 p+3}}\right)+O_{p}^{x}\left(\sqrt{\frac{h_{n}^{2 p+1}}{n}}\right)
\end{aligned}
\]

uniformly in \(\theta_{1}\). Finally, the continuous mapping theorem gives \((a) \xrightarrow{p}\left(O\left(h_{n}\right)+O_{p}^{X}\left(\frac{1}{\sqrt{n} h_{n}}\right)\right)\left(O\left(\sqrt{\frac{h_{n}^{2 p+1}}{n}}\right)+O_{p}^{\chi}\left(\sqrt{n h_{n}^{2 p+3}}\right)+\right.\) \(\left.O_{p}^{\chi}\left(\sqrt{\frac{h_{n}^{2 p+1}}{n}}\right)\right)=o_{p}^{\chi}(1)\) uniformly in \(\theta_{1}\).

Step 3 For term (b), Minkowski's inequality under the supremum norm implies

\[
\begin{aligned}
& \left|\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right|_{\Theta_{1}} \\
& \leq \left\lvert\, \frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right. \\
& \quad-\left.E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right]\right|_{\Theta_{1}} \\
& +\left|E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right]-0\right|_{\Theta_{1}} .
\end{aligned}
\]

By construction, local polynomial regression satisfies \(E\left[\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mid X\right]=E\left[g\left(Y_{i}, \theta_{1}\right)-\mu_{1}\left(X_{i}, \theta_{1}\right) \mid X\right]=0\), thus by the law of iterated expectations, we have

\[
\begin{aligned}
& E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right] \\
= & E\left[\frac{1}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) E\left[\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mid X\right]\right]=0 .
\end{aligned}
\]

Therefore, in light of (A.1), in order to show (b) \(\xrightarrow{p} \sum_{i=1}^{n} \frac{\left(\Gamma_{p}^{+}\right)^{-1} \delta_{i}^{+} K\left(\frac{x_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{x_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)}{\left.\sqrt{n h_{1, n}\left(\theta_{1}\right)}\right)_{X}(0)}\) uniformly in \(\theta_{1}\), it remains to be shown that for each coordinate \(0 \leq s \leq p\)

\[
\sup _{\theta_{1} \in \Theta_{1}}\left|\frac{e_{s}^{\prime}}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right|=O_{p}^{x}(1)
\]

First note that, under Assumptions 1(i), (ii) (c), (iii), (iv),

\[
\begin{aligned}
& \sup _{\theta_{1} \in \Theta_{1}} E\left[\frac{e_{s}^{\prime}}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right]^{2} \\
= & \sup _{\theta_{1} \in \Theta_{1}} \int_{\mathbb{R}_{+}} \sigma_{11}\left(\theta_{1}, \theta_{1} \mid 0^{+}\right) K^{2}(u) e_{s}^{\prime} r_{p}(u) r_{p}^{\prime}(u) e_{s} f_{X}\left(0^{+}\right) d u+O_{p}^{x}\left(h_{n}\right) \\
\leq & f_{X}(0) e_{s}^{\prime} \Psi_{p}^{+} e_{\theta_{1}} \sup _{\theta_{1} \in \Theta_{1}} \sigma_{11}\left(\theta_{1}, \theta_{1} \mid 0^{+}\right)+O_{p}^{X}\left(h_{n}\right) \lesssim f_{X}(0) e_{s}^{\prime} \Psi_{p}^{+} e_{s}+O_{p}^{X}\left(h_{n}\right),
\end{aligned}
\]

where the right hand side is bounded and does not depend on \(\theta_{1}\). Using Markov's inequality, we know that for some constant that does not depend on \(n\), for each \(M>0\)

\[
\mathbb{P}^{x}\left(\left|\frac{e_{s}^{\prime}}{\sqrt{n h_{1, n}\left(\theta_{1}\right)}} \sum_{i=1}^{n} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right|>M\right) \leq \frac{C\left[f_{X}(0) e_{s}^{\prime} \Psi_{p}^{+} e_{s}+h_{n}\right]}{M^{2}} .
\]

Since the right hand side can be made arbitrarily small by choosing \(M\), it is bounded in probability uniformly in \(\theta_{1}\), which concludes the proof.

Remark 7. As an anonymous referee pointed out, it is worth discussing the proof of Lemma 1 in comparison to Einmahl and Mason (2000). The main difference between their results and our convergence of stochastic parts is that the rates are different; the framework of Einmahl and Mason (2000) is uniform in both \(x\) and a set of functions. Therefore, their rate has an extra \(\log h_{n}\) factor and thus the estimator based on their result does not converge weakly to a Gaussian process.

In addition, their results are about the uniform convergence of a kernel type estimator to its expectation, which can be considered counterparts of the stochastic parts in Steps 2 and 3 in our proof of Lemma 1. This is insufficient, and we also need to handle the asymptotic bias terms since our uniform Bahadur representation is only an approximation. Note that much of our proof for Lemma 1 is about establishing the uniform validity of such an approximation.

\section*{A.2. Auxiliary lemmas for the general result}
Since we are working on two probability spaces, \(\left(\Omega^{x}, \mathcal{F}^{x}, \mathbb{P}^{x}\right)\) and \(\left(\Omega^{\xi}, \mathcal{F}^{\xi}, \mathbb{P}^{\xi}\right)\), we use the following notations to clarify the sense of various modes of convergence and expectations. We let \(\xrightarrow{p}\) denote the convergence in probability with respect to the probability measure \(\mathbb{P}^{\bullet}\), let \(E_{\xi \mid x}\) denote the conditional expectation with respect to the product probability measure \(\mathbb{P}^{x} \times \mathbb{P}^{\xi}\) given the events in \(\mathcal{F}^{x}\), and let \(E_{x}\) denote the expectation with respect to the probability measure \(\mathbb{P}^{x}\). Following Section 1.13 of van der Vaart and Wellner (1996), we define the conditional weak convergence in probability, or convergence of the conditional limit laws of bootstraps, denoted by \(X_{n} \underset{\xi}{p} X\), by \(\sup _{h \in B L_{1}}\left|E_{\xi \mid x} h\left(X_{n}\right)-E h(X)\right| \xrightarrow[x]{p} 0\), where \(B L_{1}\) is the set of functions with Lipschitz constant and supremum norm bounded by 1 . We state and prove the following lemma, which can be seen as a conditional weak convergence analogy of Theorem 18.10 (iv) of van der Vaart (1998).

The following lemma is used for the purpose of bounding estimation errors in the EMP to approximate the MP.\\
Lemma 2. Let \(\left(\Omega^{x} \times \Omega^{\xi}, \mathcal{F}^{x} \otimes \mathcal{F}^{\xi}, \mathbb{P}^{x \times \xi}\right)\) be the product probability space of \(\left(\Omega^{x}, \mathcal{F}^{x}, P^{x}\right)\) and \(\left(\Omega^{\xi}, \mathcal{F}^{\xi}, P^{\xi}\right)\), where \(\mathcal{F}^{x} \otimes \mathcal{F}^{\xi}\) stands for the product sigma field of \(\mathcal{F}^{x}\) and \(\mathcal{F}^{\xi}\). For a metric space \(\left(\mathbb{T}, d\right.\), consider \(X_{n}, Y_{n}, X: \Omega^{x} \times \Omega^{\xi} \rightarrow \mathbb{T}, n=1,2, \ldots\). If \(X_{n} \underset{\xi}{\stackrel{p}{\longrightarrow}} X\) and \(d\left(Y_{n}, X_{n}\right) \xrightarrow[x \times \xi]{\stackrel{p}{\rightarrow}} 0\), then \(Y_{n} \underset{\xi}{\stackrel{p}{\xi}} X\).

Proof. For each \(h \in B L_{1}\), we can write \(\left|E_{\xi \mid x} h\left(Y_{n}\right)-\operatorname{Eh}(X)\right| \leq\left|E_{\xi \mid x} h\left(Y_{n}\right)-E_{\xi \mid x} h\left(X_{n}\right)\right|+\left|E_{\xi \mid x} h\left(X_{n}\right)-\operatorname{Eh}(X)\right|\). For the second term on the right-hand side, \(\left|E_{\xi \mid x} h\left(X_{n}\right)-E h(X)\right| \underset{x}{p} 0\) by the assumption \(X_{n} \underset{\underset{\xi}{p}}{\underset{\sim}{p}} X\) and the definition of \(B L_{1}\). To analyze the first term on the right-hand side, note that for any \(\varepsilon \in(0,1)\), we have \(\left|E_{\xi \mid x} h\left(Y_{n}\right)-E_{\xi \mid x} h\left(X_{n}\right)\right| \leq \varepsilon E_{\xi \mid x} \mathbb{1}\left\{d\left(X_{n}, Y_{n}\right) \leq \varepsilon\right\}+2 E_{\xi \mid x} \mathbb{1}\left\{d\left(X_{n}, Y_{n}\right)>\right.\) \(\varepsilon\}\). The first part can be set arbitrarily small by letting \(\varepsilon \rightarrow 0\). To bound the second part, note first that the assumption of \(d\left(Y_{n}, X_{n}\right) \xrightarrow[x \times \xi]{p} 0\) yields \(\lim _{n \rightarrow \infty} \mathbb{P}^{x \times \xi}\left(d\left(X_{n}, Y_{n}\right)>\varepsilon\right)=\lim _{n \rightarrow \infty} E\left[\mathbb{1}\left\{d\left(X_{n}, Y_{n}\right)>\varepsilon\right\}\right]=0\). By the law of iterated expectations and the dominated convergence theorem, we obtain \(\lim _{n \rightarrow \infty} E\left[\mathbb{1}\left\{d\left(X_{n}, Y_{n}\right)>\varepsilon\right\}\right]=\lim _{n \rightarrow \infty} E_{x}\left[E_{\xi \mid x}\left[\mathbb{1}\left\{d\left(X_{n}, Y_{n}\right)>\varepsilon\right\}\right]\right]=\) \(E_{x}\left[\lim _{n \rightarrow \infty} E_{\xi \mid x}\left[\mathbb{1}\left\{d\left(X_{n}, Y_{n}\right)>\varepsilon\right\}\right]\right]=0\) In other words, \(\lim _{n \rightarrow \infty} E_{\xi \mid x}\left[\mathbb{1}\left\{d\left(X_{n}, Y_{n}\right)>\varepsilon\right\}\right]=0 \mathbb{P}^{x}\)-almost surely.

The following lemma will be used for deriving the weak convergence of Wald type statistics from joint weak convergence results for the numerator and denominator processes. It can be easily checked by using definition of Hadamard differentiation, and so we omit a proof.

Lemma 3. Let \((A(\cdot), B(\cdot)) \in \ell^{\infty}(\Theta) \times \ell^{\infty}(\Theta)\). If \(B(\cdot)>C>0\) on \(\Theta\), then \((F, G) \stackrel{\Phi}{\mapsto} F / G\) is Hadamard differentiable at \((A, B)\) tangentially to \(\ell^{\infty}(\Theta)\) with the Hadamard derivative \(\Phi_{(A, B)}^{\prime}\) given by \(\Phi_{(A, B)}^{\prime}(g, h)=(B g-A h) / B^{2}\).

We restate the Functional Central Limit Theorem of Pollard (1990) as the following lemma, which plays a pivotal role in the proof of our main Theorem. To cope with some measurability issues, we present the version with sufficient conditions for measurability by Kosorok (2003, Lemma1). See also Theorem 10.6 of Pollard (1990).

Lemma 4 (Pollard, 1990; Kosorok, 2003). Denote outer expectation, as defined in Section 1.2 of van der Vaart and Wellner (1996), by \(E^{*}\). Let a triangular array of almost measurable Suslin (AMS) stochastic processes \(\left\{f_{n i}(t): i=1, \ldots, n, t \in T\right\}\) be row independent, and define \(v_{n}(t)=\sum_{i=1}^{n}\left[f_{n i}(t)-E f_{n i}(\cdot, t)\right]\). Define \(\rho_{n}(s, t)=\left(\sum_{i=1}^{n}\left[f_{n i}(s)-f_{n i}(t)\right]^{2}\right)^{1 / 2}\). Suppose that the following conditions are satisfied. 1. The \(\left\{f_{n i}\right\}\) are manageable, with envelope \(\left\{F_{n i}\right\}\) which are also independent within rows; 2. \(H(s, t)=\lim _{n \rightarrow \infty} E v_{n}(s) \nu_{n}(t)\) exists for every \(s, t \in T ; 3\). \(\lim \sup _{n \rightarrow \infty} \sum_{i=1}^{n} E^{*} F_{n i}^{2}<\infty ; 4 . \lim _{n \rightarrow \infty} \sum_{i=1}^{n} E^{*} F_{n i}^{2} \mathbb{1}\left\{F_{n i}>\epsilon\right\}=0\) for each \(\epsilon>0\); 5. \(\rho(s, t)=\lim _{n \rightarrow \infty} \rho_{n}(s, t)\) exists for every \(s, t \in T\), and for all deterministic sequences \(\left\{s_{n}\right\}\) and \(\left\{t_{n}\right\}\) in \(T\), if \(\rho\left(s_{n}, t_{n}\right) \rightarrow 0\) then \(\rho_{n}\left(s_{n}, t_{n}\right) \rightarrow 0\). Then \(T\) is totally bounded under the \(\rho\) pseudometric and \(X_{n}\) converges weakly to a tight mean zero Gaussian process \(X\) concentrated on \(\left\{z \in \ell^{\infty}(T): z\right.\) is uniformly \(\rho\)-continuous \(\}\), with covariance \(H(s, t)\).

Remark 8. The AMS condition is technical and thus we refer the readers to Kosorok (2003). In this paper, we will make use of the following separability as a sufficient condition for AMS (Lemma 2; Kosorok (2003)):

Denote \(\mathbb{P}^{*}\) as outer probability, as defined in Section 1.2 of van der Vaart and Wellner (1996). A triangular array of stochastic processes \(\left\{f_{n i}(t): i=1, \ldots, n, t \in T\right\}\) is said to be separable if for every \(n \geq 1\), there exists a countable subset \(T_{n} \subset T\) such that \(\mathbb{P}^{*}\left(\sup _{t \in T} \inf _{s \in T_{n}} \sum_{i=1}^{n}\left(f_{n i}(s)-f_{n i}(t)\right)^{2}>0\right)=0\).

Checking the manageability in condition 1 above is usually not straightforward. In practice, we use VC type as a sufficient condition. We state Proposition 3.6.12 of GinÃ© and Nickl (2015) as a lemma below, which is used for establishing the VC type of functions we encounter.

Lemma 5. Let \(f\) be a function of bounded \(p\)-variation, \(p \geq 1\). Then, the collection \(\mathscr{F}\) of translations and dilation of \(f\), \(\mathscr{F}=\{x \mapsto f(t x-s): t>0, s \in \mathbb{R}\}\) is of VC type.

We also cite some results of Chernozhukov et al. (2014) as the following lemma, which shows the stability of VC type classes under element-wise addition and multiplication.

Lemma 6. Let \(\mathscr{F}\) and \(\mathscr{G}\) be of VC type with envelopes \(F\) and \(G\) respectively. Then the collection of element-wise sums \(\mathscr{F}+\mathscr{G}\) and the collection of element-wise products \(\mathscr{F} \mathscr{G}\) are of VC type with envelope \(F+G\) and \(F G\), respectively.

The first one is a special case of Lemma A. 6 of Chernozhukov et al. (2014). The second one is proven in Corollary A. 1 of Chernozhukov et al. (2014).

\section*{A.3. Proof of Theorem 1 (the main result)}
\(\operatorname{Proof.} \operatorname{Part}(\mathbf{i}) \operatorname{For}(\theta, k) \in \mathbb{T}\), we define \(\left.f_{n i}(\theta, k)=\frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{x_{i}}{h_{k, n}\left(\theta_{k}\right)}\right)}{\left.\sqrt{n h_{k, n}}\right)} \mathcal{E}_{k}\left(Y_{i}\right), D_{i}, X_{i}, \theta\right) K\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right) \delta_{i}^{+}=\frac{a_{0}+a_{1}\left(\frac{x_{i}}{c_{k}\left(\theta_{k}\right) h_{n}}\right)+\cdots+a_{p}\left(\frac{x_{i}}{c_{k}\left(\theta_{k}\right) h_{n}}\right)^{p}}{\sqrt{n c_{k}\left(\theta_{k}\right) h_{n} f_{X}(0)}}\) \(\mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) K\left(\frac{X_{i}}{c_{k}\left(\theta_{k}\right) h_{n}}\right) \delta_{i}^{+}\)and \(v_{n}^{+}(\theta, k)=\sum_{i=1}^{n}\left[f_{n i}(\theta, k)-E f_{n i}(\theta, k)\right]\). By Assumption 1(i)(a), the triangular array \(\left\{f_{n i}(\theta, k)\right\}\) is row independent. The separability follows from the same argument as in the proof of Theorem 4 in Kosorok (2003) and the left or right continuity (in both \(\theta_{1}\) and \(\theta_{2}\) ) of the process \(f_{n i}(\theta, k)\), which follows from Assumption 1 (ii)(d),(iii) and (iv). We claim that it satisfies the conditions required by Lemma 4.

For condition 1, we note that \(\mathcal{E}_{k}\left(Y_{i}, X_{i}, \cdot\right)\) is a VC type (Euclidean) class with envelope \(2 F_{\mathcal{E}}\) by Assumption 1(ii)(a) and Lemma 6. Notice that for a fixed \(n\), denote \(\delta_{x}^{+}=\mathbb{1}\{x>0\}\). Both \(\left\{x \mapsto \frac{\left(a_{0}+a_{1}\left(\frac{x}{c_{k}\left(\theta_{k}\right) h_{n}}\right)+\cdots+a_{p}\left(\frac{c_{x}}{c_{k}\left(k_{k}\right) h_{n}}\right)^{p}\right) \mathbb{1}\left\{|x| \leq \bar{c} h_{n}\right\} \delta_{x}^{+}}{\sqrt{n c_{k}\left(\theta_{k} k\right) h_{n} f}(0)}:(\theta, k) \in \mathbb{T}\right\}\) and \(\left\{x \mapsto K\left(\frac{x}{c_{k}\left(\theta_{k}\right) h_{n}}\right):(\theta, k) \in \mathbb{T}\right\}\) are of VC type with envelopes \(\frac{c_{1}}{\sqrt{n h_{n}}} \mathbb{1}\left\{|x| \leq \bar{c} h_{n}\right\}\) and \(\mathbb{1}\left\{|x| \leq \bar{c} h_{n}\right\}\|K\|_{\infty}\), respectively, under Assumptions 1 (i),(iii) and (iv) and Lemma 5. By Lemma 6, their product is a VC type class with envelope \(F_{n i}(y, d, x)=\) \(\frac{C_{3}}{\sqrt{n h_{n}}} F_{\mathcal{E}}(y, d, x) \mathbb{1}\left\{C_{2} \frac{x}{h_{n}} \in[-1,1]\right\}\). Applying Lemma 9.14 (iii) and Theorem 9.15 of Kosorok (2008), we obtain that \(\left\{f_{n i}\right\}\) is a bounded uniform entropy integral class with row independent envelopes \(F_{n i}\). Theorem 1 of Andrews (1994) then implies that \(\left\{f_{n i}\right\}\) is manageable with respect to the envelope \(\left\{F_{n i}\right\}\), and therefore condition 1 is satisfied.

To check condition 2, notice that \(E\left[v_{n}^{+}(\theta, k) \nu_{n}^{+}(\vartheta, l)\right]=\sum_{i=1}^{n} E f_{n i}(\theta, k) f_{n i}(\vartheta, l)-\left(\sum_{i=1}^{n} E f_{n i}(\theta, k)\right) \cdot\left(\sum_{i=1}^{n} E f_{n i}(\vartheta, l)\right)\). Under Assumptions 1(i)(b),(ii)(c),(iii),(iv)(a) we can write

\[
\begin{aligned}
& \sum_{i=1}^{n} E f_{n i}(\theta, k) f_{n i}(\vartheta, l) \\
= & \int_{\mathbb{R}^{+}} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(u / c_{k}\left(\theta_{1}\right)\right) r_{p}^{\prime}\left(u / c_{l}\left(\vartheta_{l}\right)\right)\left(\Gamma_{p}^{+}\right)^{-1} e_{v}}{\sqrt{c_{k}\left(\theta_{k}\right) c_{l}\left(\vartheta_{l}\right)} f_{X}^{2}(0)} \sigma_{k l}\left(\theta, \vartheta \mid u h_{n}\right) K\left(\frac{u}{c_{k}\left(\theta_{k}\right)}\right) K\left(\frac{u}{c_{l}\left(\vartheta_{l}\right)}\right) f_{X}\left(u h_{n}\right) d u \\
= & \frac{\sigma_{k l}\left(\theta, \vartheta \mid 0^{+}\right) e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} \Psi_{p}^{+}((\theta, k),(\vartheta, l))\left(\Gamma_{p}^{+}\right)^{-1} e_{v}}{\sqrt{c_{k}\left(\theta_{k}\right) c_{l}\left(\vartheta_{l}\right)} f_{X}(0)}+O\left(h_{n}\right) .
\end{aligned}
\]

\(\Psi_{p}^{+}((\theta, k),(\vartheta, l))\) exists under Assumptions 1(iii) and (iv)(a). All entries in the matrix part are bounded under Assumption 1 (iii),(iv)(a)(c). In the last line, \(n\) enters only through \(O\left(h_{n}\right)\). Therefore, by Assumption 1(iii), the limit exists and is finite. Thus, \(\lim _{n \rightarrow \infty} \sum_{i=1}^{n} E f_{n i}\left(\theta_{1}, l_{1}\right) f_{n i}\left(\theta_{2}, l_{2}\right)\) exists. Since \(E f_{n i}\left(\theta_{1}, l_{1}\right)=0\) implies \(\lim _{n \rightarrow \infty}\left(\sum_{i=1}^{n} E f_{n i}(\theta, k)\right)\left(\sum_{i=1}^{n} E f_{n i}(\vartheta, l)\right)=\) 0 , and condition 2 is satisfied.

Under Assumption 1(i)(a), (ii)(a), (iii), and (iv)(a), it is clear that \(\sum_{i=1}^{n} E^{*} F_{n i}^{2}=\sum_{i=1}^{n} E F_{n i}^{2} \lesssim \int_{\mathscr{Y} \times \mathscr{O} \times \mathscr{X}} F_{\mathcal{E}}^{2}\left(y, d, u h_{n}\right) \mathbb{1}\left\{C_{2} u\right.\) \(\in[-1,1]\} d \mathbb{P}^{x}\left(y, d, u h_{n}\right)+o\left(h_{n}\right)<\infty\) as \(n \rightarrow \infty\). This shows condition 3 .

To show condition 4, note that for any \(\epsilon>0\)

\[
\begin{aligned}
& \lim _{n \rightarrow \infty} \sum_{i=1}^{n} E^{*} F_{n i}^{2} \mathbb{1}\left\{F_{n i}>\epsilon\right\}=\lim _{n \rightarrow \infty} \sum_{i=1}^{n} E F_{n i}^{2} \mathbb{1}\left\{F_{n i}>\epsilon\right\} \\
\lesssim & \lim _{n \rightarrow \infty} \int_{\mathscr{Y} \times \mathscr{O} \times \mathscr{X}} F_{\mathcal{E}}^{2}\left(y, d, u h_{n}\right) \mathbb{1}\left\{\frac{C_{3}}{\left.\sqrt{\sqrt{h_{n}}} F_{\mathcal{E}}\left(y, d, u h_{n}\right) \mathbb{1}\left\{C_{2} u \in[-1,1]\right\}>\epsilon\right\} d \mathbb{P}^{x}\left(y, d, u h_{n}\right)}\right. \\
= & \int_{\mathscr{Y} \times \mathscr{O} \times \mathscr{X}} F_{\mathcal{E}}^{2}\left(y, d, u h_{n}\right) \lim _{n \rightarrow \infty} \mathbb{1}\left\{\frac{C_{3}}{\sqrt{n h_{n}}} F_{\mathcal{E}}\left(y, d, u h_{n}\right) \mathbb{1}\left\{C_{2} u \in[-1,1]\right\}>\epsilon\right\} d \mathbb{P}^{x}\left(y, d, u h_{n}\right)=0
\end{aligned}
\]

by the dominated convergence theorem under Assumption 1(ii)(a), (iii).\\
To show condition 5 , note that we can write \(\rho_{n}^{2}((\theta, k),(\vartheta, l))=\sum_{i=1}^{n} E\left[f_{n i}(\theta, k)-f_{n i}(\vartheta, l)\right]^{2}=n E\left[f_{n i}^{2}(\theta, k)+f_{n i}^{2}(\vartheta, l)-\right.\) \(\left.2 f_{n i}(\theta, k) f_{n i}(\vartheta, l)\right]\). From our calculations on the way to show condition 2 , we know that each term exists on the righthand side. Since \(n\) enters the expression only through the \(O\left(h_{n}\right)\) part, for all deterministic sequences \(\left\{s_{n}\right\}\) and \(\left\{t_{n}\right\}\) in \(\mathbb{T}\), \(\rho^{2}\left(s_{n}, t_{n}\right) \rightarrow 0\) implies \(\rho_{n}^{2}\left(s_{n}, t_{n}\right) \rightarrow 0\).

Now, applying Lemma 4 , we have \(\nu_{n}^{+}(\cdot)\) converging weakly to a tight mean-zero Gaussian process \(\mathbb{G}_{H^{+}}(\cdot)\) with covariance function \(H^{+}((\theta, k),(\vartheta, l))=\frac{\sigma(\theta, \vartheta \mid 0) e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} \Psi^{+}((\theta, k),(\vartheta, l))\left(\Gamma_{p}^{+}\right)^{-1} e_{v}}{\sqrt{c_{k}\left(\theta_{k} c_{k}(\vartheta)\right.}(\vartheta) f_{x}(0)}\). Slutsky's Theorem and Assumption 1(iv) then give

\[
\sqrt{n h_{n}^{1+2 v}}\left[\begin{array}{l}
\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)-\mu_{1}^{(v)}\left(0^{+}, \cdot\right) \\
\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{+}, \cdot\right)
\end{array}\right] \rightsquigarrow\left[\begin{array}{l}
\mathbb{G}_{H^{+}}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)} \\
\mathbb{G}_{H^{+}}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}
\end{array}\right]
\]

Applying the functional delta method under Assumption 2(i), we then have

\[
\sqrt{n h_{n}^{1+2 v}}\left[\begin{array}{l}
\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)-\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot) \\
\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)-\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)(\cdot)
\end{array}\right] \rightsquigarrow\left[\begin{array}{c}
\phi_{\mu_{1}^{\prime}(v)\left(0^{+}, \cdot\right)}\left(\mathbb{G}_{H^{+}}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot) \\
\psi_{\mu_{2}^{\prime(v)}\left(0^{+}, \cdot\right)}\left(\mathbb{G}_{H^{+}}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)
\end{array}\right]
\]

All arguments above can be replicated for the left limit objects, and thus by Assumption 1(i)(a), we obtain

\[
\begin{aligned}
& \sqrt{n h_{n}^{1+2 v}}\left[\begin{array}{c}
\left(\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{-}, \cdot\right)\right)\right)(\cdot)-\left(\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right)(\cdot) \\
\left(\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{-}, \cdot\right)\right)\right)(\cdot)-\left(\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right)(\cdot)
\end{array}\right] \\
& \leadsto\left[\begin{array}{l}
\phi_{\mu_{1}^{\prime}\left(0^{+}, \cdot\right)}^{\prime}\left(\mathbb{G}_{H^{+}}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot)-\phi_{\mu_{1}^{(v)}\left(0^{-}, \cdot\right)}^{\prime}\left(\mathbb{G}_{H^{-}}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot) \\
\psi_{\mu_{2}^{\prime}(v)\left(0^{+}, \cdot\right)}\left(\mathbb{G}_{H^{+}}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)-\psi_{\mu_{2}^{\prime}\left(0^{-}, \cdot\right)}^{\left.\prime()^{\prime}\right)}\left(\mathbb{G}_{H^{-}}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)
\end{array}\right]=\left[\begin{array}{l}
\mathbb{G}^{\prime}(\cdot, 1) \\
\mathbb{G}^{\prime}(\cdot, 2)
\end{array}\right] .
\end{aligned}
\]

Finally, by another application of the functional delta method, the chain rule for the functional delta method (Lemma 3.9.3 of van der Vaart and Wellner (1996)), and Lemma 3 under Assumption 2(i) and (ii), we obtain

\[
\begin{aligned}
\sqrt{n h_{n}^{1+2 v}}[\hat{\tau}(\cdot)-\tau(\cdot)] & =\sqrt{n h_{n}^{1+2 v}}\left[\Upsilon\left(\frac{\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\hat{\mu}_{1, p}^{(v)}\left(0^{-}, \cdot\right)\right)}{\psi\left(\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\hat{\mu}_{, p, p}^{(v)}\left(0^{-}, \cdot\right)\right)}\right)(\cdot)-\Upsilon\left(\frac{\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)}{\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)}\right)(\cdot)\right] \\
& \rightsquigarrow \Upsilon_{W}^{\prime}\left(\frac{\left.\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right]\right]^{\prime}(\cdot, 1)-\left[\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right] G^{\prime}(\cdot, 2)}{\left(\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right)^{2}}\right)(\cdot) .
\end{aligned}
\]

\section*{A.4. Proof of Theorem 2}
Proof. Part (ii) We introduce the following notations.

\[
\begin{aligned}
& v_{\xi, n}^{+}(\theta, k)=\sum_{i=1}^{n} \xi_{i} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right)}{\sqrt{n c_{k}\left(\theta_{k}\right) h_{n}} f_{X}(0)} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) K\left(\frac{X_{i}}{c_{k}\left(\theta_{k}\right) h_{n}}\right) \delta_{i}^{+} \\
& \hat{v}_{\xi, n}^{+}(\theta, k)=\sum_{i=1}^{n} \xi_{i} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{X_{i}}{h_{k, n}\left(\theta_{k}\right)}\right)}{\sqrt{n c_{k}\left(\theta_{k}\right) h_{n}} \hat{f}_{X}(0)} \hat{\varepsilon}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) K\left(\frac{X_{i}}{c_{k}\left(\theta_{k}\right) h_{n}}\right) \delta_{i}^{+}
\end{aligned}
\]

Applying Theorem 2 of Kosorok (2003) (which is also the same as Theorem 11.19 of Kosorok (2008)), we have \(v_{\xi, n}^{+}\) \(\underset{\xi}{p} \mathbb{G}_{H+}\). In order to apply Lemma 2 , we need to show \(\sup _{(\theta, k) \in T}\left|\nu_{\xi, n}^{+}(\theta, k)-\hat{v}_{\xi, n}^{+}(\theta, k)\right| \xrightarrow[x \times \xi]{p} 0\).

We will focus on the case of \(k=1\), and same argument applies to the case of \(k=2\). Note that under Assumption 4, \(\left|\hat{f}_{X}(0)-f_{X}(0)\right|=o_{p}^{X \times \xi}(1)\). Thus under Assumption 1(i)(b),

\[
\begin{aligned}
& \nu_{\xi, n}^{+}(\theta, 1)-\hat{v}_{\xi, n}^{+}(\theta, 1) \\
= & \frac{1}{f_{X}(0) \hat{f}_{X}(0)} \sum_{i=1}^{n} \xi_{i} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)}{\sqrt{n c_{1}\left(\theta_{1}\right) h_{n}}} K\left(\frac{X_{i}}{c_{1}\left(\theta_{1}\right) h_{n}}\right) \delta_{i}^{+}\left[\hat{\mathcal{E}}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right) f_{X}(0)-\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \hat{f}_{X}(0)\right] \\
= & \frac{1}{f_{X}^{2}(0)+o_{p}^{X \times \xi}(1)} \sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right)\left[\hat{\mathcal{E}}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)-\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right] f_{X}(0) \\
& +\frac{o_{p}^{x \times \xi}(1)}{f_{X}^{2}(0)+o_{p}^{X \times \xi}(1)} \sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)=(1)+(2)
\end{aligned}
\]

where \(Z_{i}\left(\theta_{1}\right)=\xi_{i} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{X_{i}}{h_{1, n}\left(\theta_{1}\right)}\right)}{\sqrt{n c_{1}\left(\theta_{1}\right) h_{n}}} K\left(\frac{X_{i}}{c_{1}\left(\theta_{1}\right) h_{n}}\right) \delta_{i}^{+}\). It can be shown following the same procedures in the proof of Theorem 1 that under Assumption \(1,2, \sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right) \rightsquigarrow \mathbb{G}_{1}\) and \(\sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \rightsquigarrow \mathbb{G}_{2}\) for some zero mean Gaussian processes \(\mathbb{G}_{1}, \mathbb{G}_{2}: \Omega^{x} \times \Omega^{\xi} \mapsto \ell^{\infty}(\Theta)\). By Prohorov's Theorem, the weak convergence implies asymptotic tightness and therefore implies that \(\sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right)=O_{p}^{x \times \xi}(1)\) uniformly on \(\Theta\) and \(\sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)=O_{p}^{x \times \xi}(1)\) uniformly on \(\Theta\). Thus (2) \(=\) \(\frac{o_{p}^{x \times \xi}(1)}{f_{X}^{2}(0)+o_{p}^{x \times \xi}(1)}\) uniformly on \(\Theta\). We then control (1). Assumption 4 implies \(\sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right)\left[\hat{\mathcal{E}}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)-\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right] f_{X}(0)\) \(=\sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right)\left[o_{p}^{x \times \xi}(1)\right] f_{X}(0)=f_{X}(0)\left[O_{p}^{x \times \xi}(1)\right] \sum_{i=1}^{n} Z_{i}\left(\theta_{1}\right)\) uniformly on \(\Theta\). Therefore, we have \(\sup _{(\theta, k) \in \mathbb{T}} \mid \nu_{\xi, n}^{+}(\theta, k)-\) \(\hat{\nu}_{\xi, n}^{+}(\theta, k) \mid \underset{x \times \xi}{p} 0\), and thus we can apply Lemma 2 to conclude \(\hat{\nu}_{\xi, n}^{+} \underset{\xi}{p} \mathbb{G}_{H^{+}}\). With similar arguments, we can derive that \(\hat{\nu}_{\xi, n}^{-} \underset{\xi}{p} \mathbb{G}_{H^{-}}\).

The continuous mapping theorem for bootstrap (Kosorok, 2008, Proposition 10.7) and the continuity of the Hadamard derivatives imply

\[
\left[\begin{array}{l}
\widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 1) \\
\widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 2)
\end{array}\right]=\left[\begin{array}{c}
\phi_{\mu_{1}^{(v)}\left(0^{+}, \cdot\right)}^{\prime}\left(\hat{v}_{\xi, n}^{+}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot)-\phi_{\mu_{1}^{(v)}\left(0^{-}, \cdot\right)}^{\prime}\left(\hat{v}_{\xi, n}^{+}(\cdot, 1) / \sqrt{c_{1}^{1+2 v}(\cdot)}\right)(\cdot) \\
\psi_{\mu_{2}^{\prime(v)}\left(0^{+}, \cdot\right)}\left(\hat{\nu}_{\xi, n}^{+}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)-\psi_{\mu_{2}^{\prime}\left(0^{-}, \cdot\right)}^{\prime}\left(\hat{\nu}_{\xi, n}^{+}(\cdot, 2) / \sqrt{c_{2}^{1+2 v}(\cdot)}\right)(\cdot)
\end{array}\right] \underset{\underset{\xi}{p}\left[\begin{array}{l}
\mathbb{G}^{\prime}(\cdot, 1) \\
\mathbb{G}^{\prime}(\cdot, 2)
\end{array}\right]}{ }
\]

Recursively applying Functional Delta for Bootstrap (Theorem 2.9 of Kosorok (2008)) then gives

\[
\begin{aligned}
& \Upsilon_{W}^{\prime}\left(\frac{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 1)-\left[\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \widehat{\mathbb{X}}_{n}^{\prime}(\cdot, 2)}{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)\right]^{2}}\right) \\
& \underset{\underset{\xi}{p}}{\Upsilon_{W}^{\prime}}\left(\frac{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)\right] \mathbb{G}^{\prime}(\cdot, 1)-\left[\phi\left(\mu_{1}^{(v)}\left(0^{+}, \cdot\right)\right)-\phi\left(\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right)\right] \mathbb{G}^{\prime}(\cdot, 2)}{\left[\psi\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)\right)-\psi\left(\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)\right]^{2}}\right)
\end{aligned}
\]

This completes the proof.

\section*{A.5. Uniform consistency}
Theorem 1 implies the following useful asymptotic identities. They state the uniform consistency of the numerator and the denominator of Wald ratios. Because these implications themselves prove useful when we apply the main theorem to the specific example, we state them as corollaries below for convenience of reference.

Corollary 5. Under Assumptions 1 and \(2, \hat{\mu}_{l, p}^{(v)}\left(0^{ \pm}, \cdot\right)-\mu_{k}^{(v)}\left(0^{ \pm}, \cdot\right) \xrightarrow[x]{p} 0\) uniformly.\\
Corollary 6. Under Assumptions 1 and \(2, \phi\left(\hat{\mu}_{l, p}^{(v)}\left(0^{ \pm}, \cdot\right)\right)-\phi\left(\mu_{k}^{(v)}\left(0^{ \pm}, \cdot\right)\right) \xrightarrow[x]{p} 0\) uniformly.\\
Assumption 2(iv) further implies that the mode \(\underset{x}{p}\) of convergence in the above corollaries can be replaced by the mode \(\underset{x \times \xi}{p}\) of convergence.

\section*{A.6. First stage estimators}
To estimate MP, we replace \(\mu_{k}(x, \theta) \mathbb{1}\left\{\left|x / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\) by its estimate \(\tilde{\mu}_{k, p}(x, \theta) \mathbb{1}\left\{\left|x / h_{k, n}\left(\theta_{k}\right)\right| \leq 1\right\}\), which is uniformly consistent across \((x, \theta)\). Lemma 7 proposes such a uniformly consistent estimator without requiring to solve an additional optimization problem; by a mean-value expansion and uniform boundedness of \(\mu_{k}^{(p+1)}\), we can reuse the first stage local polynomial estimates of \(\hat{\mu}_{l, p}^{(v)}\left(0^{ \pm}, \theta\right)\) for all \(v \leq p\). This auxiliary result will prove useful when we apply Theorems 1 and 2 to specific examples. In fact, we will prove a more general result that allows us to use any first \(r\) terms of our \(p\) th order polynomial estimator for a \(t\) such that \(0 \leq t \leq p\).

Lemma 7. Fix an integer \(t\) such that \(0 \leq t \leq p\). Suppose that Assumptions 1 and 2 are satisfied. Let \(\mathcal{E}_{1}(y, d, x, \theta)=\) \(g_{1}\left(y, \theta_{1}\right)-\mu_{1}\left(x, \theta_{1}\right), \mathcal{E}_{2}(y, d, x, \theta)=g_{2}\left(d, \theta_{2}\right)-\mu_{2}\left(d, \theta_{2}\right), \delta_{x}^{+}=\mathbb{1}\{x \geq 0\}\) and \(\delta_{x}^{-}=\mathbb{1}\{x \leq 0\}\). Define \(\tilde{\mu}_{1, t}\left(x, \theta_{1}\right)=\) \(r_{t}\left(x / h_{1, n}\left(\theta_{1}\right)\right)^{\prime} \hat{\alpha}_{1+, t}\left(\theta_{1}\right) \delta_{x}^{+}+r_{t}\left(x / h_{1, n}\left(\theta_{1}\right)\right)^{\prime} \hat{\alpha}_{1-, t}\left(\theta_{1}\right) \delta_{x}^{-}\)and \(\tilde{\mu}_{2, t}\left(x, \theta_{2}\right)=r_{t}\left(x / h_{2, n}\left(\theta_{2}\right)\right)^{\prime} \hat{\alpha}_{2+, t}\left(\theta_{2}\right) \delta_{x}^{+}+r_{t}\left(x / h_{2, n}\left(\theta_{2}\right)\right)^{\prime} \hat{\alpha}_{2-, t}\left(\theta_{2}\right) \delta_{x}^{-}\) Then we have \(\hat{\mathcal{E}}_{1}(y, d, x, \theta)=\left[g_{1}\left(y, \theta_{1}\right)-\tilde{\mu}_{1, t}\left(x, \theta_{1}\right)\right] \mathbb{1}\left\{\left|x / h_{1, n}\left(\theta_{1}\right)\right| \leq 1\right\}\) and \(\hat{\mathcal{E}_{2}}(y, d, x, \theta)=\left[g_{2}\left(d, \theta_{2}\right)-\tilde{\mu}_{2, t}\left(x, \theta_{2}\right)\right] \mathbb{1}\left\{\mid x / h_{2, n}\right.\) \(\left.\left(\theta_{2}\right) \mid \leq 1\right\}\) are uniformly consistent for \(\mathcal{E}_{1}(y, d, x, \theta) \mathbb{1}\left\{\left|x / h_{1, n}\left(\theta_{1}\right)\right| \leq 1\right\}\) and \(\mathcal{E}_{2}(y, d, x, \theta) \mathbb{1}\left\{\left|x / h_{2, n}\left(\theta_{2}\right)\right| \leq 1\right\}\) on \([\underline{x}, \bar{x}] \times \mathscr{Y} \times \mathscr{D}\), respectively.

Proof. We will show the \(1+\) part only, since the other parts can be shown similarly. Recall that \(\mathcal{E}_{1}(y, d, x, \theta)=\) \(g_{1}\left(y, \theta_{1}\right)-\mu_{1}\left(x, \theta_{1}\right)\). If \(x>0, \mu_{1}\left(x, \theta_{1}\right) \mathbb{1}\left\{\left|x / h_{1, n}\left(\theta_{1}\right)\right| \leq 1\right\}=\left(\mu_{1}\left(0^{+}, \theta_{1}\right)+\mu_{1}^{(1)}\left(0^{+}, \theta_{1}\right) x+\cdots+\mu_{1}^{(t)}\left(0^{+}, \theta_{1}\right) \frac{x^{t}}{t!}+\right.\) \(\left.\mu_{1}^{(t+1)}\left(x_{n i}^{*}, \theta_{1}\right) \frac{x^{(t+1)}}{(t+1)!}\right) \mathbb{1}\left\{\left|x / c_{1}(\theta) h_{n}\right| \leq 1\right\}\). By Corollary \(5, \hat{\mu}_{1, t}^{(v)}\left(0^{ \pm}, \theta_{1}\right)\) is uniformly consistent for \(\mu_{1}^{(v)}\left(0^{ \pm}, \theta_{1}\right), v=0,1, \ldots, t\). Thus, \(\left[\hat{\mathcal{E}}_{1}(y, d, x, \theta)-\mathcal{E}_{1}(y, d, x, \theta)\right] \mathbb{1}\left\{\left|x / h_{1, n}(\theta)\right| \leq 1\right\}=\left(\hat{\mu}_{1, t}\left(0^{+}, \theta_{1}\right)+\hat{\mu}_{1, t}^{(1)}\left(0^{+}, \theta_{1}\right) x+\cdots+\hat{\mu}_{1, t}^{(t)}\left(0^{+}, \theta_{1}\right) \frac{x^{t}}{t!}\right) \mathbb{1}\left\{\left|x / h_{1, n}(\theta)\right| \leq 1\right\}-\) \(\left(\mu_{1}\left(0^{+}, \theta_{1}\right)+\mu_{1}^{(1)}\left(0^{+}, \theta_{1}\right) x+\cdots+\mu_{1}^{(t)}\left(0^{+}, \theta_{1}\right) \frac{x^{t}}{t!}+\mu_{1}^{(t+1)}\left(x^{*}, \theta_{1}\right) \frac{x^{(t+1)}}{(t+1)!}\right) \mathbb{1}\left\{\left|x / h_{1, n}\left(\theta_{1}\right)\right| \leq 1\right\}\)\\
\(=o_{p}^{x}(1)-\mu_{1}^{(t+1)}\left(x^{*}, \theta_{1}\right) \frac{\chi^{(t+1)}}{(t+1)!} \mathbb{1}\left\{|x| \leq h_{1, n}(\theta)\right\}=o_{p}^{x}(1)+O\left(h_{n}\right)\), where the last equality is by Assumption 1 (iii) and by the uniform boundedness of \(\mu^{(t+1)}\) under Assumption 1 (ii)(a).

Finally, we also provide consistent estimators, \(\hat{f}_{X}(0)\) and \(\hat{f}_{Y^{d} \mid C}(\cdot)\). For \(f_{X}(0)\), we propose to use the kernel density estimator \(\hat{f}_{X}(0)=\frac{1}{n b_{n}} \sum_{i=1}^{n} K\left(\frac{x_{i}}{b_{n}}\right)\) with \(b_{n} \rightarrow 0\) and \(n b_{n} \rightarrow \infty\). For \(f_{Y^{d} \mid C}(\cdot)\), we propose

\[
\begin{aligned}
& \hat{f}_{Y^{1} \mid C}(y)=\frac{\hat{f}_{Y^{*} \mid X D^{*}}\left(y \mid x_{0}^{+}, 1\right) \hat{P}\left(D_{i}^{*}=1 \mid X_{i}=x_{0}^{+}\right)-\hat{f}_{Y^{*} \mid X D^{*}}\left(y \mid x_{0}^{-}, 1\right) \hat{P}\left(D_{i}^{*}=1 \mid X_{i}=x_{0}^{-}\right)}{\hat{\mu}_{2,2}\left(0^{+}, 1\right)-\hat{\mu}_{2,2}\left(0^{-}, 1\right)} \quad \text { and } \\
& \hat{f}_{Y^{0} \mid C}(y)=\frac{\hat{f}_{Y^{*} \mid X D^{*}}\left(y \mid x_{0}^{+}, 0\right) \hat{P}\left(D_{i}^{*}=0 \mid X_{i}=x_{0}^{+}\right)-\hat{f}_{Y^{*} \mid X D^{*}}\left(y \mid x_{0}^{-}, 0\right) \hat{P}\left(D_{i}^{*}=0 \mid X_{i}=x_{0}^{-}\right)}{-\left(\hat{\mu}_{2,2}\left(0^{+}, 0\right)-\hat{\mu}_{2,2}\left(0^{-}, 0\right)\right)}
\end{aligned}
\]

where \(\hat{f}_{Y^{*} \mid X D^{*}}\left(y \mid x_{0}^{ \pm}, 1\right)=\frac{\frac{1}{n a_{n}^{2}} \sum_{i=1}^{n} K\left(\frac{X_{i}}{a_{n}}\right) K\left(\frac{Y_{i}^{*}-y}{a_{n}}\right) D_{i}^{*} \delta_{i}^{ \pm}}{\frac{1}{n a_{n}} \sum_{i=1}^{n} K\left(\frac{X_{i}}{a_{n}}\right) D_{i}^{*} \delta_{i}^{ \pm}}, \hat{f}_{Y^{*} \mid X D^{*}}\left(y \mid x_{0}^{ \pm}, 0\right)=\frac{\frac{1}{n a_{n}^{2}} \sum_{i=1}^{n} K\left(\frac{X_{i}}{a_{n}}\right) K\left(\frac{Y_{i}^{*}-y}{a_{n}}\right)\left(1-D_{i}^{*}\right) \delta_{i}^{ \pm}}{\frac{1}{n a_{n}} \sum_{i=1}^{n} K\left(\frac{X_{i}}{a_{n}}\right)\left(1-D_{i}^{*}\right) \delta_{i}^{ \pm}}, \hat{P}\left(D_{i}^{*}=1 \mid X_{i}=x_{0}^{ \pm}\right)=\) \(\frac{\sum_{i=1}^{n} K\left(\frac{X_{i}}{c_{n}}\right) D_{i}^{*} \delta_{i}^{ \pm}}{\sum_{i=1}^{n} K\left(\frac{X_{i}}{c_{n}}\right) \delta_{i}^{ \pm}}\), and \(\hat{P}\left(D_{i}^{*}=0 \mid X_{i}=x_{0}^{ \pm}\right)=\frac{\sum_{i=1}^{n} K\left(\frac{X_{i}}{c_{n}}\right)\left(1-D_{i}^{*}\right) \delta_{i}^{ \pm}}{\sum_{i=1}^{n} K\left(\frac{X_{i}}{c_{n}}\right) \delta_{i}^{ \pm}}\).

Lemma 8. Suppose Assumptions \(\mathrm{K}, \mathrm{M}, \mathrm{S}\) and \(\operatorname{FQRD}\) (i)-(iv) hold. Assume that the conditional \(C D F f_{Y^{*} \mid X D^{*}}\left(\cdot \mid 0^{+}, d\right)\) is continuously differentiable and its derivative is uniformly bounded on \(\mathcal{Y}_{1}\) for \(d \in\{0,1\}\). If \(a_{n}\) satisfies \(a_{n} \rightarrow 0, n a_{n}^{2} /\left|\log a_{n}\right| \rightarrow\) \(\infty, a_{n} \leq c a_{2 n}\) for some \(c>0\) and \(\left|\log a_{n}\right| / \log \log n \rightarrow \infty\) and \(c_{n}\) satisfies \(c_{n} \rightarrow 0\) and \(n c_{n} \rightarrow \infty\), then \(\sup _{y \in \mathcal{Y}_{1}} \mid \hat{f}_{Y^{1} \mid c}(y)-\) \(f_{Y^{1} \mid C}(y) \mid=o_{p}^{x}(1)\) and \(\sup _{y \in \mathcal{Y}_{1}} \hat{f}_{Y^{0} \mid C}(y)-f_{Y^{0} \mid C}(y) \mid=o_{p}^{x}(1)\).

Proof. We will prove this lemma for \(f_{Y^{1} \mid C}\), and the corresponding results for \(f_{Y^{0} \mid C}(y)\) follow similarly. From (3.1) and Assumption FQRD (i), (vi) \(f_{Y^{1} \mid C}(y)=\frac{\partial}{\partial y} F_{Y^{1} \mid C}(y)=\frac{\frac{\partial}{\partial y} \lim _{x \downarrow 0} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{H}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]-\frac{\partial}{\partial y} \lim _{x \uparrow 0} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{P}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]}{\lim _{x \downarrow 0} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]-\lim _{x \uparrow 0} E\left[\mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=x\right]}\). Theorem 1 then implies that the denominator terms can be consistently estimated by \(\hat{\mu}_{2,2}\left(0^{+}, 1\right)-\hat{\mu}_{2,2}\left(0^{-}, 1\right)\) at the rate of \(O_{p}^{x}\left(1 / \sqrt{n h_{n}}\right)\) and the limit is bounded away from zero by Assumption FQRD (iii).

As for the numerator terms, the boundedness of the integrand ensures that we can interchange the expectation and limit. Thus, we can write \(\frac{\partial}{\partial y} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \cdot \mathbb{1}\left\{D_{i}^{*}=d\right\} \mid X_{i}=0^{+}\right]=\frac{\partial}{\partial y} E\left[\mathbb{1}\left\{Y_{i}^{*} \leq y\right\} \mid X_{i}=0^{+}, D_{i}^{*}=1\right] \mathbb{P}^{x}\left(D_{i}^{*}=1 \mid X_{i}=0^{+}\right)+0\) \(=\frac{\partial}{\partial y} F_{Y^{*} \mid X D^{*}}\left(y \mid 0^{+}, 1\right) \mathbb{P}^{x}\left(D_{i}^{*}=1 \mid X_{i}=0^{+}\right)=f_{Y^{*} \mid X D^{*}}\left(y \mid 0^{+}, 1\right) \mathbb{P}^{x}\left(D_{i}^{*}=1 \mid X_{i}=0^{+}\right)\). The standard point-wise convergence result under the stated assumptions for Nadaraya-Watson estimator and Assumption FQRD (i) imply the consistency of \(\hat{P}\left(D_{i}^{*}=1 \mid X_{i}=0^{+}\right)\)for \(\mathbb{P}^{x}\left(D_{i}^{*}=1 \mid X_{i}=0^{+}\right)\). The uniform consistency of \(\hat{f}_{Y^{*} \mid X D^{*}}\left(\cdot \mid 0^{+}, 1\right)\) follows from Assumption FQRD (i), (ii), continuous differentiability, uniform boundedness of derivatives of \(\hat{Y}_{Y^{*} \mid X D^{*}}\left(\cdot \mid 0^{+}, 1\right)\) on \(\mathcal{Y}_{1}\) and Theorem 2.3 of GinÃ© and Guillou (2002), which is applicable under the above bandwidth assumptions for \(a_{n}\) and Assumption K.

\section*{A.7. Proof of Corollary 2}
Proof. We first show that Lemma 1 holds under cluster sampling. Note that Steps 1 and 2 of the Proof for Lemma 1 follow through. To see this, note that for the inverse factor in Step 1, the deterministic part is now \(E\left[\frac{1}{G h_{G}} \sum_{g=1}^{G} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{G}}\right) r_{p}\right.\) \(\left.\left(\frac{x_{i}}{h_{G}}\right) r_{p}^{\prime}\left(\frac{x_{i}}{h_{G}}\right)\right]=\frac{1}{G} \sum_{g=1}^{G} \sum_{i \in C_{g}} E\left[\frac{1}{h_{G}} \delta_{i}^{+} K\left(\frac{x_{i}}{h_{G}}\right) r_{p}\left(\frac{x_{i}}{h_{G}}\right) r_{p}^{\prime}\left(\frac{x_{i}}{h_{G}}\right)\right]\). Assumption \(5(\mathrm{i})\), (iii), and (iv) imply that the above becomes \(\frac{1}{G} \sum_{g=1}^{G} \sum_{i \in C_{g}} f_{X_{i}}(0)\left(\int_{\mathbb{R}^{+}} K(u) r_{p}(u) r_{p}^{\prime}(u)+O\left(h_{G}\right)\right)=\bar{f}_{X}(0) \Gamma_{p}^{+}+O\left(h_{G}\right)\). For the stochastic part \(\frac{1}{G} \sum_{g=1}^{G}\) \(\left(\frac{1}{h_{G}} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{x_{i}}{h_{G}}\right) r_{p}\left(\frac{x_{i}}{h_{G}}\right) r_{p}^{\prime}\left(\frac{x_{i}}{h_{G}}\right)\right)-E\left[\frac{1}{G} \sum_{g=1}^{G}\left(\frac{1}{h_{G}} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{x_{i}}{h_{G}}\right) r_{p}\left(\frac{x_{i}}{h_{G}}\right) r_{p}^{\prime}\left(\frac{x_{i}}{h_{G}}\right)\right)\right]=o_{p}^{X}(1)\) follows from the WLLN for the row-wise independent triangular array since nothing depends on \(\theta\) under Assumption 5(ii)(a).

For Step 2, the deterministic part follows the same argument under Assumption 5(i), (ii)(b), (iii), and (iv)(a), and thus we have \(E\left[\frac{1}{\sqrt{G h_{G}}} \sum_{g=1}^{G} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{G}}\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) \mu_{1}^{(p+1)}\left(x_{n i}^{*}, \theta_{1}\right) h_{G}^{p+1} \frac{\left(\frac{x_{i}}{h_{G}}\right)^{p+1}}{(p+1)!}\right]=O\left(\sqrt{\frac{h_{G}^{2 p+1}}{G}}\right)\) uniformly in \(\theta_{1}\). For the stochastic part, by the same Lipschitz argument under Assumption 5 (ii), (iii) and (iv)(a), it suffices to consider \(\frac{1}{\sqrt{G h_{G}}} \sum_{g=1}^{G} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{x_{i}}{h_{G}}\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) \mu_{1}^{(p+1)}\left(0^{+}, \theta_{1}\right) h_{G}^{p+1} \frac{\left(\frac{x_{i}}{h_{G}}\right)^{p+1}}{(p+1)!}\). For each cluster \(g\), we relabel the subscript \(i\) as \(t g\), where \(t \in\left\{1, \ldots, n_{g}\right\}\) indicates the index in cluster \(g\). Write \(W_{g}=\left(W_{1 g}, \ldots, W_{\bar{N} g}\right)\), where \(W_{t g}=\left(Y_{t g}, D_{t g}, X_{t g}\right)\) if \(t \leq n_{g}\) and \(W_{t g}=0\) if \(t>n_{g}\). Let \(\mathscr{F}_{s}^{t}=\left\{W_{g} \mapsto \mathbb{1}\left\{X_{t g}>0\right\} K\left(a X_{t g}\right)\left(a X_{t g}\right)^{s+p+1} \mu_{1}^{p+1}\left(0^{+}, \theta_{1}\right) \mathbb{1}\left\{X_{t g} \in[-1,1]\right\}: a \geq 1 / h_{0}, \theta_{1} \in \Theta_{1}\right\}\) and \(\mathscr{F}_{G, s}=\left\{W_{g} \mapsto \sum_{t=1}^{\bar{N}} \mathbb{1}\left\{X_{t g}>0\right\} K\left(X_{t g} / h_{G}\right)\left(X_{t g} / h_{G}\right)^{s+p+1} \mu_{1}^{p+1}\left(0^{+}, \theta_{1}\right): \theta_{1} \in \Theta_{1}\right\}\) for each integer \(s\) such that \(0 \leq s \leq p\). Note that \(\mathbb{1}\left\{X_{t g}>0\right\}=0\) for \(t>n_{g}\). Under Assumption 5 (ii), (iii), and (iv), \(\mathscr{F}_{G, s} \subset \mathscr{F}_{s}:=\sum_{t=1}^{\bar{N}_{s}} \mathscr{F}_{s}^{t}\) for all G. \(\mathscr{F}_{s}\) is a VC type class with an integrable envelope \(F_{s}=\bar{N} \bar{M}\|K\|_{\infty}\) by Lemma 6. Thus, applying Lemma 7 of Chiang (2018) in place of Theorem 5.2 of Chernozhukov et al. (2014) yields \(\sup _{f \in \mathscr{F}_{S}}\left|\frac{1}{\sqrt{G h_{G}}} \sum_{g=1}^{G}\left(f\left(W_{g}\right)-E f\left(W_{g}\right)\right) h_{G}^{p+1}\right|=O_{p}^{x}\left(\frac{h_{G}^{2 P+1}}{G}\right)\), and thus the same conclusion can be made.

For Step 3, under Assumption 5(i)(a), (ii)(d) and (iv)(a), its deterministic part now becomes

\[
\begin{aligned}
& E\left[\frac{1}{\sqrt{G h_{G}}} \sum_{g=1}^{G} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{G}}\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right] \\
= & E\left[\frac{1}{\sqrt{G h_{G}}} \sum_{g=1}^{G} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{G}}\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) E\left[\mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right) \mid X_{i}\right]\right]=0 .
\end{aligned}
\]

The stochastic part follows from Markov's inequality, Assumption 5(i), (ii)(c), (iii), (iv) and (v), and the calculation:

\[
\begin{aligned}
& \sup _{\theta_{1} \in \Theta_{1}} E\left[\frac{e_{s}^{\prime}}{\sqrt{G h_{G}}} \sum_{g=1}^{G} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{G}}\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right]^{2} \\
= & \sup _{\theta_{1} \in \Theta_{1}} \sum_{g=1}^{G} \frac{1}{G} E\left[\frac{e_{v}^{\prime}}{h_{G}} \sum_{i \in C_{g}} \delta_{i}^{+} K\left(\frac{X_{i}}{h_{G}}\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) \mathcal{E}_{1}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\right]^{2} \leq \bar{N}^{2} \bar{M}^{2}\|K\|_{\infty}^{2}=O(1)
\end{aligned}
\]

for each \(1 \leq v \leq p\). Combining all three steps, we have \(\sqrt{G h_{G}^{1+2 v}}\left(\hat{\mu}_{k, p}^{(v)}\left(0^{ \pm}, \theta_{k}\right)-\mu_{k}^{(v)}\left(0^{ \pm}, \theta_{k}\right)-h_{G}^{p+1-v} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \Lambda_{p, p+1}^{ \pm}}{(p+1)!} \mu_{k}^{(p+1)}\right.\) \(\left.\left(0^{ \pm}, \theta_{k}\right)\right)=v!\sum_{g=1}^{G} \sum_{i \in C_{g}} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{ \pm}\right)^{-1} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) r_{p}\left(\frac{X_{i}}{h_{G}}\right) K\left(\frac{X_{i}}{h_{G}}\right) \delta_{i}^{ \pm}}{\sqrt{n h_{G}} \tilde{J}_{X}(0)}+o_{p}^{\chi}(1)\) uniformly for all \(\theta_{k} \in \Theta_{k}\) for each \(k \in\{1,2\}\).

Second, we show that Theorem 1 holds under cluster sampling. This is straightforward since Lemma 4 holds for non-identically distributed row-wise independent processes. Specifically, for \((\theta, k) \in \mathbb{T}\), we define \(f_{G g}(\theta, k)=\) \(\sum_{i \in C_{g}} \frac{e_{v}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{X_{i}}{h_{G}}\right)}{\sqrt{G_{G} \bar{f}_{X}}(0)} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right) K\left(\frac{X_{i}}{h_{G}}\right) \delta_{i}^{+}\)and \(v_{G}^{+}(\theta, k)=\sum_{g=1}^{G}\left[f_{G g}(\theta, k)-E f_{G g}(\theta, k)\right]\). Then, using the closure-undersummation property for VC type classes from Lemma 6 and following the same argument as in proof of Theorem 1 , Condition 1 required by Lemma 4 can be established. Condition 2 required by Lemma 4 holds under Assumption 5(v). Conditions 3,4 and 5 follow from similar arguments to those in the proof of Theorem 1. By an application of Lemma 4, \(v^{+}\) converges weakly to a tight mean-zero Gaussian process \(\mathbb{G}_{\Sigma^{+}}\)with covariance function \(\Sigma^{+}\)defined in Assumption 5(v). Finally, by an application of the functional delta method, the chain rule for the functional delta method (Lemma 3.9.3 of van der Vaart and Wellner (1996)), and Lemma 3 under Assumption 2 (i) and (ii), we obtain

\[
\begin{aligned}
\sqrt{G h_{G}^{1+2 v}}[\hat{\tau}(\cdot)-\tau(\cdot)] & =\sqrt{G h_{G}^{1+2 v}}\left[\Upsilon\left(\frac{\hat{\mu}_{1, p}^{(v)}\left(0^{+}, \cdot\right)-\hat{\mu}_{1, p}^{(v)}\left(0^{-}, \cdot\right)}{\hat{\mu}_{2, p}^{(v)}\left(0^{+}, \cdot\right)-\hat{\mu}_{2, p}^{(v)}\left(0^{-}, \cdot\right)}\right)(\cdot)-\Upsilon\left(\frac{\mu_{1}^{(v)}\left(0^{+}, \cdot\right)-\mu_{1}^{(v)}\left(0^{-}, \cdot\right)}{\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)}\right)(\cdot)\right] \\
& \rightsquigarrow \Upsilon_{W}^{\prime}\left(\frac{\left[\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right] \mathbb{G}(\cdot, 1)-\left[\mu_{1}^{(v)}\left(0^{+}, \cdot\right)-\mu_{1}^{(v)}\left(0^{-}, \cdot\right)\right] \mathbb{G}(\cdot, 2)}{\left(\mu_{2}^{(v)}\left(0^{+}, \cdot\right)-\mu_{2}^{(v)}\left(0^{-}, \cdot\right)\right)^{2}}\right)(\cdot),
\end{aligned}
\]

where \(\mathbb{G}:=\mathbb{G}_{\Sigma^{+}}-\mathbb{G}_{\Sigma^{-}}\).

Finally, we argue that Theorem 2 holds under cluster sampling. This is also straightforward since Theorem 2 of Kosorok (2003) only requires row-wise independence as well. Let us denote \(v_{\xi, G}^{+}(\theta, k)=\sum_{g=1}^{G} \xi_{g} \sum_{i \in C_{g}} \frac{e_{y}^{\prime}\left(\Gamma_{p}^{+}\right)^{-1} r_{p}\left(\frac{X_{i}}{h_{G}}\right)}{\sqrt{G_{G} \bar{f}_{X}(0)}} \mathcal{E}_{k}\left(Y_{i}, D_{i}, X_{i}, \theta\right)\) \(K\left(\frac{X_{i}}{h_{G}}\right) \delta_{i}^{+}\). Applying Theorem 2 of Kosorok (2003), we have \(v_{\xi, G}^{+} \underset{\xi}{p} \mathbb{G}_{\Sigma^{+}}\)and similar results hold that \(\underset{\xi, G}{-} \underset{\xi}{\underset{\xi}{m}} \mathbb{G}_{\Sigma^{-}}\). To apply Lemma 2, note that it follows from Assumption 5 (viii) that \(\sup _{(\theta, k) \in \mathrm{T}}\left|\nu_{\xi, n}^{+}(\theta, k)-\hat{v}_{\xi, n}^{+}(\theta, k)\right| \underset{x \times \xi}{p} 0\). The result then follows from the proof of Theorem 2.

\section*{A.8. Auxiliary lemma for the extended result with covariates}
Lemma 9. Suppose Assumptions C, K, M, S and FQRD hold, then \(\sup _{(y, d) \in \mathscr{Y}_{1} \times\{0,1\}}\left|\check{\gamma}_{1+}(y, d)-\gamma_{1+}(y, d)\right|+\sup _{(y, d) \in \mathscr{Y}_{1} \times\{0,1\}} \mid \check{\gamma}_{1-}\)\\
\includegraphics[max width=\textwidth, center]{2025_02_11_5e26b331b95b59abcde1g-28}

Proof. Section 2.1 and Lemma SA-14 of the Supplementary Appendix of Calonico et al. (2018b) imply that for each fixed \((y, d)\), under Assumptions \(\operatorname{FQRD}(\mathrm{i})(\) iii \()\) and C , one has \(\frac{\check{\mu}_{1}\left(0^{+}, y, d\right)-\breve{\mu}_{1}\left(0^{-}, y, d\right)}{\check{\mu}_{2}\left(0^{+}, d\right)-\check{\mu}_{2}\left(0^{-}, d\right)} \xrightarrow{p}\) \(\frac{\mu_{1}\left(0^{+}, y, d\right)-\mu_{1}\left(0^{-}, y, d\right)-\left[\gamma_{1+}^{\prime}(y, d) \mu_{Z}\left(0^{+}\right)-\gamma_{1-}^{\prime}(y, d) \mu_{Z}\left(0^{-}\right)\right]}{\mu_{2+}(d)-\mu_{2}-(d)-\left[\gamma_{2+}^{\prime}(d) \mu_{Z}\left(0^{+}\right)-\gamma_{2-}^{\prime}(d) \mu_{Z}\left(0^{-}\right)\right]}\). Assumptions C (i) and C (ii) further imply that for all \((y, d) \in \mathscr{Y}_{1} \times\{0,1\}\), it holds that \(\gamma_{1+}^{\prime}(y, d) \mu_{Z}\left(0^{+}\right)-\gamma_{1-}^{\prime}(y, d) \mu_{Z}\left(0^{-}\right)=0\) and \(\gamma_{2+}^{\prime}(d) \mu_{Z}\left(0^{+}\right)-\gamma_{2-}^{\prime}(d) \mu_{Z}\left(0^{-}\right)=0\). Thus

\[
\frac{\check{\mu}_{1}\left(0^{+}, y, d\right)-\check{\mu}_{1}\left(0^{-}, y, d\right)}{\check{\mu}_{2}\left(0^{+}, d\right)-\check{\mu}_{2}\left(0^{-}, d\right)} \xrightarrow{p} F_{Y^{d} \mid C}(y)
\]

This convergence can be made uniformly over \(\mathscr{Y}_{1} \times\{0,1\}\). To see this, note that Section 8 of Supplementary Appendix of Calonico et al. (2018b) implies that \(\check{\gamma}_{1+}(y, d)=\left[\mathbf{Z}^{\prime} \mathbf{K}_{+} \mathbf{Z} / n-\Upsilon_{Z+}^{\prime} \Gamma_{+}^{-1} \Upsilon_{Z+}\right]^{-1}\left[\mathbf{Z}^{\prime} \mathbf{K}_{+} \mathbf{Y}(y, d) / n-\Upsilon_{Z+}^{\prime} \Gamma_{+}^{-1} \Upsilon_{Y+}(y, d)\right]\), where \(\mathbf{Z}=\left[Z_{1}, \ldots, Z_{n}\right]^{\prime}, \mathbf{K}_{+}=\operatorname{diag}\left\{K\left(X_{1} / h_{n}\right) \delta_{1}^{+}, \ldots, K\left(X_{n} / h_{n}\right) \delta_{n}^{+}\right\}, \mathbf{Y}(y, d)=\left[\mathbb{1}\left\{Y_{1} \leq y, D_{1}=d\right\}, \ldots, \mathbb{1}\left\{Y_{n} \leq y, D_{n}=d\right\}\right]^{\prime}\), \(\Upsilon_{Z+}=\frac{1}{n} \sum_{i=1}^{n} r\left(X_{i} / h_{n}\right) K\left(X_{i} / h_{n}\right) \delta_{i}^{+} Z_{i}\), and \(\Upsilon_{Y+}(y, d)=\frac{1}{n} \sum_{i=1}^{n} r\left(X_{i} / h_{n}\right) K\left(X_{i} / h_{n}\right) \delta_{i}^{+} \mathbb{1}\left\{Y_{i} \leq y, D_{i}=d\right\}\). Note also that \(\mathbf{Z}^{\prime} \mathbf{K}_{+} \mathbf{Z} / n\) and \(\Upsilon_{Z+}\) converge to their respective probability limits following Lemmas SA-2 and SA-3 of Supplementary Appendix of Calonico et al. (2018b). Convergence of \(\Gamma_{+}^{-1}\) follows from proof of Lemma 1, and uniform convergence of \(\mathbf{Y}(y, d)\) and \(\Upsilon_{Y+}(y, d)\) follows the same arguments as those for the convergence of \(\Gamma_{+}^{-1}\) in Assumption 1. Therefore, \(\check{\gamma}_{1+}(y, d)\) converges uniformly in probability to its probability limit \(\gamma_{1+}(y, d)\). This shows that (A.4) holds uniformly over \(\mathscr{Y}_{1} \times\{0,1\}\).

\section*{A.9. Proof of Corollary 4}
Proof. Let \(\check{g}_{1}\left(Y_{i}, \theta\right)=\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d\right\}-Z_{i}^{\prime} \check{\gamma}_{1+}(y, d)\) and \(\check{g}_{2}\left(D_{i}, \theta\right)=\mathbb{1}\left\{D_{i}^{*}=d\right\}-Z_{i}^{\prime} \check{\gamma}_{1+}(d)\). Then, Lemma 9 implies that \(g_{1}\left(Y_{i}, \theta\right)=\mathbb{1}\left\{Y_{i}^{*} \leq y, D_{i}^{*}=d\right\}-Z_{i}^{\prime} \gamma_{1}(y, d)+o_{p}(1)\) and \(g_{2}\left(D_{i}, \theta\right)=\mathbb{1}\left\{D_{i}^{*}=d\right\}-Z_{i}^{\prime} \gamma_{1}(d)+o_{p}(1)\) uniformly over \(\mathscr{Y}_{1} \times\{0,1\}\). Lemma 1 holds for such \(\check{g}_{1}\left(Y_{i}, \theta\right)\) and \(\check{g}_{2}\left(D_{i}, \theta\right)\) and using \(\check{g}_{1}\) and \(\check{g}_{2}\) in place leads to asymptotically equivalent expressions. Theorem 1 thus follows. Furthermore, the proof of Theorem 2 follows by replacing \(\gamma_{1}\) by \(\check{\gamma}_{1 \pm}\) and replacing \(\gamma_{2}\) by \(\check{\gamma}_{2 \pm}\). From these results follows Corollary 1.

\section*{Appendix B. Supplementary data}
The supplementary material includes additional examples, additional proofs, and additional simulation results.\\
Supplementary material related to this article can be found online at \href{https://doi.org/10.1016/j.jeconom.2019.03.006}{https://doi.org/10.1016/j.jeconom.2019.03.006}.

\section*{References}
Anderson, T.W., Rubin, Herman, 1949. Estimation of the parameters of a single equation in a complete system of stochastic equations. Ann. Math. Stat. 20 (1), 46-63.\\
Andrews, Donald W.K., 1994. Empirical process methods in econometrics(chapter 37), vol.4. In: Handbook of Econometrics, Elsevier, pp. \(2247-2294\).\\
Andrews, Donald W.K., Moreira, Marcelo J., Stock, James H., 2006. Optimal two-sided invariant similar tests for instrumental variables regression. Econometrica 74 (3), 715-752.\\
Arai, Yoichi, Ichimura, Hidehiko, 2016. Optimal bandwidth selection for the fuzzy regression discontinuity estimator. Econom. Lett. 141, \(103-106\).\\
Arai, Yoichi, Ichimura, Hidehiko, 2018. Simultaneous selection of optimal bandwidths for the sharp regression discontinuity estimator. Quant. Econ. 9 (1), 441-482.\\
Barrett, Garry F., Donald, Stephen G., 2003. Consistent tests for stochastic dominance. Econometrica 71 (1), 71-104.\\
Bartalotti, OtÃ¡vio, Brummet, Quentin, 2017. Regression discontinuity designs with clustered data. In: Cattaneo, M.D., Escanciano, J.C. (Eds.), Advances in Econometrics: Regression Discontinuity Designs: Theory and Applications, Vol. 38. Emerald Publishing.\\
Bartalotti, OtÃ¡vio, Calhoun, Gray, He, Yang, 2017. Bootstrap confidence intervals for sharp regression discontinuity designs with the uniform kernel. In: Cattaneo, M.D., Escanciano, J.C. (Eds.), Advances in Econometrics: Regression Discontinuity Designs: Theory and Applications, vol. 38. Emerald Publishing.\\
Bernal, Noelia, Carpio, Miguel A., Klein, Tobias J., 2017. The effects of access to health insurance: evidence from a regression discontinuity design in Peru. J. Public Econ. 154, 122-136.

Calonico, Sebastian, Cattaneo, Matias D., Farrell, Max H., Titiunik, Rocio, 2018b. Regression discontinuity designs using covariates. Rev. Econ. Stat. (forthcoming).\\
Calonico, Sebastian, Cattaneo, Matias D., Max, H., 2016. Coverage error optimal confidence intervals for regression discontinuity designs. In: Working Paper.\\
Calonico, Sebastian, Cattaneo, Matias D., Max, H., 2018a. On the effect of bias estimation on coverage accuracy in nonparametric inference. J. Amer. Statist. Assoc. 113 (522), 767-779.\\
Calonico, Sebastian, Cattaneo, Matias D., Titiunik, Rocio, 2014. Robust nonparametric confidence intervals for regression discontinuity designs. Econometrica 82 (6), 2295-2326.\\
Card, David, Lee, David S., Pei, Zhuan, Weber, Andrea, 2015. Inference on causal effects in a generalized regression kink design. Econometrica 83 (6), 2453-2483.\\
Cattaneo, Matias D., Escanciano, Juan Carlos, 2017. Regression discontinuity designs: theory and applications. In: Advances in Econometrics, vol. 38. Emerald Publishing.\\
Chen, Heng, Fan, Yanqin, 2011. Identification and wavelet estimation of LATE in a class of switching regime models. In: Working Paper.\\
Chernozhukov, Victor, Chetverikov, Denis, Kato, Kengo, 2014. Gaussian Approximation of suprema of empirical processes. Ann. Statist. 42 (4), 1564-1597.\\
Chiang, Harold D., 2018. Cluster-robust simultaneous inference for many average partial effects in binary/fractional models. ArXiv preprint arXiv: 1812.09397.

Chiang, Harold D., Sasaki, Yuya, 2019. Causal inference by quantile regression kink designs. J. Econometrics (forthcoming).\\
Clark, Damon, Martorell, Paco, 2014. The signaling value of a high school diploma. J. Political Economy 122 (2), 282-318.\\
Cook, Thomas D., 2008. Waiting for life to arrive: a history of the regression-discontinuity design in psychology, statistics and economics. J. Econometrics 142 (2), 636-654.\\
Deshpande, Manasi, 2016. Does welfare inhibit success? The long-term effects of removing low-income youth from the disability rolls. Amer. Econ. Rev. 106 (11), 3300-3330.\\
Donald, Stephen G., Hsu, Yu-Chin, Barrett, Garry F., 2012. Incorporating covariates in the measurement of welfare and inequality: methods and applications. Econom. J. 15 (1), C1-C30.\\
Dong, Yingying, 2016. Jump or kink? Identification of binary treatment regression discontinuity design without the discontinuity. In: Working Paper.\\
Dony, Julia, Einmahl, Uwe, Mason, David M., 2006. Uniform in bandwidth consistency of local polynomial regression function estimators. Austrian J. Stat. 35 (2 \& 3), 105-120.\\
Einmahl, Uwe., Mason, David M., 2000. An empirical process approach to the uniform consistency of kernel-type function estimators. J. Theoret. Prob. 13 (1), 1-37.\\
Fan, Jianqing, Gijbels, Irene, 1996. Local Polynomial Modelling and Its Applications: Monographs on Statistics and Applied Probability, vol. 66, CRC Press.\\
Fan, Yanqin, Guerre, Emmanuel, 2016. Multivariate local polynomial estimators: uniform boundary properties and asymptotic linear representation. In: Hill, R.C., Gonzalez-Rivera, G., Lee, T.-H. (Eds.), Advances in Econometrics, vol. 36. Emerald Publishing, pp. 489-537.\\
Fan, Yanqin, Liu, Ruixuan, 2016. A direct approach to inference in nonparametric and semiparametric quantile models. J. Econometrics 191 (1), 196-216.\\
Feir, Donna, Lemieux, Thomas, Marmer, Vadim, 2016. Weak identification in fuzzy regression discontinuity designs. J. Bus. Econom. Statist. 34 (2), 185-196.\\
Frandsen, Brigham R., FrÃ¶lich, Markus, Melly, Blaise, 2012. Quantile treatment effects in the regression discontinuity design. J. Econometrics 168 (2), 382-395.\\
GinÃ©, Evarist, Guillou, Armelle, 2002. Rates of strong uniform consistency for multivariate kernel density estimators. Annales de l'Institut Henri PoincarÃ© ProbabilitÃ©s et Statistiques 38 (6), 907-921.\\
GinÃ©, Evarist, Nickl, Richard, 2015. Mathematical Foundations of Infinite-Dimensional Statistical Models. Cambridge University Press.\\
GinÃ©, Evarist, Zinn, Joel, 1990. Bootstrapping general empirical measures. Ann. Probab. 18 (2), 851-869.\\
Gormley Jr., William T., Gayer, Ted, Phillips, Deborah, Dawson, Brittany, 2005. The effects of universal pre-K on cognitive development. Developmental Psychology 41 (6), 872-884.\\
Guerre, Emmanuel, Sabbah, Camille, 2012. Uniform bias study and bahadur representation for local polynomial estimators of the conditional quantile function. Econom. Theory 28 (1), 87-129.\\
Hahn, Jinyong, Todd, Petra, van der Klaauw, Wilbert, 2001. Identification and estimation of treatment effects with a regression-discontinuity design. Econometrica 69 (1), 201-209.\\
Hansen, Bruce E., 1996. Inference when a nuisance parameter is not identified under the null hypothesis. Econometrica 64 (2), 413-430.\\
Imbens, Guido, Kalyanaraman, Karthik, 2012. Optimal bandwidth choice for the regression discontinuity estimator. Rev. Econom. Stud. 79 (3), \(933-959\).\\
Imbens, Guido, Lemieux, Thomas, 2008. Special issue editors' introduction: the regression discontinuity design - theory and applications. J. Econometrics 142 (2), 611-614.\\
Imbens, Guido W., Wooldridge, Jeffrey M., 2009. Recent developments in the econometrics of program evaluation. J. Econ. Lit. 47 (1), 5-86.\\
Ito, Koichiro, 2015. Asymmetric incentives in subsidies: evidence from a large-scale electricity rebate program. Amer. Econ. J.: Econom. Policy 7 (3), 209-237.\\
Kleibergen, Frank, 2002. Pivotal statistics for testing structural parameters in instrumental variables regression. Econometrica 70 (5), \(1781-1803\).\\
Kong, Efang, Linton, Oliver, Xia, Yingcun, 2010. Uniform bahadur representation for local polynomial estimates of M-regression and its application to the additive model? Econom. Theory 26 (5), 1529-1564.\\
Kosorok, Michael R., 2003. Bootstraps of sums of independent but not identically distributed stochastic processes. J. Multivariate Anal. 84 (2), \(299-318\).\\
Kosorok, Michael R., 2008. Introduction To Empirical Processes and Semiparametric Inference. Springer.\\
Landais, Camille, 2015. Assessing the welfare effects of unemployment benefits using the regression kink design. Amer. Econ. J.: Econ. Policy 7 (4), 243-278.\\
Ledoux, Michel, Talagrand, Michel, 1988. Un Critere sur les Petites Boules dans le ThÃ©oreme Limite Central. Probab. Theory Related Fields 77 (1), 29-47.\\
Lee, David S., 2008. Randomized experiments from non-random selection in U.S. house elections. J. Econometrics 142 (2), 675-697.\\
Lee, David S., Lemieux, Thomas, 2010. Regression discontinuity designs in economics. J. Econ. Lit. 48 (2), 281-355.\\
Lee, Sokbae, Song, Kyungchul, Whang, Yoon-Jae, 2015. Uniform asymptotics for nonparametric quantile regression with an application to testing monotonicity. ArXiv preprint arXiv:1506.05337.\\
Masry, Elias, 1996. Multivariate local polynomial regression for time series: Uniform strong consistency and rates. J. Time Series Anal. 17 (6), \(571-599\).\\
Moreira, Marcelo J., 2003. A conditional likelihood ratio test for structural models. Econometrica 71 (4), 1027-1048.\\
Nielsen, Helena Skyt, SÃ¸rensen, Torben, Taber, Christopher, 2010. Estimating the effect of student aid on college enrollment: Evidence from a government grant policy reform. Amer. Econ. J.: Econ. Policy 2 (2), 185-215.\\
Otsu, Taisuke, Xu, Ke-Li, Matsushita, Yukitoshi, 2015. Empirical likelihood for regression discontinuity design. J. Econometrics 186 (1), 94-112.

Pollard, David, 1990. Empirical Processes: Theory and Applications. In: NSF-CBMS Regional Conference Series in Probability and Statistics, vol. 2. Porter, Jack, 2003. Estimation in the Regression Discontinuity Model. Department of Economics, University of Wisconsin at Madison, (unpublished). Qu, Zhongjun, Yoon, Jungmo, 2015. Nonparametric estimation and inference on conditional quantile processes. J. Econometrics 185 (1), 1-19.\\
Qu, Zhongjun, Yoon, Jungmo, 2018. Uniform inference on quantile effects under sharp regression discontinuity designs. J. Bus. Econom. Statist. (forthcoming).\\
Shigeoka, Hitoshi, 2014. The effect of patient cost sharing on utilization, health, and risk protection. Amer. Econ. Rev. 104 (7), \(2152-2184\).\\
Simonsen, Marianne, Skipper, Lars, Skipper, Niels, 2016. Price sensitivity of demand for prescription drugs: exploiting a regression kink design. J. Appl. Econometrics 31 (2), 320-337.\\
Thistlethwaite, Donald L., Campbell, Donald T., 1960. Regression-discontinuity analysis: an alternative to the ex post facto experiment. J. Edu. Psych. 51 (6), 309-317\\
Tsybakov, Alexandre B., 2008. Introduction to Nonparametric Estimation. In: Springer Series in Statistics.\\
van der Vaart, Aad W., 1998. Asymptotic Statistics. Cambridge University Press.\\
van der Vaart, Aad W., Wellner, Jon A., 1996. Weak Convergence and Empirical Processes. Springer-Verlag.\\
van der Vaart, Aad, Wellner, Jon A., 2011. A local maximal inequality under uniform entropy. Electron. J. Stat. 5, 192-203.

\begin{itemize}
  \item 
\end{itemize}


\end{document}