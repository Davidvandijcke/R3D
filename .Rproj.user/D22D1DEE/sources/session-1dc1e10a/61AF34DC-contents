\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{multirow}

\title{Minimum Distance Estimation of Quantile Panel Data Models* }

\author{Blaise Melly \({ }^{\dagger}\) and Martina Pons \({ }^{\ddagger}\)\\
First version: November 2020\\
This version: February 25, 2025\\
Link to the newest version}
\date{}


%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\def\Perp{\perp\!\!\!\perp}

\begin{document}
\maketitle


\begin{abstract}
We propose a minimum distance estimation approach for quantile panel data models where unit effects may be correlated with covariates. This computationally efficient method involves two stages: first, computing quantile regression within each unit, then applying GMM to the first-stage fitted values. Our estimators apply to (i) classical panel data, tracking units over time, and (ii) grouped data, where individual-level data are available, but treatment varies at the group level. Depending on the exogeneity assumptions, this approach provides quantile analogs of classic panel data estimators, including fixed effects, random effects, between, and Hausman-Taylor estimators. In addition, our method offers improved precision for grouped (instrumental) quantile regression compared to existing estimators. We establish asymptotic properties as the number of units and observations per unit jointly diverge to infinity. Additionally, we introduce an inference procedure that automatically adapts to the potentially unknown convergence rate of the estimator. Monte Carlo simulations demonstrate that our estimator and inference procedure perform well in finite samples, even when the number of observations per unit is moderate. In an empirical application, we examine the impact of the food stamp program on birth weights. We find that the program's introduction increased birth weights predominantly at the lower end of the distribution, highlighting the ability of our method to capture heterogeneous effects across the outcome distribution.
\end{abstract}

\section*{1 Introduction}
Quantile regression, introduced by Koenker and Bassett (1978), is a powerful tool for analyzing the effect of policies on the distribution of an outcome variable. The quantile treatment effect function provides more information than the average treatment effect, allowing, for instance, evaluation of the treatment's impact on inequality. When panel data are available, new identification and estimation strategies become feasible. Researchers can alleviate endogeneity

\footnotetext{*We are grateful to Manuel Arellano, Bo Honor√©, Aleksey Tetenov, and seminar participants at Bern University, Geneva University, UC Irvine, Fribourg University, the 27th International Panel Data Conference, the COMPIE 2022 Conference, the 2nd International Econometrics PhD Conference at Erasmus University Rotterdam, the 2023 Ski and Labor Seminar, the 2023 Young Swiss Economists Meeting, the 2023 IAAE Conference, the 2023 Summer Meeting of the Econometric Society, and the third Causal Inference Optimization-Conscious Econometrics Conference at the University of Chicago for useful comments.\\
\({ }^{\dagger}\) Department of Economics, University of Bern, Schanzeneckstrasse 1, 3001 Bern, Switzerland. \href{mailto:blaise.melly@unibe.ch}{blaise.melly@unibe.ch}.\\
\({ }^{\ddagger}\) Department of Economics, University of Bern, Schanzeneckstrasse 1, 3001 Bern, Switzerland. \href{mailto:martina.pons@unibe.ch}{martina.pons@unibe.ch}.
}
concerns, for instance, by allowing for correlated group effects. They can obtain more precise estimates using a random-effects estimator or exploit individual-level variables to identify the impact of group-level variables, e.g., with the Hausman and Taylor (1981) estimator.

We define panel data as a dataset structure where observations are organized along two dimensions. While classical panel data typically consist of individuals repeatedly measured over time, our results extend to group data, where individual-level observations are available, and treatments often vary at the group level. For example, Autor et al. (2013) use commuting zones in a given decade as groups, while Angrist and Lang (2004) employ schools. In both cases, treatments vary only between groups, but individual data are essential for estimating the conditional distribution of outcomes within each group. This paper adopts a general notation ( \(i\) and \(j\) subscripts) applicable to both classical panel and group data. We primarily use terminology (individuals and groups) that is more common to group data, as our application falls into this category.

As a first contribution of the paper, we propose a new class of minimum distance estimators for quantile panel data models. This class of estimators provides quantile analogs of established panel data methods, including fixed effects, random effects, between, and Hausman-Taylor estimators. Our estimation approach involves two stages. The first stage consists of group-level quantile regressions using individual-level covariates. In the second stage, we regress the fitted values from the first stage on individual-level and group-level variables. If these variables are potentially endogenous, an instrumental variable regression or, more generally, the generalized method of moments (GMM) estimator can be applied. This approach allows for the straightforward inclusion of external or internal instruments in the second stage. The proposed estimator is simple to implement, flexible, computationally fast, and applicable across various fields. \({ }^{1}\) While this two-step procedure may seem unconventional, we demonstrate in Section 2.3 that it is numerically equivalent to standard estimators when the least squares estimator is used in the first stage along with appropriate instruments.

As a nonlinear estimator, first-stage quantile regression is subject to a finite-sample bias that diminishes as the number of observations per group increases. Thus, our inference procedures are justified within an asymptotic framework where the number of observations per group \(n\) and the number of groups \(m\) diverge to infinity. \({ }^{2}\) The asymptotic variance of the sample moments has two components: one arising from the first-stage quantile regression and another from the second-stage GMM regression. Since the regressors vary within groups in the first stage, the relevant number of observations is \(m n\), making the variance proportional to \(1 /(m n)\). The second-stage estimation error, on the other hand, stems from the randomness of the group effects, with the corresponding variance proportional to \(1 / m\). However, this second component

\footnotetext{\({ }^{1}\) We provide general-purpose packages for both R and Stata.\\
\({ }^{2}\) Large \(n\) (often called large \(T\) ) asymptotics have been widely applied in the nonlinear and dynamic panel data literature. For seminal contributions, see Phillips and Moon (1999), Hahn and Kuersteiner (2002), and Alvarez and Arellano (2003).
}
is zero if no group effects exist or the instrument exploits only within variation. Thus, the asymptotic distribution of the estimator is dominated by the component with the slower rate of convergence, which is the second stage error unless there is no group heterogeneity or the instruments are uncorrelated with group membership. As a result, the asymptotic distribution of the estimator is non-standard, as the convergence rate of a coefficient depends on the presence of group heterogeneity and the variation used to identify that coefficient.

In Section 3.2, we consider three specific scenarios before suggesting adaptive estimation and inference procedures in Section 3.3. In the first case, we assume the presence of group heterogeneity and demonstrate that only the coefficients identified through within-group variation can be estimated at the \(\sqrt{m n}\) rate. For example, this applies to the fixed effects estimator. In contrast, coefficients associated with variables that are constant within groups rely on between-group variation for identification and are only estimable at the slower \(\sqrt{m}\) rate. Only the second-stage error shows up in the first-order asymptotic distribution for these coefficients. Consequently, in this scenario, some coefficients converge faster than others. Case 2 assumes no group heterogeneity, eliminating second-stage variance and yielding a uniform \(\sqrt{m n}\) convergence rate for all coefficients. Finally, we consider the intermediate case when group-level heterogeneity is present but vanishes precisely at the correct rate such that both components of the variance matter asymptotically.

These asymptotic results provide valuable insights into the mechanics of our estimator, but applying them requires knowing whether group-level heterogeneity is present or not, leading to non-adaptive inference. As the second main contribution of the paper, Section 3.3 introduces adaptive estimation and inference procedures that address this issue. The proposed methodology is robust to different degrees of heterogeneity, allowing the variance of group effects to be zero, bounded, or diminishing at arbitrary rates. We first show that the leading variance term can be adaptively estimated using a traditional cluster-robust variance estimator. This result offers a practical procedure that is simple to implement and circumvents the need to estimate challenging components like the variance of first-stage coefficients. By inverting this estimated variance, we obtain a GMM estimator that is uniformly efficient in the unknown relative convergence rates of the moments. This result is non-standard because of the potentially different convergence rates of the moments; the efficient weighting matrix may be asymptotically singular. Finally, we suggest an overidentification test, which provides, for instance, the quantile equivalent of the Hausman test for the exogeneity of the between variation.

In the context of group data, the most closely related work is the IV quantile regression estimator proposed by Chetverikov et al. (2016). They focus on the effect of variables that vary only between groups and implicitly assume the presence of group-level heterogeneity. \({ }^{3}\) While both their approach and ours share the same first-stage estimation, the second stage differs: we

\footnotetext{\({ }^{3}\) Their asymptotic distribution is degenerate in the absence of group effects, suggesting that the rate of convergence is faster in such cases.
}
regress the fitted values on all variables, whereas they regress the estimated intercept on the group-level regressors. As a result, their estimator is not invariant to reparametrizations of the individual-level regressors. In Table 1, simulations using the same data-generating process as Chetverikov et al. (2016) show that our minimum distance (MD) estimator exhibits substantially lower variance and mean squared error (MSE) across all sample sizes considered‚Äîreducing the MSE by a factor of up to 20. In Section 4, we demonstrate and explain why our estimator is more precise than theirs. Furthermore, we contribute to this literature by providing efficient adaptive estimation and inference procedures that remain valid regardless of the degree of grouplevel heterogeneity, deriving the limiting distribution of the estimator for the coefficients on the individual-level variables, and relaxing the growth condition of \(n\) relative to \(m\).

Our class of estimators includes the MD estimators of Chamberlain (1994) as a special case. We extend his framework by incorporating individual-level regressors and accommodating endogenous regressors and group effects but requiring the number of groups to approach infinity. \({ }^{4}\) Interestingly, in Chamberlain (1994), all the variance arises from the first-stage estimation, consistent with classical MD estimation, while in Chetverikov et al. (2016), the variance originates entirely from the second stage. In our framework, the variance can stem from either the first or second stage, depending on the presence or absence of group effects, with the estimated standard errors capturing the relevant leading component.

Our paper also contributes to the literature on quantile panel data models. \({ }^{5}\) Koenker (2004) introduced a penalized quantile fixed effects estimator that treats individual heterogeneity as a pure location shift. Kato et al. (2012) extend this approach by allowing group effects to depend on the quantile of interest and by contributing to the asymptotic theory of the estimator. Galvao and Wang (2015) propose a two-step minimum distance (MD) estimator as a computationally efficient method for estimating fixed effects quantile models. Our framework nests this estimator. However, their focus is solely on the effects of individual-level covariates without exploiting variation between individuals. \({ }^{6}\) In contrast, we aim to estimate the effects of individual- and group-level regressors while allowing for internal and external instruments. Galvao and Poirier (2019) suggest using the usual pooled quantile regression estimator in the presence of random effects. Our random effects estimator differs in that it targets the conditional quantile function given the group effects (see Remark 1 for a discussion on conditional effects). In other words, we estimate a different parameter for which quantile regression is inconsistent, even when the random effects are uncorrelated with the covariates.

\footnotetext{\({ }^{4}\) Chamberlain (1994) uses different terminology because he focuses on cross-sectional regressions. He analyzes a quantile regression model with a finite number of combinations of regressor values, where the number of cells (groups in our terminology) is finite, and the regressors are constant within each cell.\\
\({ }^{5}\) The main text focuses on the large \(n\) (often called large \(T\) ) literature. In short panels, Chernozhukov et al. (2013a) derive bounds for quantile effects. Arellano and Bonhomme (2016) introduce a class of correlated random effects quantile regression estimators that are consistent in finite \(n\). They apply this approach to study earnings and consumption dynamics in Arellano and Bonhomme (2017).\\
\({ }^{6}\) Galvao and Wang (2015) consider a traditional panel data setting; thus, in their terminology, they focus on estimating the effects of time-varying regressors.
}Chernozhukov et al. (2013b) introduced distribution regression as an alternative to quantile regression for estimating the entire conditional distribution of outcomes given covariates. Fern√°ndez-Val et al. (2022) extend this approach by developing a dynamic distribution regression panel data model with heterogeneous coefficients across groups. In their framework, the first stage involves regressing the outcome on individual-level covariates using distribution regression, followed by a second stage where the coefficients are projected onto group-level instruments. We build on their proof strategy to demonstrate that our inference procedure remains uniformly valid with respect to the degree of heterogeneity. However, our approach differs in several key aspects: we use quantile regression instead of distribution regression, project the fitted values rather than the coefficients, consider both individual-level and group-level instruments, and optimally combine these instruments using GMM. Although our method is limited to continuous outcomes, we find that quantile regression coefficients are easier and more intuitive to interpret.

As a third main contribution, we show the practical relevance of our approach in an empirical application. Specifically, we extend the work of Almond et al. (2011) by estimating the distributional effect of the Food Stamp Program on birth weight. Following the enactment of the Food Stamp Act, the number of counties implementing the program increased substantially in the late 1960s and early 1970s. To apply our minimum distance estimator, we define groups as county-trimester cells. The subscript \(j\) indexes a county-trimester cell, while the subscript \(i\) defines an individual within this cell. We estimate the model separately for black and white mothers and find that the Food Stamp Program has a positive impact on the lower tail of the birth weight distribution, particularly among black mothers.

The remainder of the paper is structured as follows. Section 2 introduces the model and the estimator and shows that traditional least-squares estimators can be implemented as MD estimators. Section 3 develops the asymptotic theory. Section 4 extends the discussion to grouped data and compares our estimator with the grouped IV quantile regression approach of Chetverikov et al. (2016). Section 5 applies our framework to traditional panel data, proposing quantile analogs of the within, between, and random effects estimators and the Hausman test. Both Sections 4 and 5 include Monte Carlo simulations to evaluate finite sample performance. In Section 6, we present the empirical application, and Section 7 concludes.

\section*{2 Model and Minimum Distance Estimator}
\subsection*{2.1 Quantile Model}
We want to learn the effects of the individual-level variables \(x_{1 i j}\) and the group-level variables \(x_{2 j}\) on the distribution of an outcome \(y_{i j}\). We observe these variables for the groups \(j=1, \ldots, m\) and individuals \(i=1, \ldots, n .^{7}\) For some quantile index \(0<\tau<1\), we assume that

\[
Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right)=x_{1 i j}^{\prime} \beta(\tau)+x_{2 j}^{\prime} \gamma(\tau)+\alpha\left(\tau, v_{j}\right)
\]

\footnotetext{\({ }^{7}\) We assume a balanced panel for notational simplicity. However, the results generalize to unbalanced datasets.
}
where \(Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right)\) is the \(\tau\) th conditional quantile function of the response variable \(y_{i j}\) for individual \(i\) belonging to group \(j\) given the \(K_{1}\)-vector of individual-level regressors \(x_{1 i j}\), the \(K_{2}\)-vector of group-level variables \(x_{2 j}\), and an unobserved random vector \(v_{j}\) of unrestricted and unknown dimension. In total, there are \(K_{1}+K_{2}=K\) parameters to estimate. The parameters \(\beta(\tau), \gamma(\tau)\) and the unobserved group heterogeneity \(\alpha\left(\tau, v_{j}\right)\) can depend on the quantile index \(\tau\). Depending on the setting, \(\beta(\tau)\) or \(\gamma(\tau)\) (or both) might be the parameters of interest. We normalize \(\mathbb{E}\left[\alpha\left(\tau, v_{j}\right)\right]=0\), which is not restrictive because \(x_{2 j}\) includes a constant.

Remark 1 (Conditional versus unconditional effects). In contrast to the average effect, the definition of a quantile treatment effect depends on the conditioning variables. In this paper, we model the distribution of \(y_{i j}\) conditionally on the covariates and the group effect \(\alpha\left(\tau, v_{j}\right)\). Thus, even if the group effects are independent of the regressors, we identify different parameters than those identified by quantile regression as introduced by Koenker and Bassett (1978) or by instrumental variable quantile regression as introduced by Chernozhukov and Hansen (2005). The following example illustrates the difference between these parameters. Consider an application where each group \(j\) corresponds to a region and each unit \(i\) to an individual within this region. We do not have any \(x_{1 i j}\) variable. We are interested in the effect of a binary treatment \(x_{2 j}\), which has been randomized and is, therefore, independent from \(\alpha\left(\tau, v_{j}\right) . \gamma(\tau)\) is the effect of this treatment for individuals that rank at the \(\tau\) quantile of \(y_{i j}\) in their region. On the other hand, the quantile regression of \(y_{i j}\) on \(x_{2 j}\) identifies the effect for individuals that rank at the \(\tau\) quantile in the whole country (given the treatment status). These are different parameters except if \(\alpha\left(\tau, v_{j}\right)=0\) for all \(j\) or if the treatment effect is homogeneous such that \(\gamma(\tau)=\gamma\) for all \(\tau\). Whether conditional or unconditional quantile treatment effects are of interest depends on the question. Conditional quantile treatment effects are particularly useful for studying within-group inequalities when groups might be regions or industries. For example, Autor et al. (2016) and Engelhardt and Purcell (2021) study the effect of the minimum wage on within-state inequality, while Autor et al. (2021) study the effect of trade shock on wage inequality within local labor markets. If the unconditional effect is of interest, one can naturally obtain the unconditional distribution functions by integrating out the group effects (and possibly the other variables) and then inverting the resulting distribution functions to obtain the unconditional quantile functions, see Chernozhukov et al. (2013b).

When model (1) holds, the \(\tau\) quantile regression of \(y_{i j}\) on \(x_{1 i j}\) and a constant using only observations for group \(j\) identifies the slope \(\beta(\tau)\) and the intercept \(x_{2 j}^{\prime} \gamma(\tau)+\alpha\left(\tau, v_{j}\right)\). We need to consider variation across groups to identify the coefficient on the group-level variables. Note that model (1) implies

\[
\mathbb{E}\left[Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right) \mid x_{1 i j}, x_{2 j}\right]=x_{1 i j}^{\prime} \beta(\tau)+x_{2 j}^{\prime} \gamma(\tau)+\mathbb{E}\left[\alpha\left(\tau, v_{j}\right) \mid x_{1 i j}, x_{2 j}\right]
\]

If \(\alpha\left(\tau, v_{j}\right)\) is exogenous with respect to \(x_{1 i j}\) and \(x_{2 j}\) and the linear model is correctly specified,\\
\(\mathbb{E}\left[\alpha\left(\tau, v_{j}\right) \mid x_{1 i j}, x_{2 j}\right]=0\) and a linear regression identifies the parameters of interest. \({ }^{8}\) The last representation suggests a two-step estimation strategy: (i) group-level quantile regression of \(y_{i j}\) on \(x_{1 i j}\), (ii) OLS regression of the fitted values from the first stage on \(x_{1 i j}\) and \(x_{2 j}\).

When the group effects \(\alpha\left(\tau, v_{j}\right)\) are endogenous (possibly correlated with \(x_{1 i j}\) and \(x_{2 j}\) ), we assume that there is a \(L\)-dimensional vector \((L \geq K)\) of valid instruments \(z_{i j}\) satisfying

\[
\mathbb{E}\left[z_{i j} \alpha\left(\tau, v_{j}\right)\right]=\mathbb{E}\left[z_{i j}\left(Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right)-x_{1 i j}^{\prime} \beta(\tau)-x_{2 j}^{\prime} \gamma(\tau)\right)\right]=0
\]

Note that \(\beta(\tau)\) is identified in model (1) as long as there is some variation in \(x_{1 i j}\) within some groups. For instance, we can include the demeaned regressors, \(\dot{x}_{1 i j}=x_{1 i j}-\bar{x}_{1 j}\) with \(\bar{x}_{1 j}=n^{-1} \sum_{i=1}^{n} x_{1 i j}\), in the vector of instruments \(z_{i j}\) because this variable will satisfy condition (2) under strict exogeneity. \({ }^{9}\) On the other hand, we need additional instruments to identify \(\gamma(\tau)\). Equation (2) suggests a similar estimation strategy as in the exogenous case but with the instrumental variable estimator (or more generally the GMM estimator) in the second stage: (i) group-level quantile regression of \(y_{i j}\) on \(x_{1 i j}\), (ii) GMM regression of the fitted values from the first stage on \(x_{1 i j}\) and \(x_{2 j}\) using \(z_{i j}\) as instrument.

Remark 2 (Skorohod representation). The following Skorohod representation implies the model defined in equation (1):

\[
\begin{aligned}
y_{i j} & =x_{1 i j} \beta\left(u_{i j}\right)+x_{2 j} \gamma\left(u_{i j}\right)+\alpha\left(u_{i j}, v_{j}\right) \\
& =q\left(x_{1 i j}, x_{2 j}, u_{i j}, v_{j}\right)
\end{aligned}
\]

where \(q\left(x_{1 i j}, x_{2 j}, u_{i j}, v_{j}\right)\) is strictly increasing in the third argument (while fixing the other arguments). \({ }^{10}\) We normalize \(u_{i j} \mid x_{1 i j}, x_{2 j}, v_{j} \sim U(0,1)\) such that \(q\left(x_{1 i j}, x_{2 j}, \tau, v_{j}\right)\) is the \(\tau\) conditional quantile function. \(u_{i j}\) ranks the individuals within a group and \(v_{j}\) captures the group heterogeneity. In this model, a sufficient condition for equation (2) is \(\left(u_{i j}, v_{j}\right) \Perp z_{i j}\). If the instrument does not vary within groups, only \(v_{j} \Perp z_{j}\) is sufficient.

Remark 3 (Heterogeneous coefficients). Our model allows only the intercept to differ between groups. Now consider a more general model where we also allow the slopes to differ between groups:

\[
y_{i j}=x_{1 i j}^{\prime} \beta\left(u_{i j}, v_{j}\right)+x_{2 j}^{\prime} \gamma\left(u_{i j}, v_{j}\right)+\alpha\left(u_{i j}, v_{j}\right)
\]

If we maintain the conditional strict monotonicity assumption with respect to \(u_{i j}\), this model implies that

\[
Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right)=x_{1 i j}^{\prime} \beta\left(\tau, v_{j}\right)+x_{2 j}^{\prime} \gamma\left(\tau, v_{j}\right)+\alpha\left(\tau, v_{j}\right)
\]

\footnotetext{\({ }^{8}\) Uncorrelation between \(\alpha\left(\tau, v_{j}\right)\) and \(x_{1 i j}\) and \(x_{2 j}\) is sufficient to identify the linear projection.\\
\({ }^{9}\) In the special case of traditional panel data, the demeaned regressors correspond to the within transformation.\\
\({ }^{10}\) This is the same model as in Chetverikov et al. (2016), where a similar Skorohod representation is derived in their footnote 7 .
}In the exogenous case where \(\left(x_{1 i j}, x_{2 j}\right) \Perp v_{j}\), it follows that

\[
\begin{aligned}
\mathbb{E}\left[Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right) \mid x_{1 i j}, x_{2 j}\right] & =x_{1 i j}^{\prime} \int \beta(\tau, v) d F_{V}(v)+x_{2 j}^{\prime} \int \gamma(\tau, v) d F_{V}(v)+\int \alpha(\tau, v) d F_{V}(v) \\
& =x_{1 i j}^{\prime} \bar{\beta}(\tau)+x_{2 j}^{\prime} \bar{\gamma}(\tau)
\end{aligned}
\]

because we have normalized \(\mathbb{E}\left[\alpha\left(\tau, v_{j}\right)\right]=0\). This implies that the linear projection of \(Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right)\) on \(x_{1 i j}\) and \(x_{2 j}\) identifies the coefficients \(\beta(\tau)\) and \(\gamma(\tau)\) when the homogenous model (1) holds and the average effect over all groups at the \(\tau\) quantile of their conditional distribution when the heterogenous model (4) holds. \({ }^{11}\) Naturally, it is also possible to model the heterogeneity between groups by estimating more flexible linear projections of \(Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right)\). For instance, we can interact \(x_{1 i j}\) with observable characteristics \(x_{2 j} .{ }^{12}\)

\subsection*{2.2 Quantile Minimum Distance Estimators}
Motivated by the representation in equation (2), we suggest the following the two-step procedure. In the first step, for each group \(j\) and quantile \(\tau\), we regress \(y_{i j}\) on individual-level variables \(x_{1 i j}\) and a constant using quantile regression. The intercept of the first stage regression captures both the group effect \(\alpha\left(\tau, v_{j}\right)\) and the term \(x_{2 j}^{\prime} \gamma(\tau)\) as these vary only between groups. In a second step, we regress the fitted values of the first stage on \(x_{1 i j}\) and \(x_{2 j}\), using GMM with instruments \(z_{i j}\).

Formally, the first stage quantile regression solves the following minimization problem for each group and quantile separately:

\[
\hat{\beta}_{j}(\tau)=\left(\hat{\beta}_{0, j}, \hat{\beta}_{1, j}^{\prime}\right)^{\prime}=\underset{\left(b_{0}, b_{1}\right) \in \mathbb{R}^{K_{1}+1}}{\arg \min } \frac{1}{n} \sum_{i=1}^{n} \rho_{\tau}\left(y_{i j}-b_{0}-x_{1 i j}^{\prime} b_{1}\right)
\]

where \(\rho_{\tau}(x)=(\tau-1\{x<0\}) x\) for \(x \in \mathbb{R}\) is the check function. The true vector of coefficients for group \(j\) is given by \(\beta_{j}(\tau)=\left(\alpha\left(\tau, v_{j}\right)+x_{2 j}^{\prime} \gamma(\tau), \beta(\tau)^{\prime}\right)^{\prime}\). When the model does not contain any \(x_{1 i j}\) variables, quantile regression computes the sample quantiles in each group.

Notation. Throughout the paper, we will use the following notation. Let \(\tilde{x}_{i j}=\left(1, x_{1 i j}^{\prime}\right)^{\prime}\) and \(x_{i j}=\left(x_{1 i j}^{\prime}, x_{2 j}^{\prime}\right)^{\prime}\). For each group \(j\) we define the following matrices. The \(n \times K_{1}\) matrix of individual-level regressors \(X_{1 j}=\left(x_{11 j}, x_{12 j}, \ldots, x_{1 n j}\right)^{\prime}\), the \(n \times K\) matrix containing all regressors \(X_{j}=\left(x_{1 j}, x_{2 j}, \ldots, x_{n j}\right)^{\prime}\) and the \(n \times L\) matrix of instruments \(Z_{j}=\left(z_{1 j}, z_{2 j}, \ldots, z_{n j}\right)^{\prime}\). Further, we define two matrices for all observations. The \(m n \times K\) matrix of regressors for all groups \(X=\left(X_{1}^{\prime}, \ldots, X_{m}^{\prime}\right)^{\prime}\) and the \(m n \times L\) matrix of instruments for all groups as \(Z=\left(Z_{1}^{\prime}, \ldots, Z_{m}^{\prime}\right)^{\prime}\).

\footnotetext{\({ }^{11}\) In the endogenous case, we obtain the instrumental variable projection instead of the standard linear projection. For instance, if \(x_{2 j}\) is an endogenous binary variable and \(z_{i j}\) is a binary instrument, we identify the average treatment effects for the compliers at the \(\tau\) quantile of their conditional distribution.\\
\({ }^{12}\) Starting with model (3), one can simultaneously analyze both within-group and inter-group heterogeneity by constructing a quantile function with two quantile indices: one for the heterogeneity across groups and one for the heterogeneity within groups. These heterogeneous coefficients are identified through a two-step quantile regression: (i) a group-by-group quantile regression of \(y_{i j}\) on \(x_{1 i j}\), followed by (ii) a quantile regression of the fitted values from the first stage on \(x_{1 i j}\) and \(x_{2 j}\). Pons (2024) explores a version of this model that defines different parameters and falls outside the scope of this paper.
}We let \(Y\) be the response variable's \(m n \times 1\) vector. The fitted value for individual \(i\) in group \(j\) at quantile \(\tau\) is \(\hat{y}_{i j}(\tau)=\hat{\beta}_{0, j}(\tau)+x_{1 i j}^{\prime} \hat{\beta}_{1, j}(\tau)\). We denote the \(n \times 1\) column vector of fitted values for group \(j\) by \(\hat{Y}_{j}(\tau)=\left(\hat{y}_{1 j}(\tau), \ldots, y_{n j}(\tau)\right)^{\prime}\), and the \(m n \times 1\) vector of fitted values by \(\hat{Y}(\tau)=\left(\hat{Y}_{1}^{\prime}(\tau), \ldots, \hat{Y}_{m}^{\prime}(\tau)\right)^{\prime}\).

Remark 4 (Alternative first-stage estimators). The quantile regression estimator proposed by Koenker and Bassett (1978) is not necessarily efficient. Newey and Powell (1990) suggest a semiparametrically efficient weighted estimator of \(\beta_{j}(\tau)\). However, we opt for the unweighted quantile regression estimator due to the challenges associated with estimating the weights and the complicated interpretation of the estimates in cases of misspecification. In our model (1), the variation within groups is assumed to be exogenous. If this assumption were violated, one could apply an instrumental variable (IV) quantile regression (see, e.g., Chernozhukov and Hansen, 2006) in the first stage, followed by the second-stage GMM regression described below. \({ }^{13}\) We do not pursue this (computationally intensive) extension in this paper.

The second stage consists of the linear GMM regression of \(\hat{Y}(\tau)\) on \(X\) using \(Z\) as an instrument. The estimator has the following closed-form expression:

\[
\hat{\delta}(\hat{W}, \tau)=\left(X^{\prime} Z \hat{W}(\tau) Z^{\prime} X\right)^{-1} X^{\prime} Z \hat{W}(\tau) Z^{\prime} \hat{Y}(\tau)
\]

where \(\hat{W}(\tau)\) is a \(L \times L\) symmetric weighting matrix. When \(L=K\), the second step estimator in equation (6) simplifies to the IV estimator using \(Z\) as an instrument, and we can drop the dependence on \(\hat{W}(\tau)\).

Our two-step estimator is extremely simple to implement; it requires only routines performing quantile regression and GMM estimation, which are already available in many software applications. Quantile regression, which is computationally more demanding due to the absence of a closed-form solution, is used only in the first stage, where there are fewer observations and a limited number of parameters to estimate. The first stage is also embarrassingly parallelizable, increasing the computational speed. For this reason, our estimator remains computationally attractive in large datasets with numerous groups. The second stage is a straightforward GMM estimator, which includes OLS and two-stage least squares as special cases. Traditional panel data methods can also be used in the second stage. For instance, in our application, we observe individuals born in a given trimester in a given county. The subscript \(j\) defines a county-trimester cell, while the subscript \(i\) defines an individual within this cell. In the second stage, we include trimester, county, and state \(\times\) year fixed effects to estimate the effect of food stamps on the birth weight distribution.

Remark 5 (Interpretation as a minimum distance estimator). Our estimator can be written as an MD estimator, where the second stage imposes restrictions on the first-stage

\footnotetext{\({ }^{13}\) An IV extension of the MD estimator by Galvao and Wang (2015) is suggested in Dai and Jin (2021).
}
coefficients. For simplicity, in this remark, we consider the case where all the regressors are exogenous and \(Z=X\). Define\\[0pt]
[

\textbackslash underset\{\textbackslash left(K\_\{1\}+1\textbackslash right) \textbackslash times K\}\{R\_\{j\}\}=\textbackslash left(\[
\begin{array}{cc}
0 & x_{2 j}^{\prime} \\
I_{K_{1}} & 0
\end{array}
\]\textbackslash right)\\[0pt]
]

such that \(\tilde{X}_{j} R_{j}=X_{j}\). It follows that our MD estimator minimizes

\[
\begin{aligned}
\hat{\delta}(\tau) & =\underset{\delta}{\arg \min } \sum_{j=1}^{m}\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-X_{j} \delta\right)^{\prime}\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-X_{j} \delta\right) \\
& =\underset{\delta}{\arg \min } \sum_{j=1}^{m}\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-\tilde{X}_{j} R_{j} \delta\right)^{\prime}\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-\tilde{X}_{j} R_{j} \delta\right) \\
& =\underset{\delta}{\arg \min } \sum_{j=1}^{m}\left(\hat{\beta}_{j}(\tau)-R_{j} \delta\right)^{\prime} \tilde{X}_{j}^{\prime} \tilde{X}_{j}\left(\hat{\beta}_{j}(\tau)-R_{j} \delta\right),
\end{aligned}
\]

which corresponds to the definition of a weighted minimum distance estimator that imposes the linear restrictions \(\beta_{j}(\tau)=R_{j} \delta(\tau)\) with weights \(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\).

Thus, our estimator is an MD estimator. However, it does not correspond to the textbook definition of a "classical minimum distance" estimator. \({ }^{14}\) In the classical MD setup, all the sampling variance arises in the first stage: if we know the first stage coefficients, we know the final coefficients. It follows that the efficient weighting matrix \(\tilde{W}(\tau)\) is the inverse of the first-stage variance. In our case, the second stage also contributes to the variance due to the presence of the group effects \(\alpha\left(\tau, v_{j}\right)\). Even if we know \(\beta_{j}(\tau)\) (for a finite number of groups), we cannot exactly pinpoint \(\gamma(\tau)\). The group effects play a role similar to misspecification in the classical MD, but with our estimator, the resulting bias disappears asymptotically as the number of groups increases. This is the second important difference: the dimension of our first stage estimates increases with the sample size while it is fixed for classical MD estimators.

\subsection*{2.3 Least Squares Minimum Distance Estimators}
This paper proposes a two-step estimator in which the first stage involves performing quantile regressions within each group. Although this approach might seem unusual and specific to quantile models, we demonstrate in this subsection that this method yields numerically identical results to traditional least squares panel estimators, provided that OLS is applied in the first stage. A more detailed discussion, including formal statements, is provided in Appendix B.1, with proofs available in Appendix B.2.

Consider first a model with group effects and individual-level regressors

\[
y_{i j}=x_{1 i j}^{\prime} \beta+\alpha_{j}+\varepsilon_{i j} .
\]

Typically, when estimating this model with fixed effects, most researchers apply the within

\footnotetext{\({ }^{14}\) See section 14.6 in Wooldridge (2010).
}
transformation, which is widely recognized as being equivalent to a dummy variable regression. \({ }^{15}\) Yet, a third equivalent method exists for computing the least squares fixed effects estimator, which involves exploiting the exogenous within-group variation using instrumental variables. Setting \(\dot{x}_{1 i j}\) as an instrument for \(x_{1 i j}\) in an instrumental variable regression is numerically identical to the least squares fixed effects.

Corollary 1 in Appendix B. 1 presents a fourth way to compute the least squares fixed effects estimator, which aligns with our paper's approach. This minimum distance method involves two steps: first, regressing with OLS the dependent variable on \(x_{1 i j}\) within each group, then using IV to regress the first-stage fitted values on \(x_{1 i j}\), with \(\dot{x}_{1 i j}\) as the instrument. This approach offers the most computationally efficient alternative for quantile estimation, as it divides the problem into two convex optimization steps rather than a single nonconvex IV quantile regression, and it avoids the challenges of high-dimensional quantile regression.

The two-step procedure is not specific to fixed effects but applies to a wide range of estimators. Proposition 5 in Appendix B shows that the MD least squares estimator is algebraically identical to the one-step GMM regression of \(y_{i j}\) on \(x_{i j}\) under the condition that for each group \(j\), the matrix of instruments lies in the column space of the matrix of first stage regressors. \({ }^{16}\) For example, \(\dot{x}_{1 i j}, \bar{x}_{1 j}\), and \(x_{2 j}\) satisfy the condition.

We extend the model by incorporating group-level regressors, \(x_{2 j}\) :

\[
y_{i j}=x_{1 i j}^{\prime} \beta+x_{2 j}^{\prime} \gamma+\alpha_{j}+\varepsilon_{i j}
\]

By selecting different instrumental variables for the second-step GMM regression, we can numerically obtain the most common least squares panel data estimators. For example, using the group-averaged variables, \(\bar{x}_{1 j}\) and \(x_{2 j}\), as instruments yields the between estimator. Instrumental variable approaches are also available for random effects estimation. Although FGLS is the most common estimator for the random effects model, Im et al. (1999) show that the overidentified 3SLS estimator, with instruments \(\dot{x}_{1 i j}, \bar{x}_{1 j}\), and \(x_{2 j}\), is numerically identical to the random effects estimator. Since 3SLS is a special case of a GMM estimator, using the first-stage fitted values as dependent variables does not change the estimates. Alternatively, the random effects estimator can be implemented using the theory of optimal instrument with a just identified 2SLS regression (see Im et al., 1999; Hansen, 2022b). Additionally, the Hausman and Taylor (1981) estimator can be implemented by selecting the following instruments: \(\dot{x}_{1 i j}\) and the group average of the exogenous regressors. External instruments might also be included.

\footnotetext{\({ }^{15}\) In the context of traditional panel data, we would refer to the \(j\) units as "individuals" and the \(i\) units as "time periods". Thus, this transformation corresponds to the time-demeaning applied in the traditional panel data literature, eliminating time-invariant individual effects.\\
\({ }^{16}\) The intuition is as follows. The fitted values of the first-stage least squares regression can be written as \(P_{X_{j}} Y_{j}\) where \(P_{X_{j}}\) is the first-stage least squares projection matrix of group \(j\). If the instrument matrix, \(Z_{j}\), is in the column space of \(\tilde{X}_{j}\), it follows that \(P_{X_{j}} Z_{j}=Z_{j}\). Therefore, \(Z^{\prime} \hat{Y}=Z^{\prime} Y\) and the two GMM regressions are numerically identical.
}\section*{3 Asymptotic Theory}
\subsection*{3.1 Preliminaries: Assumptions, Consistency, and Sample Moments}
In this section, we state the assumptions and present the asymptotic results. All the proofs are included in Appendix A. For simplicity of notation, in the following, we write \(\alpha_{j}(\tau)\) instead of \(\alpha\left(\tau, v_{j}\right)\). We prove weak uniform consistency and weak convergence of the whole quantile regression process for \(\tau \in \mathcal{T}\), where \(\mathcal{T}\) is a compact set included in \((0,1)\). The symbol \(\ell^{\infty}(\mathcal{T})\) denotes the set of component-wise bounded vector valued function of \(\mathcal{T}, \rightsquigarrow\) denotes weak convergence, and for a random variable \(h_{i j}, \mathbb{E}_{i \mid j}\left[h_{i j}\right]\) indicates the expectation over \(i\) in group \(j\).

We start by writing the sampling error of \(\hat{\delta}(\hat{W}, \tau)\) as a sum of a component arising from the first stage estimation error of \(\beta_{j}(\tau)\) and a component arising from the second stage noise \(\alpha_{j}(\tau)\) :

Lemma 1 (Sampling error). Assume that the model in equation (1) holds, then

\[
\hat{\delta}(\hat{W}, \tau)-\delta(\tau)=\hat{G}(\tau) \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j}\left(\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+\alpha_{j}(\tau)\right)
\]

where \(\hat{G}(\tau)=\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau)\) and \(S_{Z X}=\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}\).\\
To keep the notation light, we suppress the dependency of \(\hat{G}(\tau)\) on \(\hat{W}(\tau)\). We now state assumptions that ensure that both components are well-behaved. For the analysis of the first stage estimator, we rely on results derived in Galvao et al. (2020) and make the assumptions required in their Theorem 2:

Assumption 1 (Sampling). (i) The processes \(\left\{\left(y_{i j}, x_{i j}, z_{i j}\right): i=1, \ldots, n\right\}\) are independent across \(j\). (ii) For each \(j\), the observations \(\left(y_{i j}, x_{i j}, z_{i j}\right)_{i=1, \ldots, n}\) are i.i.d. across \(i\).

Assumption 2 (Covariates). (i) For all \(j=1, \ldots, m\) and all \(i=1, \ldots, n,\left\|x_{i j}\right\| \leq C\) almost surely. (ii) The eigenvalues of \(\mathbb{E}_{i \mid j}\left[\tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right]\) are bounded away from zero and infinity uniformly across \(j\).

Assumption 3 (Conditional distribution). The conditional distribution \(F_{y_{i j} \mid x_{1 i j}, v_{j}}(y \mid x, v)\) is twice differentiable w.r.t. \(y\), with the corresponding derivatives \(f_{y_{i j} \mid x_{1 i j}, v_{j}}(y \mid x, v)\) and \(f_{y_{i j} \mid x_{1 i j}, v_{j}}^{\prime}(y \mid x, v)\). Further, assume that

\[
f_{\max }=\sup _{j} \sup _{y \in \mathbb{R}, x \in \mathcal{X}}\left|f_{y_{i j} \mid x_{1 i j}, v_{j}}(y \mid x, v)\right|<\infty
\]

and

\[
\bar{f}^{\prime}=\sup _{j} \sup _{y \in \mathbb{R}, x \in \mathcal{X}}\left|f_{y_{i j} \mid x_{1 i j}, v_{j}}^{\prime}(y \mid x, v)\right|<\infty
\]

where \(\mathcal{X}\) is the support of \(x_{1 i j}\)

Assumption 4 (Bounded density). There exists a constant \(f_{\text {min }}<f_{\text {max }}\) such that

\[
0<f_{\min } \leq \inf _{j} \inf _{\tau \in \mathcal{T}} \inf _{x \in \mathcal{X}} f_{y_{i j} \mid x_{1 i j}, v_{j}}\left(Q\left(\tau, y_{i j} \mid x, v\right) \mid x, v\right)
\]

These are quite standard assumptions in the quantile regression literature. In Assumption 1, we assume that the processes are independent across \(j\); this assumption can also be relaxed by allowing for clustering between groups. We also assume that the observations are i.i.d. within groups, but this can be relaxed at the cost of a more complex notation by applying Theorem 4 in Galvao et al. (2020), which requires only stationarity and \(\beta\)-mixing. The estimator of the asymptotic variance that we suggest below is consistent in both cases. Assumption 2 requires that the regressors are bounded and that \(\mathbb{E}_{i \mid j}\left[\tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right]\) is invertible. Assumptions 3 and 4 impose smoothness and boundedness of the conditional distribution, the density, and its derivatives.

For the second stage GMM regression we impose the following assumptions:\\
Assumption 5 (Instruments). (i) For all \(j=1, \ldots, m\) and all \(i=1, \ldots, n,\left\|z_{i j}\right\| \leq C\) a.s. (ii) For all \(j=1, \ldots, m\) and all \(i=1, \ldots, n, \mathbb{E}\left[z_{i j} \alpha_{j}(\tau)\right]=0\). (iii) For all \(j=1, \ldots, m\) and all \(i=1, \ldots, n, y_{i j}\) is independent of \(z_{i j}\) conditional on \(\left(x_{i j}, v_{j}\right)\). (iv) As \(m \rightarrow \infty\), \(m^{-1} \sum_{j=1}^{m} \mathbb{E}_{i \mid j}\left[z_{i j} x_{i j}^{\prime}\right] \rightarrow \Sigma_{Z X}\) where the singular values of \(\Sigma_{Z X}\) are bounded from below and from above.

\section*{Assumption 6 (Group effects).}
(i) For all \(j=1, \ldots, m, \mathbb{E}\left[\sup _{\tau \in \mathcal{T}}\left|\alpha_{j}(\tau)\right|^{4+\varepsilon_{C}}\right] \leq C\) for \(\varepsilon_{C}>0\). (ii) For some (matrix-valued) function \(\Omega_{2}: \mathcal{T} \times \mathcal{T} \rightarrow \mathbb{R}^{L \times L}, m^{-1} \sum_{j=1}^{m} \mathbb{E}_{i \mid j}\left[\alpha_{j}\left(\tau_{1}\right) \alpha_{j}\left(\tau_{2}\right) z_{i j} z_{i j}^{\prime}\right]{ }_{p} \Omega_{2}\left(\tau_{1}, \tau_{2}\right)\) uniformly over \(\tau_{1}, \tau_{2} \in \mathcal{T}\). (iii) For all \(\tau_{1}, \tau_{2} \in \mathcal{T},\left|\alpha_{j}\left(\tau_{2}\right)-\alpha_{j}\left(\tau_{1}\right)\right| \leq C\left|\tau_{2}-\tau_{1}\right|\).

These assumptions are the same as in Chetverikov et al. (2016). For the instrumental variables, we assume that (i) they are bounded, (ii) they are not correlated with the group effect (exclusion restriction), (iii) they do not affect the first stage estimation (this is often satisfied by construction, e.g. when the instruments do not vary within individuals or are a linear transformation of the first stage regressors), and (iv) they satisfy the relevance conditions. For the group effects, we assume that they have a finite fourth moment, and the average variance of \(z_{i j} \alpha_{j}(\tau)\) converges to a well-defined matrix.

Since the unobserved heterogeneity \(\alpha_{j}(\tau)\) is group-specific, we require that the number of groups \(m\) diverges to infinity. The first stage quantile regression estimator is a nonlinear estimator that is potentially biased in finite samples. Hence, the number of observations per group, \(n\), must also diverge to infinity for consistency. Galvao et al. (2020) show that the bias is approximately of order \(1 / n\). For unbiased asymptotic normality, we need the bias to shrink faster than the standard deviation of the estimator. We will see that some elements of \(\hat{\delta}(\hat{W}, \tau)\) converge at the \(\sqrt{m}\) rate such that we need that \(n\) goes to infinity more quickly than \(\sqrt{m}\). On the other hand, other elements converge at the \(\sqrt{m n}\) rate so that \(n\) must go to infinity more quickly than \(m\). We state these three different relative growth rates in the following assumption:

Assumption 7 (Growth rates). As \(m \rightarrow \infty\), we have\\
(a) \(\frac{\log m}{n} \rightarrow 0\),\\
(b) \(\frac{\sqrt{m} \log n}{n} \rightarrow 0\),\\
(c) \(\frac{m(\log n)^{2}}{n} \rightarrow 0\).

Finally, we assume that the estimated weighting matrix uniformly converges to a strictly positive definite matrix that is continuous in the quantile index.

Assumption 8 (Full-rank weighting matrix). Uniformly in \(\tau \in \mathcal{T}, \hat{W}(\tau) \underset{p}{\rightarrow} W(\tau)\) where \(W(\tau)\) is strictly positive definite and, for all \(\tau_{1}, \tau_{2} \in \mathcal{T},\left\|W\left(\tau_{2}\right)-W\left(\tau_{1}\right)\right\| \leq C\left|\tau_{2}-\tau_{1}\right|\).

Our first result establishes the uniform consistency of our estimator under the weakest growth rate condition:

Theorem 1 (Uniform consistency). Let the model in equation (1), Assumptions 1-6, 7(a), and 8 hold. Then,

\[
\sup _{\tau \in \mathcal{T}}\|\hat{\delta}(\tau)-\delta(\tau)\|=o_{p}(1) .
\]

We now study the asymptotic distribution of our estimator. In Lemma 1, we see that the sample moment condition is the sum of two terms. It is useful to consider them separately:

\[
\begin{aligned}
& \bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)=\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right) \\
& \bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)=\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \alpha_{j}(\tau)
\end{aligned}
\]

such that total moment condition is the sum of both components: \(\bar{g}_{m n}(\hat{\delta}, \tau)=\bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)+\) \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)=\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} g_{i j}(\hat{\delta}, \tau)\). Lemma 2 establishes joint asymptotic normality for the entire moment condition processes.

Lemma 2 (Asymptotic distribution of the sample moments). Let the model in equation (1), and Assumptions 1-6 hold.\\
(i) Under Assumption 7(c), as \(m \rightarrow \infty\),

\[
\sqrt{m n} \bar{g}_{m n}^{(1)}(\hat{\delta}, \cdot) \rightsquigarrow \mathbb{Z}_{1}(\cdot) \text {, in } \ell^{\infty}(\mathcal{T}),
\]

where \(\mathbb{Z}_{1}(\cdot)\) is a mean-zero Gaussian process with uniformly continuous sample paths and covariance function \(\Omega_{1}\left(\tau, \tau^{\prime}\right)=\mathbb{E}\left[\Sigma_{Z X j} V_{j}\left(\tau, \tau^{\prime}\right) \Sigma_{Z X j}^{\prime}\right]\) with \(\Sigma_{Z X j}=\mathbb{E}_{i \mid j}\left[z_{i j} \tilde{x}_{i j}^{\prime}\right]\) and \(V_{j}\left(\tau, \tau^{\prime}\right)\) is the asymptotic variance-covariance matrix of \(\hat{\beta}_{j}(\tau)\) and \(\hat{\beta}_{j}\left(\tau^{\prime}\right)\) :

\[
\begin{aligned}
& V_{j}\left(\tau, \tau^{\prime}\right)=\mathbb{E}_{i \mid j}\left[f_{y \mid x}\left(Q_{y \mid x, \nu_{j}}\left(\tau \mid \tilde{x}_{i j}\right) \mid \tilde{x}_{i j}\right) \tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right]^{-1}\left(\min \left(\tau, \tau^{\prime}\right)-\tau \tau^{\prime}\right) \mathbb{E}_{i \mid j}\left[\tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right] \\
& \times \mathbb{E}_{i \mid j}\left[f_{y \mid x}\left(Q_{y \mid x, \nu_{j}}\left(\tau^{\prime} \mid \tilde{x}_{i j}\right) \mid \tilde{x}_{i j}\right) \tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right]^{-1}
\end{aligned}
\]

(ii) Under Assumption 7(b), As \(m \rightarrow \infty\),

\[
\sqrt{m} \bar{g}_{m n}^{(2)}(\hat{\delta}, \cdot) \rightsquigarrow \mathbb{Z}_{2}(\cdot) \text {, in } \ell^{\infty}(\mathcal{T}) \text {, }
\]

where \(\mathbb{Z}_{2}(\cdot)\) is a mean-zero Gaussian process with uniformly continuous sample paths and covariance function \(\Omega_{2}\left(\tau, \tau^{\prime}\right)\), which is defined in Assumption 6(ii).\\
(iii) Under Assumption \(7(c)\), as \(m \rightarrow \infty, \sup _{\tau, \tau^{\prime} \in \mathcal{T}}\left\|\operatorname{Cov}\left(\bar{g}_{m n}^{(1)}(\hat{\delta}, \tau), \bar{g}_{m n}^{(2)}\left(\hat{\delta}, \tau^{\prime}\right)\right)\right\|=o_{p}\left(\frac{1}{\sqrt{m n}}\right)\).\\
\(\bar{g}_{m n}^{(1)}(\hat{\delta}, \cdot)\) reflects the estimation error that arises in the first-stage quantile regression estimation. Since the first-stage regressors vary within groups, the relevant number of observations is \(m n\), and correspondingly, the variance is proportional to \(1 /(m n)\). On the other hand, since the expected bias of the first-stage quantile regression is of order \(1 / n\), for asymptotic unbiasedness, we must require that \(n\) goes to infinity slightly faster than \(m\). In the proof, we build on results derived in Volgushev et al. (2019) and in Galvao et al. (2020). \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \cdot)\) reflects the estimation error due to the randomness in \(\alpha_{j}(\tau)\). This moment can also be interpreted as the moment that would be relevant if we knew \(\beta_{j}(\tau)\). Since \(\alpha_{j}(\tau)\) varies only between groups, the relevant number of observations here is \(m\) and, accordingly, the variance of this moment converges at the slower rate of \(1 / m\). For asymptotic unbiasedness, we need only the weaker condition \(7(\mathrm{~b})\), which requires that \(n\) goes to infinity slightly faster than \(\sqrt{m}\).

\subsection*{3.2 Asymptotic Distribution when the Degree of Heterogeneity is Known}
The sample moment condition \(\bar{g}_{m n}(\hat{\delta}, \tau)\) is thus the sum of two components that converge to zero at different rates. The asymptotic distribution of the estimator is dominated by the component with the slower rate of convergence, \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)\), except its variance is zero, which is the case if \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)=0\) or if \(\bar{z}_{j}=\frac{1}{n} \sum_{i=1}^{n} z_{i j}=0\) for \(j=1, \ldots, n\). Since the degree of grouplevel heterogeneity affects this variance, it is useful to consider three cases: strong, no, and weak heterogeneity. In this subsection, we derive the asymptotic distribution of our estimator when the degree of heterogeneity is known. We suggest adaptive estimation and inference procedures in the following subsection.

Case 1: Strong group-level heterogeneity. We start with the case of strong heterogeneity that we define to be \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)>\varepsilon>0\) uniformly in \(\tau\). The variance of an element of the vector \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)=\frac{1}{m n} \sum_{j=1}^{m} \bar{z}_{j} \alpha_{j}(\tau)\) equals to zero when the corresponding instrument satisfies \(\bar{z}_{j}=0\) for all \(j\). For this reason, we distinguish between two sorts of instruments: \(L_{1}\) instruments in \(z_{1 i j}\) satisfy \(\bar{z}_{1 j}=n^{-1} \sum_{i=1}^{n} z_{1 i j}=0\) for all \(j\), while \(L_{2}\) instruments in \(z_{2 i j}\) satisfy \(\bar{z}_{2 j} \neq 0\) at least for some groups \(j .{ }^{17}\) We order the instruments such that \(z_{i j}=\left(z_{1 i j}^{\prime}, z_{2 i j}^{\prime}\right)^{\prime}\). It

\footnotetext{\({ }^{17}\) Note that all the instruments that vary only within groups can be normalized to have mean zero. For instance, we can identify the effect of the individual-level variable \(x_{1 i j}\) by using the instrument \(\dot{x}_{1 i j}\), which has a zero mean in all groups.
}
follows that
\[
\bar{g}_{m n}(\hat{\delta}, \tau)=\binom{\bar{g}_{m n, 1}(\hat{\delta}, \tau)}{\bar{g}_{m n, 2}(\hat{\delta}, \tau)}=\binom{\bar{g}_{m n, 1}^{(1)}(\hat{\delta}, \tau)}{\bar{g}_{m n, 2}^{(1)}(\hat{\delta}, \tau)+\bar{g}_{m n, 2}^{(2)}(\hat{\delta}, \tau)}
\]
where \(g_{m n, 1}(\hat{\delta}, \tau)\) is a \(L_{1} \times 1\) vector and \(g_{m n, 2}(\hat{\delta}, \tau)\) is a \(L_{2} \times 1\) vector. Thus, in the case of strong heterogeneity, some moments converge at the fast rate \(\sqrt{m n}\) while others converge at the slow rate \(\sqrt{m}\).

We order the coefficients in \(\delta(\tau)\) (and the corresponding regressors in \(x_{i j}\) ) such that the first \(M_{1}\) elements are identified using only the \(L_{1}\) fast moments while the remaining \(M_{2}\) elements require the \(L_{2}\) slow moments for identification. We denote by \(\delta_{1}(\tau)\) the former and by \(\delta_{2}(\tau)\) the latter coefficients. Formally, we partition \(\Sigma_{Z X}\) such that it is block lower triangular:

\[
\Sigma_{Z X}=\left(\begin{array}{cc}
\Sigma_{11} & \Sigma_{12} \\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)=\left(\begin{array}{cc}
\Sigma_{11} & 0 \\
\Sigma_{21} & \Sigma_{22}
\end{array}\right)
\]

where \(\Sigma_{11}\) is a full column rank \(L_{1} \times M_{1}\) matrix, \(\Sigma_{12}\) is \(L_{1} \times M_{2}, \Sigma_{21}\) is \(L_{2} \times M_{1}\) and \(\Sigma_{22}\) is \(L_{2} \times M_{2}\). Assumption 5 (iv) implies that \(\Sigma_{22}\) also has full column rank. Note that \(L_{1}\) and \(M_{1}\) can be equal to zero such that equation (15) is a definition and not an assumption. Accordingly, the adaptive estimation and testing procedures suggested in Section 3.3 do not require the user to classify the instruments or the regressors.

In the exactly identified case, our estimator simplifies to the instrumental variable estimator such that

\[
\hat{G}(\tau)=S_{Z X}^{-1} \underset{p}{\rightarrow} \Sigma_{Z X}^{-1}=\left(\begin{array}{cc}
\Sigma_{11}^{-1} & 0 \\
-\Sigma_{22}^{-1} \Sigma_{21} \Sigma_{11}^{-1} & \Sigma_{22}^{-1}
\end{array}\right)
\]

and the first-order asymptotic distributions can be written as

\[
\begin{aligned}
& \sqrt{m n}\left(\hat{\delta}_{1}(\tau)-\delta_{1}(\tau)\right){ }_{d} \Sigma_{11}^{-1} \sqrt{m n} \bar{g}_{m n, 1}^{(1)}(\hat{\delta}, \tau) \\
& \sqrt{m}\left(\hat{\delta}_{2}(\tau)-\delta_{2}(\tau)\right){\underset{d}{ } \Sigma_{22}^{-1} \sqrt{m} \bar{g}_{m n, 2}^{(2)}(\hat{\delta}, \tau)}^{2}
\end{aligned}
\]

\(\hat{\delta}_{1}(\tau)\) is only a function of \(\bar{g}_{m n}^{(1)}\) and converges thus at the \(\sqrt{m n}\) rate. \(\hat{\delta}_{2}(\tau)\) depends on both \(\bar{g}_{m n}^{(1)}\) and \(\bar{g}_{m n}^{(2)}\) but its first-order asymptotic distribution is dominated by the slower \(\bar{g}_{m n}^{(2)}\) term.

Example 1. We can illustrate this notation with a simple example. Consider the case of one individual-level variable \(x_{1 i j}\), one group-level variable \(\tilde{x}_{2 j}\), and a constant. As instrumental variables, we use \(\left(\dot{x}_{1 i j}, \tilde{x}_{2 j}, 1\right)\), which corresponds to applying the fixed effects estimator for the coefficient on \(x_{1 i j}\) and the between estimator for the other two coefficients. In this case, only the first instrument has mean zero in all groups such that \(L_{1}=1\) and \(L_{2}=2\). By construction, \(\dot{x}_{1 i j}\) is uncorrelated with \(\tilde{x}_{2 j}\) and with the constant such that \(\Sigma_{Z X}\) is block lower diagonal as defined in equation (15) with \(M_{1}=1\) and \(M_{2}=2\). The coefficient on \(x_{1 i j}\) converges at the fast rate because it is not affected by the group-level effects \(\alpha_{j}(\tau)\). On the other hand, the intercept and the coefficient on the group-level variable \(\tilde{x}_{2 j}\) converge only at the slow rate of convergence \(\sqrt{m}\) because they are affected by the group effects. In a many application, we see that \(M_{1}=K_{1}\), but\\
this is not always true. For instance, in the previous example, all coefficients would converge at the slow rate if we used \(\left(x_{1 i j}, \tilde{x}_{2 j}, 1\right)\) as instrumental variables such that \(L_{1}\) and \(M_{1}\) would be equal to zero.

In an overidentified model, using a full-rank weighting matrix \(W(\tau)\) as in Assumption 8 can lead to contamination of the entire parameter vector \(\hat{\delta}(\tau)\) by the slower-converging moments. This occurs because \(\hat{G}(\tau)\) will not retain a block-lower-triangular structure, even if \(S_{Z X}\) does. In Example 2 below, this issue arises with the 2SLS estimator of \(\delta_{1}(\tau)\), which converges at the slow \(\sqrt{m}\) rate. In contrast, both the exactly identified IV estimator and the efficient GMM estimator achieve the faster \(\sqrt{m n}\) rate.

To avoid contamination, we must give asymptotically infinitely higher weights to fast moments than slow ones. The following assumption imposes this critical condition on the weighting matrix. \({ }^{18}\)

Assumption 8' (Heterogeneous weighting matrix). Uniformly in \(\tau \in \mathcal{T}\),

\[
\hat{W}(\tau)=\underbrace{\left(\begin{array}{cc}
W_{11}(\tau) & a_{n}(\tau) W_{12}(\tau) \\
a_{n}(\tau) W_{21}(\tau) & a_{n}(\tau) W_{22}(\tau)
\end{array}\right)}_{W_{m n}(\tau)}+\left(\begin{array}{cc}
o_{p}(1) & o_{p}\left(\sqrt{a_{n}(\tau)}\right) \\
o_{p}\left(\sqrt{a_{n}(\tau)}\right) & o_{p}\left(a_{n}(\tau)\right)
\end{array}\right)
\]

where \(a_{n}(\tau)=\frac{1}{1+\operatorname{Var}\left(\alpha_{j}(\tau)\right) n} . W_{11}(\tau)\) and \(W_{22}(\tau)\) are respectively a \(L_{1} \times L_{1}\) and a \(L_{2} \times L_{2}\) full rank matrix. For all \(l_{1}, l_{2} \in\{1,2\}\) and \(\tau_{1}, \tau_{2} \in \mathcal{T},\left\|W_{l_{1} l_{2}}\left(\tau_{2}\right)-W_{l_{1} l_{2}}\left(\tau_{1}\right)\right\| \leq C\left|\tau_{2}-\tau_{1}\right|\).

Example 2. Consider an extension of Example 1 where the vector of instrumental variable is now \(\left(\dot{x}_{1 i j}, \bar{x}_{1 j}, x_{2 j}, 1\right)\). This model is overidentified with \(L_{1}=1, L_{2}=3\), and \(K=3\). When we impose Assumption 8, \(W(\tau)\) is a full rank weighting matrix, and the estimator converges only at the \(\sqrt{m}\) rate. We can obtain a faster-converging estimator by giving asymptotically infinitely more weight to \(\dot{x}_{i j}\) than to the other instruments. When we impose Assumption \(8^{\prime}\), the coefficient on \(x_{1 i j}\) is estimated at the \(\sqrt{m n}\) rate. We formalize these results in Theorem 2 below.

Case 2: no group-level heterogeneity. We now consider the case where there is no heterogeneity across groups, i.e., when \(\alpha_{j}(\tau)=0\) uniformly in \(j\) and \(\tau\). When this occurs, \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \cdot)=0\) such that all the variance arises from the first stage moment, which can be estimated at the fast rate \(\sqrt{m n}\). This case corresponds to the textbook definition of a classical minimum distance estimator. All the coefficients converge at the fast \(\sqrt{m n}\) rate, and the asymptotic distribution of \(\hat{\delta}(\tau)\) can be derived straightforwardly. By comparing Case 1 and 2 , we notice that the limiting distribution and even the rate of convergence of the \(M_{2}\) coefficients \(\hat{\delta}_{2}(\tau)\) differ. Their asymptotic distribution is discontinuous at 0 in the variance of \(\alpha_{j}(\tau)\), and in many applications, we do not know whether there is group-level heterogeneity. For this reason, we discuss adaptive estimation and inference in the next subsection.

\footnotetext{\({ }^{18}\) The intuition behind the sequence \(a_{n}(\tau)\) will be elucidated in the following subsection, where we delve into a comprehensive discussion of our efficient GMM estimator.
}Remark 6 (Related literature). Chamberlain (1994) considers a setting with a finite number of design points (groups in our terminology), exogenous group-level variables, and no individuallevel variables. His correctly specified case corresponds to our Case 2 (no heterogeneity). Accordingly, our asymptotic distribution corresponds to his in this special case. \({ }^{19}\) Chamberlain (1994) also considers a misspecified case. We can interpret his misspecification errors as our group effects. However, in this case, he considers pseudo-true parameters that absorb a nonvanishing bias while we allow the number of groups to go to infinity to avoid bias. For this reason, our results fundamentally differ from his results in Case 1.

Chetverikov et al. (2016) do not consider Case 2 (no heterogeneity) in their theoretical results even if, interestingly, it corresponds to one of their data-generating processes in their simulations. In Case 2, their matrix-valued function \(J\left(u_{1}, u_{2}\right)\) defined in their Assumption 6(ii) is uniformly equal to zero. This implies that their asymptotic covariance function \(\mathcal{C}\left(u_{1}, u_{2}\right)\) defined in their Theorem 1 is also uniformly equal to 0 . This degenerate asymptotic distribution indicates that the convergence rate is faster than \(\sqrt{m}\) in this case.

Case 3: weak group-level heterogeneity. In Case 1, the second-stage variance dominates the asymptotic distribution of \(\hat{\delta}_{2}(\tau)\), while in Case 2 , the first-stage variance dominates. It is interesting to consider the intermediate case when group-level heterogeneity is present but vanishes exactly at the right rate such that both components of the variance matter asymptotically. This should provide a good approximation for the applications where the first-stage and the second-stage variances are similar. Formally, we assume that \(\Omega_{2}\left(\tau_{1}, \tau_{2}\right)\), the covariance function of \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)\) defined in Assumption 6(ii), converges to zero at the \(n\) rate:

\[
\Omega_{2}\left(\tau_{1}, \tau_{2}\right)=n^{-1} \bar{\Omega}_{2}\left(\tau_{1}, \tau_{2}\right)
\]

Under this assumption, \(\bar{g}_{m n}^{(1)}(\hat{\delta}, \cdot)\) and \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \cdot)\) converge at the same rate. Lemma 2 implies then

\[
\sqrt{m n} \bar{g}_{m n}(\hat{\delta}, \cdot) \rightsquigarrow \mathbb{Z}(\cdot), \text { in } \ell^{\infty}(\mathcal{T}),
\]

where \(\mathbb{Z}(\cdot)\) is a mean-zero Gaussian process with uniformly continuous sample paths and covariance function \(\Omega_{1}\left(\tau, \tau^{\prime}\right)+\bar{\Omega}_{2}\left(\tau, \tau^{\prime}\right)\).

Theorem 2 formally states the results for these three cases. Parts (i) provides the asymptotic distribution for the fast and slow coefficients when there is strong heterogeneity, part (ii) in the absence of heterogeneity, and part (iii) when there is weak heterogeneity.

Theorem 2 (Asymptotic distribution when the degree of heterogeneity is known). Let Assumptions 1-6 hold.\\
(i) Case 1 (strong heterogeneity): \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)>\varepsilon>0\) uniformly in \(\tau\) and Assumption \(8^{\prime}\) holds.

\footnotetext{\({ }^{19}\) In the absence of group-level heterogeneity, the asymptotic distribution of \(\hat{\delta}(\tau)\) stated in part (ii) of Theorem 2 is also valid when the number of groups is fixed.
}
(a) In addition, let Assumption 7(c) hold. Then,
\[
\sqrt{m n}\left(\hat{\delta}_{1}(\hat{W}(\cdot), \cdot)-\delta_{1}(\cdot)\right) \rightsquigarrow G_{11}(\cdot) \mathbb{Z}_{11}(\cdot), \text { in } \ell^{\infty}(\mathcal{T})
\]
where \(G_{11}(\tau)=\left(\Sigma_{11}^{\prime} W_{11}(\tau) \Sigma_{11}\right)^{-1} \Sigma_{11}^{\prime} W_{11}(\tau)\) and \(\mathbb{Z}_{11}(\cdot)\) is the Gaussian process consisting of the first \(L_{1}\) elements of \(\mathbb{Z}_{1}(\cdot)\) defined in Lemma 2(i).\\
(b) In addition, let Assumption 7(b) hold. Then,
\[
\sqrt{m}\left(\hat{\delta}_{2}(\hat{W}(\cdot), \cdot)-\delta_{2}(\cdot)\right) \rightsquigarrow G_{22}(\cdot) \mathbb{Z}_{22}(\cdot), \text { in } \ell^{\infty}(\mathcal{T})
\]
where \(G_{22}(\tau)=\left(\Sigma_{22}^{\prime} W_{22}(\tau) \Sigma_{22}\right)^{-1} \Sigma_{22}^{\prime} W_{22}(\tau)\) and \(\mathbb{Z}_{22}(\cdot)\) is the Gaussian process consisting of the last \(L_{2}\) elements of \(\mathbb{Z}_{2}(\cdot)\) defined in Lemma 2(ii).\\
(ii) Case 2 (no heterogeneity): \(\alpha_{j}(\tau)=0\) uniformly in \(j\) and \(\tau\). In addition, Assumption \({ }^{7}(c)\) and 8 hold. Then,
\[
\sqrt{m n}(\hat{\delta}(\hat{W}(\cdot), \cdot)-\delta(\cdot)) \rightsquigarrow G(\cdot) \mathbb{Z}_{1}(\cdot), \text { in } \ell^{\infty}(\mathcal{T})
\]
where \(G(\tau)=\left(\Sigma_{Z X}^{\prime} W(\tau) \Sigma_{Z X}\right)^{-1} \Sigma_{Z X}^{\prime} W(\tau)\) and \(\mathbb{Z}_{1}(\cdot)\) is defined in Lemma 2(i).\\
(iii) Case 3 (weak heterogeneity): \(\Omega_{2}\left(\tau, \tau^{\prime}\right)=n^{-1} \bar{\Omega}_{2}\left(\tau, \tau^{\prime}\right)\). Assumption 8 and \(7(c)\) hold. Then,
\[
\sqrt{m n}(\hat{\delta}(\hat{W}(\cdot), \cdot)-\delta(\cdot)) \rightsquigarrow G(\cdot) \mathbb{Z}(\cdot), \text { in } \ell^{\infty}(\mathcal{T})
\]
where \(G(\tau)=\left(\Sigma_{Z X}^{\prime} W(\tau) \Sigma_{Z X}\right)^{-1} \Sigma_{Z X}^{\prime} W(\tau)\) and \(\mathbb{Z}(\cdot)\) is a mean-zero Gaussian process with uniformly continuous sample paths and covariance function \(\Omega_{1}\left(\tau, \tau^{\prime}\right)+\bar{\Omega}_{2}\left(\tau, \tau^{\prime}\right)\).

These asymptotic results are useful for understanding the mechanics behind our estimator, but they have several weaknesses. First, the asymptotic distribution of \(\hat{\delta}_{1}(W, \tau)\) in part (i)-(a) is only a function of the fast instruments. The same asymptotic distribution can be obtained by ignoring the slow instruments. Consider Example 2, including \(\bar{x}_{1 j}\) as an instrument does not reduce the asymptotic variance of \(\hat{\delta}_{1}(W, \tau)\) even if this instrument is valid and the between-group variation in \(x_{1 i j}\) is non-negligible. In other words, the random effects estimator is asymptotically equivalent to the fixed effects estimator. \({ }^{20}\) In some applications, the random effects estimator is appreciably more precise such that we would like to exploit the between-group variation efficiently. Second, the asymptotic distribution and the convergence rate of the slow coefficients \(\hat{\delta}_{2}(W, \tau)\) are different, depending on whether there is group-level heterogeneity or not. Thus, performing inference based on Theorem 2 requires knowing which case is relevant for the specific application. In other words, inference based directly on this result is not adaptive. Third, in Case 1, the variance coming from the first stage estimation does not appear in the asymptotic distribution of \(\hat{\delta}_{2}(W, \tau)\) because it converges to zero at a quicker rate. Consequently, inference may have poor properties. We solve these issues in the following subsection by suggesting an efficient estimator and adaptive inference that are both valid in all three cases and more generally uniformly valid in the variance of \(\alpha_{j}(\tau)\).

\footnotetext{\({ }^{20}\) This is not specific to quantile models and also affects least squares models with large \(n\) (see Ahn and Moon, 2014).
}\subsection*{3.3 Adaptive Estimation and Inference}
To establish asymptotic results that are uniformly valid in the variance of \(\alpha_{j}(\tau)\), we allow \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)\) (and consequently \(\Omega_{2}(\tau, \tau)\) ) to be exactly zero, bounded away from zero, or a sequence converging to zero at an arbitrary rate. This approach nests all three pointwise cases described in the previous section. The sequence \(a_{n}(\tau)=\frac{1}{1+\operatorname{Var}\left(\alpha_{j}(\tau)\right)^{n}}\) defined in Assumption \(8^{\prime}\) plays a central role because it is proportional to the rate of convergence of the slow moments relative to the fast moments \(\left\|\operatorname{Var}\left(\bar{g}_{m n, 2}(\delta, \tau)\right)\right\|^{-1}\left\|\operatorname{Var}\left(\bar{g}_{m n, 1}(\delta, \tau)\right)\right\|=O_{p}\left(a_{n}(\tau)\right)\). It is equal to 1 without heterogeneity, converges to 0 at the \(n^{-1}\) rate with strong heterogeneity, and is always bounded between 0 and 1 . All matrices that are functions of \(a_{n}(\tau)\) may also depend on the sample size. When necessary, we make the notation explicit with the subscript \(m n\). For example, let

\[
G_{m n}(\tau)=\left(\Sigma_{Z X}^{\prime} W_{m n}(\tau) \Sigma_{Z X}\right)^{-1} \Sigma_{Z X}^{\prime} W_{m n}(\tau),
\]

where \(W_{m n}(\tau)\) is defined in Assumption \(8^{\prime}\).\\
Similarly to Fern√°ndez-Val et al. (2022), we define the covariance kernel of the limiting process of \(\hat{\delta}(\cdot)-\delta(\cdot)\). For a given integer \(T>0\), let \(\mathcal{T}_{T}=\left(\tau_{1}, \ldots, \tau_{T}\right)\) be an arbitrary \(T\) dimensional vector on \(\otimes_{t=1}^{T} \mathcal{T}\). Let

\[
\Sigma_{m n}\left(\tau, \tau^{\prime}\right)=G_{m n}(\tau)\left(\frac{\Omega_{1}\left(\tau, \tau^{\prime}\right)}{m n}+\frac{\Omega_{2}\left(\tau, \tau^{\prime}\right)}{m}\right) G_{m n}\left(\tau^{\prime}\right)^{\prime}
\]

and \(\Sigma_{m n}(\tau)=\Sigma_{m n}(\tau, \tau)\). The covariance kernel is now given by the limit of the elements of the following \((K T) \times(K T)\) matrix

\[
H_{m n}=\left(H_{m n}\left(\tau, \tau^{\prime}\right)\right)_{T \times T},
\]

where

\[
H_{m n}\left(\tau, \tau^{\prime}\right)=\operatorname{diag}\left(\Sigma_{m n}(\tau)\right)^{-1 / 2} \Sigma_{m n}\left(\tau, \tau^{\prime}\right) \operatorname{diag}\left(\Sigma_{m n}\left(\tau^{\prime}\right)\right)^{-1 / 2}
\]

Assumption 9 (Covariance kernel). For any integer \(T>0\) and any \(T\)-dimensional vector \(\mathcal{T}_{T}=\left(\tau_{1}, \ldots, \tau_{T}\right)\) on \(\otimes_{t=1}^{T} \mathcal{T}\), there is a \((K T) \times(K T)\) matrix \(H\), such that, almost surely,

\[
\lim _{m, n \rightarrow \infty} H_{m n}=H
\]

In addition, there is \(c_{\mathcal{T}_{T}}>0\) such that for the smallest eigenvalue we have

\[
\lambda_{\min }(H)>c_{\mathcal{T}_{T}} .
\]

We can now state the asymptotic distribution of our estimator that is uniformly valid in \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)\).

Theorem 3 (Adaptive asymptotic distribution). Assumptions 1-7(c) and 9 hold. In addition, either Assumption 8 or Assumption \(8^{\prime}\) holds. Then,

\[
\operatorname{diag}\left(\Sigma_{m n}(\cdot)\right)^{-1 / 2}(\hat{\delta}(\cdot)-\delta(\cdot)) \rightsquigarrow \mathbb{G}(\cdot)
\]

where \(\mathbb{G}\) is a centered Gaussian process with a covariance function \(H\left(\tau_{k}, \tau_{l}\right)\).

Theorem 3 allows for both types of weighting matrices: the asymptotically full-ranked weighting matrix of Assumption 8 or the heterogeneous weighting matrix of Assumption 8'. The first case covers the exactly identified case and 2SLS. As we show in Proposition 3 below, our estimated efficient weighting matrix satisfies the second condition.

In order to use these results for inference, we must provide a consistent estimator of the asymptotic variance \(\Sigma_{m n}\left(\tau, \tau^{\prime}\right)\). The crucial ingredient is the asymptotic variance of the sample moments that we denote by

\[
\Omega_{m n}\left(\tau, \tau^{\prime}\right)=\Omega_{1}\left(\tau, \tau^{\prime}\right) / n+\Omega_{2}\left(\tau, \tau^{\prime}\right)
\]

Remember that \(\Omega_{1}\left(\tau, \tau^{\prime}\right)\) arises from the first stage estimation and \(\Omega_{2}\left(\tau, \tau^{\prime}\right)\) from the presence of the group-level heterogeneity \(\alpha_{j}(\cdot)\). The difficulty resides in that the leading term may arise from the first-stage or second-stage estimation depending on the variance of \(\alpha_{j}(\cdot)\) and the type of instrument ( \(L_{1}\) or \(L_{2}\) ). This expression may suggest that we must estimate these two components separately. However, and perhaps surprisingly, we find that we can estimate the leading term of \(\Omega_{m n}\left(\tau, \tau^{\prime}\right)\) adaptively with a traditional cluster-robust estimator of the variance:

\[
\hat{\Omega}\left(\tau, \tau^{\prime}\right)=\frac{1}{m} \sum_{j=1}^{m}\left\{\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \hat{u}_{i j}(\tau)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \hat{u}_{i j}\left(\tau^{\prime}\right)\right)^{\prime}\right\}
\]

where \(\hat{u}_{i j}(\tau)\) are the second-stage residuals.\\
Proposition 1 (Properties of \(\left.\hat{\Omega}\left(\tau, \tau^{\prime}\right)\right)\). Let assumptions 1-6 and \(7(c)\) hold. Further, assume that as \(m \rightarrow \infty\), for each \(l, l^{\prime} \in\{1, \ldots, L\}\) and uniformly in \(\tau, \tau^{\prime} \in \mathcal{T}^{2}\),

\[
m^{-1} \sum_{j=1}^{m} \mathbb{E}\left[\left(\bar{z}_{j l} \bar{z}_{j l^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)-\Omega_{m n, 2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)\right)^{2}\right] \rightarrow C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right)<\infty
\]

where \(C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right)\) is continuous in \(\tau\) and \(\tau^{\prime}\). The estimator used to compute \(\hat{u}_{i j}(\tau)\) satisfies (i) \(\hat{\beta}(\tau)-\beta(\tau)=O_{p}(1 / \sqrt{m n})\), (ii) \(\hat{\gamma}(\tau)-\gamma(\tau)=O_{p}(1 / \sqrt{m n})+O_{p}\left(\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)} / \sqrt{m}\right)\) uniformly in \(\tau\). Then, for any \(l l^{\prime}\) entry of the \(\hat{\Omega}\left(\tau, \tau^{\prime}\right)\) matrix with \(l, l^{\prime} \in\{1, \ldots, L\}\) we have uniformly in \(\tau, \tau^{\prime} \in \mathcal{T}^{2}\),

\[
\hat{\Omega}_{l l^{\prime}}\left(\tau, \tau^{\prime}\right)=\Omega_{m n, l l^{\prime}}\left(\tau, \tau^{\prime}\right)+o_{p}\left(\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}\right)
\]

Beyond the numbered assumptions, two additional conditions are necessary. First, equation (23) requires the average variance of \(\bar{z}_{j} \alpha_{j}(\tau)\) to converge to a constant. This holds automatically when we sample identically distributed groups but is required because we allow for heterogeneity across groups. Second, we require that the coefficients on the individual-level variables are estimated at the \(1 / \sqrt{m n}\) rate and the coefficients on the group-level variable at the \(1 / \sqrt{m n}+\) \(\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)} / \sqrt{m}\) rate. This corresponds to assuming that \(M_{1}=K_{1}\) so that \(\beta=\delta_{1}\). This assumption ensures that estimation errors in slow coefficients do not contaminate the variance estimate of fast ones. This condition can always be achieved, for instance, by using \(\dot{x}_{1 i j}\) as an\\
instrument for \(x_{1 i j}\) and a weighting matrix that satisfies Assumption \(8^{\prime}\). We show below that our efficient weighting matrix satisfies this restriction. \({ }^{21}\)

The suggested estimator of the variance is straightforward to implement because it does not require estimating directly the variance of the first-stage quantile regression coefficients. This is a noteworthy advantage because the variance of these coefficients depends on the conditional density of the outcome given the covariates, which can be difficult to estimate. To understand this surprising result, consider the case where \(\alpha_{j}(\tau)=0\) for all groups. This implies that the entire variance comes from the first-stage estimation. The clustered estimator of the variance can be interpreted as a subsampling estimator where the groups represent the subsamples. The variance of the coefficients across groups provides information about the precision of the firststage estimates. Similar results for the cross-sectional bootstrap are provided in Liao and Yang (2018), Lu and Su (2022), and Fern√°ndez-Val et al. (2022).

Proposition 1 shows we can consistently estimate the diagonal elements of \(\Omega_{m n}(\tau)\) in the sense that the error that we make vanishes more quickly than the true variance. On the other hand, if two sample moments vanish at different rates, we might not be able to estimate their covariance consistently. However, this does not preclude testing hypotheses about combinations of coefficients because the error that we make when we estimate the covariance vanishes faster than the variance of the slowest coefficient. We formalize this result in Proposition 2 below for the case of a single linear null hypothesis. \({ }^{22}\)

Define the \(T K \times 1\) vector \(\delta\left(\tau_{1}, \ldots, \tau_{T}\right)=\left(\delta\left(\tau_{1}\right)^{\prime}, \ldots, \delta\left(\tau_{T}\right)^{\prime}\right)^{\prime}\) whose covariance matrix is given by the elements of the following \((K T) \times(K T)\) matrix:

\[
\Sigma_{m n}=\left(\Sigma_{m n}\left(\tau, \tau^{\prime}\right)\right)_{T \times T} .
\]

For each \(\tau, \tau^{\prime} \in \mathcal{T} \times \mathcal{T}, \Sigma_{m n}\left(\tau, \tau^{\prime}\right)\) is estimated by

\[
\hat{\Sigma}\left(\tau, \tau^{\prime}\right)=\hat{G}(\tau) \frac{\hat{\Omega}\left(\tau, \tau^{\prime}\right)}{m} \hat{G}\left(\tau^{\prime}\right)
\]

where \(\hat{\Omega}\left(\tau, \tau^{\prime}\right)\) is defined in equation (22). The following proposition shows that a straightforward z-test provides valid inference that is adaptive in the degree of heterogeneity.

Proposition 2 (Asymptotic normality of the test statistic). Let the conditions for Theorem 3 and Proposition 1 hold. In addition, let \(\eta \in \mathbb{R}^{T \times K}\) with \(\|\eta\|>\varepsilon>0\). Then,

\[
\frac{\eta^{\prime}\left(\hat{\delta}\left(\tau_{1}, \ldots, \tau_{T}\right)-\delta\left(\tau_{1}, \ldots, \tau_{T}\right)\right)}{\sqrt{\eta^{\prime} \hat{\Sigma} \eta}} \xrightarrow{d} N(0,1) .
\]

\footnotetext{\({ }^{21}\) The only exception in this paper is the between estimator from Section 5 . In that case, all the coefficients are estimated at the slow rate \(1 / \sqrt{m n}+\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)} / \sqrt{m}\). Proposition \(1^{\prime}\) in the Appendix demonstrates the consistency of \(\hat{\Omega}\left(\tau, \tau^{\prime}\right)\) in this case as well.\\
\({ }^{22}\) We can similarly extend this proposition to multiple non-linear hypotheses.
}The asymptotic variance derived in Theorem 3 depends on the weighting matrix. Following standard GMM arguments, the efficient weighting matrix is given by

\[
W(\tau)^{*}=\Omega_{m n}(\tau)^{-1}=\left(\Omega_{1}(\tau) / n+\Omega_{2}(\tau)\right)^{-1} .
\]

This weighting matrix automatically takes into account the different rates of convergence of the different moments. If some moments converge faster than others, then this matrix, asymptotically, gives infinitely more weight to the fast moments than the slow moments, so the parameters identified by the fast moments will converge at the faster rate.

Usually, we would simply plug in a consistent estimator of \(W^{*}(\tau)\) and obtain a feasible efficient GMM estimator, but Proposition 1 shows that the off-diagonal elements of \(\Omega_{m n}(\tau)\) might not be consistently estimated when the rates of convergence of the corresponding coefficients are different. Proposition 3 shows that the estimated efficient weighting matrix nevertheless satisfies Assumption \(8^{\prime}\) such that Theorem 3 and Proposition 2 apply to the feasible efficient GM estimator. In addition, Theorem 3 reveals that the (first-order) asymptotic distribution of the estimator that uses \(W^{*}(\tau)\) as the weighting matrix is the same as that of the estimator that uses \(\hat{W}^{*}(\tau)\) as the weighting matrix.

Proposition 3 (Adaptive Efficiency of the GMM Estimator). Assume that the conditions for Proposition 1 hold. Then, the estimated efficient weighting matrix \(\hat{W}^{*}(\tau)=\hat{\Omega}(\tau)^{-1}\) satisfies Assumption \(8^{\prime}\).

If there are more moment conditions than parameters to estimate ( \(L>K\) ), it is possible to implement overidentification tests in the spirit of Sargan (1958) and Hansen (1982). More precisely, we can test the validity of the instrumental variables while maintaining the other assumptions. The test statistic is the GMM objective function evaluated at the efficient GMM estimator:

\[
J\left(\hat{\delta}\left(\hat{W}^{*}(\tau), \tau\right), \tau\right)=m \bar{g}_{m n}\left(\hat{\delta}\left(\hat{W}^{*}(\tau), \tau\right), \tau\right)^{\prime} \hat{\Omega}(\tau)^{-1} \bar{g}_{m n}\left(\hat{\delta}\left(\hat{W}^{*}(\tau), \tau\right), \tau\right) .
\]

Proposition 4 demonstrates the adaptive validity of the \(J\)-test implemented with the clustered covariance matrix.

Proposition 4 (Overidentification Test). Under the \(\mathbb{H}_{0}: \mathbb{E}\left[z_{i j} \alpha_{i}(\tau)\right]=0\) for all \(\tau \in \mathcal{T}\) and the assumptions required for Proposition 1, as \(m \rightarrow \infty, J\left(\hat{\delta}\left(\hat{W}^{*}(\tau), \tau\right), \tau\right) \xrightarrow{d} \chi_{L-K}^{2}\).

We prove the result for a single quantile, but the proof easily extends to multiple quantiles. In Section 5.3, we use this overidentification test to suggest a quantile analog of the Hausman test for the exogeneity of the between variation.

\section*{4 Grouped (IV) Quantile Regression Model}
\subsection*{4.1 Chetverikov et al. (2016)}
Chetverikov et al. (2016, hereafter CLP) introduce an IV quantile regression estimator for\\
group-level treatments and propose an alternative two-step estimation procedure for model (1). In the first stage, they perform quantile regression separately for each group \(j\) and quantile \(\tau\), regressing \(y_{i j}\) on \(x_{1 i j}\) and a constant. In the second stage, they regress the intercept from the first stage on \(x_{2 j}\). The key distinction between their approach and ours is that CLP use the estimated intercept from the first stage as the dependent variable in the second stage, whereas we use the fitted values.

The intercept represents the fitted value for an observation with \(x_{1 i j}=0\). Consequently, their estimator is not invariant to linear reparametrizations of the individual-level regressors. In finite samples, results can differ depending on how we code the variables. For instance, the estimated coefficients may change if age is recorded as years since birth versus years since 16 or if we measure temperature in Celsius versus Fahrenheit. Our estimator, by contrast, does not suffer from this issue. Beyond the undesirable dependence of the results on an arbitrary linear transformation of the individual-level variables, this property also affects the precision of the estimates and introduces potential misspecification bias.

Figure 1 illustrates these concerns using artificial data. Panel (a) shows how the variance of the fitted values increases as we move away from the mean of the regressor. The solid line represents the median regression estimate, while the shaded area depicts the \(95 \%\) confidence interval for the median fitted values. The intercept corresponds to the fitted value for an \(x\) value that lies far outside the observed support of the variable. As a result, its variance is relatively large and would increase even more if we shifted the location of the individual-level variable.

Panel (b) highlights the misspecification bias using a similar artificial dataset. The dashed orange line represents the true median regression function, while the solid line shows the estimated linear model, which is slightly misspecified. Over the support of the covariates, the misspecification bias remains small because quantile regression minimizes the weighted mean squared error (see Angrist et al., 2006). However, the intercept is more strongly biased, as it lies outside the covariate's support, and this bias monotonically worsens as we increase the mean of the individual-level covariate.

\subsection*{4.2 Simulations}
In this subsection, we compare the MD and CLP estimators of \(\gamma(\tau)\) using Monte Carlo simulations. In the following subsection, we formally compare their asymptotic distributions to explain the simulation results. We use the same three data-generating processes (DGP) as in Chetverikov et al. (2016) and generate the outcome as follows:

\[
y_{i j}=\beta_{0}\left(u_{i j}\right)+x_{1 i j} \beta\left(u_{i j}\right)+x_{2 j} \gamma\left(u_{i j}\right)+\alpha_{j}\left(u_{i j}\right),
\]

where \(x_{1 i j}\) and \(x_{2 j}\) are \(\exp (0.25 \cdot N[0,1])\) and individual heterogeneity is introduced via the rank variable \(u_{i j} \sim U[0,1]\). The quantile coefficient functions are \(\gamma(\tau)=\beta(\tau)=\sqrt{\tau}\) and \(\beta_{1}(\tau)=\frac{\tau}{2}\) for \(\tau \in[0,1]\). In the first DGP, we set \(\alpha_{j}\left(u_{i j}\right)=0\), so there is neither group heterogeneity nor

Figure 1: First Stage Regressions\\
\includegraphics[max width=\textwidth, center]{2025_03_07_dca8d70c5c7eaa27106cg-25}

Both panels show generated data for one group and a first-stage fit. Panel (a) uses a correctly specified model. The solid line shows the regression line estimated by median regression, and the shaded area shows the \(95 \%\) confidence interval. Panel (b) uses a true quadratic model. The solid line is estimated by linear median regression. The dashed orange line is the true regression line.\\
endogeneity. In the second DGP, we introduce group-level heterogeneity as

\[
\alpha_{j}\left(u_{i j}\right)=u_{i j} \eta_{j}-\frac{u_{i j}}{2},
\]

where \(\eta_{j} \sim U(0,1)\). There is no endogeneity because the group effects are uncorrelated with the regressors. As group-level heterogeneity is multiplied with the rank variable \(u_{i j}\), there is only weak group heterogeneity in the lower tail of the distribution and strong heterogeneity in the upper tail. Finally, in the third DGP, we introduce endogeneity as

\[
x_{2 j}=z_{j}+\eta_{j}+\nu_{j}
\]

where \(z_{j}\) and \(\nu_{j}\) are each distributed \(\exp (0.25 \cdot N[0,1]) . z_{j}\) is a valid instrumental variable for the endogenous \(x_{2 j}\). We use the same sample sizes as CLP, that is, \((m, n)=\{(25,25),(200,25)\), \((25,200),(200,200)\}\). We perform 10,000 Monte Carlo replications for the set of quantiles \(\tau \in\) \(\{0.1,0.5,0.9\}\). Since the CLP estimator does not directly estimate \(\beta(\tau)\), we present only results for \(\gamma(\tau)\).

The first stage, group-by-group quantile regressions, are the same for both estimators. In the second stage, CLP regress the estimated intercepts on \(x_{2 j}\) with OLS (DGP 1 and 2) or using \(z_{j}\) as an instrument (DGP 3). We implement the MD estimator with the IV estimator instrumenting \(\left(x_{1 i j}, x_{2 j}\right)\) with \(\left(\dot{x}_{1 i j}, x_{2 j}\right)\) (DGP 1 and 2\()\) or \(\left(\dot{x}_{1 i j}, z_{j}\right)\) (DGP 3 ). With this choice of instruments, we do not exploit the between variation of \(x_{1 i j} .{ }^{23}\) Since the data generating process of Chetverikov et al. (2016) has a weak instrument when \(m\) is small, one should pay attention

\footnotetext{\({ }^{23}\) Using \(x_{1 i j}\) instead of \(\dot{x}_{1 i j}\) as an instrument has virtually no effect on the results since there is no variation in \(x_{1 i j}\) across groups.
}Table 1: Bias, Standard Deviation and MSE of \(\hat{\gamma}(\tau)\)

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow[b]{2}{*}{Quantile} & \multicolumn{3}{|l|}{No group-level heterogeneity} & \multicolumn{3}{|c|}{Exogenous} & \multicolumn{3}{|c|}{Endogenous} \\
\hline
 & MD & CLP & Rel. MSE & MD & CLP & Rel. MSE & MD & CLP & Rel. MSE \\
\hline
\multirow{3}{*}{0.1} & \multicolumn{9}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,25)\)} \\
\hline
 & 0.022 & -0.011 & 0.051 & 0.022 & -0.010 & 0.052 & 0.036 & 0.001 & 0.160 \\
\hline
 & (0.192) & (0.858) & \multirow[b]{2}{*}{0.061} & (0.195) & (0.860) &  & (2.025) & (5.062) &  \\
\hline
\multirow[t]{2}{*}{0.5} & -0.010 & -0.001 &  & -0.011 & 0.000 & \multirow[t]{2}{*}{0.088} & -0.029 & 0.039 & \multirow[t]{2}{*}{0.132} \\
\hline
 & (0.166) & (0.673) & \multirow{3}{*}{0.049} & (0.204) & (0.691) &  & (1.992) & (5.491) &  \\
\hline
\multirow[t]{2}{*}{0.9} & -0.019 & -0.003 &  & -0.020 & -0.004 & \multirow[t]{2}{*}{0.216} & -0.063 & -0.011 & \multirow[t]{2}{*}{0.176} \\
\hline
 & (0.094) & (0.435) &  & (0.227) & (0.490) &  & (2.123) & (5.065) &  \\
\hline
\multirow{3}{*}{0.1} &  &  & \multirow{3}{*}{0.060} & \multicolumn{6}{|l|}{\((\mathrm{m}, \mathrm{n})=(200,25)\)} \\
\hline
 & 0.024 & 0.003 &  & 0.024 & 0.004 & \multirow[t]{2}{*}{0.063} & 0.023 & 0.006 & \multirow[t]{2}{*}{0.057} \\
\hline
 & (0.066) & (0.284) &  & (0.067) & (0.285) &  & (0.106) & (0.456) &  \\
\hline
\multirow[t]{2}{*}{0.5} & -0.006 & -0.001 & \multirow[t]{2}{*}{0.059} & -0.006 & 0.000 & \multirow[t]{2}{*}{0.086} & -0.009 & -0.003 & \multirow[t]{2}{*}{0.071} \\
\hline
 & (0.056) & (0.232) &  & (0.069) & (0.238) &  & (0.097) & (0.366) &  \\
\hline
\multirow[t]{2}{*}{0.9} & -0.017 & -0.004 & \multirow[t]{2}{*}{0.060} & -0.017 & -0.003 & \multirow[t]{2}{*}{0.223} & -0.022 & -0.009 & \multirow[t]{2}{*}{0.142} \\
\hline
 & (0.031) & (0.145) &  & (0.075) & (0.164) &  & (0.086) & (0.234) &  \\
\hline
\multirow{3}{*}{0.1} & \multirow[b]{3}{*}{\(
\begin{aligned}
& 0.003 \\
& (0.070)
\end{aligned}
\)} &  & \multirow{3}{*}{0.059} & \multicolumn{3}{|l|}{\((\mathrm{m}, \mathrm{n})=(25,200)\)} & \multirow[b]{3}{*}{\(
\begin{aligned}
& -0.028 \\
& (2.107)
\end{aligned}
\)} & \multirow[b]{3}{*}{\(
\begin{aligned}
& -0.076 \\
& (5.618)
\end{aligned}
\)} & \multirow{3}{*}{0.141} \\
\hline
 &  & -0.002 &  & 0.003 & -0.001 & \multirow[t]{2}{*}{0.066} &  &  &  \\
\hline
 &  & (0.289) &  & (0.074) & (0.291) &  &  &  &  \\
\hline
\multirow[t]{2}{*}{0.5} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.001 \\
& (0.060)
\end{aligned}
\)} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.002 \\
& (0.247)
\end{aligned}
\)} & \multirow[t]{2}{*}{0.060} & -0.001 & -0.001 & \multirow[t]{2}{*}{0.233} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.083 \\
& (3.627)
\end{aligned}
\)} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.094 \\
& (4.575)
\end{aligned}
\)} & \multirow[t]{2}{*}{0.628} \\
\hline
 &  &  &  & (0.134) & (0.278) &  &  &  &  \\
\hline
\multirow[t]{2}{*}{0.9} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.002 \\
& (0.030)
\end{aligned}
\)} & \multirow[t]{2}{*}{\(
\begin{aligned}
& 0.000 \\
& (0.121)
\end{aligned}
\)} & \multirow[t]{2}{*}{0.061} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.001 \\
& (0.217)
\end{aligned}
\)} & \multirow[t]{2}{*}{\(
\begin{aligned}
& 0.001 \\
& (0.247)
\end{aligned}
\)} & \multirow[t]{2}{*}{0.769} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.120 \\
& (3.930)
\end{aligned}
\)} & \multirow[t]{2}{*}{\(
\begin{aligned}
& -0.114 \\
& (3.561)
\end{aligned}
\)} & \multirow[t]{2}{*}{1.218} \\
\hline
 &  &  &  &  &  &  &  &  &  \\
\hline
 &  &  &  & \((\mathrm{m}, \mathrm{n})=\) & (200, 200) &  &  &  &  \\
\hline
0.1 & 0.003 & -0.003 & 0.057 & 0.003 & -0.003 & 0.062 & 0.002 & -0.004 & 0.058 \\
\hline
 & (0.024) & (0.100) &  & (0.025) & (0.101) &  & (0.039) & (0.162) &  \\
\hline
0.5 & -0.001 & 0.000 & 0.059 & -0.001 & -0.001 & 0.222 & -0.004 & -0.004 & 0.141 \\
\hline
 & (0.020) & (0.084) &  & (0.044) & (0.093) &  & (0.051) & (0.136) &  \\
\hline
0.9 & -0.002 & 0.000 & 0.067 & -0.003 & -0.001 & 0.762 & -0.009 & -0.007 & 0.617 \\
\hline
 & (0.010) & (0.040) &  & (0.071) & (0.082) &  & (0.074) & (0.095) &  \\
\hline
\end{tabular}
\end{center}

Note:\\
The table reports mean bias, standard deviation and relative MSE from the simulations for \(\gamma(\tau)\) from 10,000 Monte Carlo simulations using the MD estimator and the CLP estimator. The relative MSE gives the MSE of the MD estimator relative to that of the CLP estimator.\\
when looking at the simulation results for the endogenous case. \({ }^{24}\) It would be straightforward to compute weak instrument robust inference, for instance, using Anderson/Rubin confidence intervals in the second stage. However, we consider this to be outside the scope of this paper.

Table 1 presents the bias, standard deviation, and the relative MSE of the MD estimator, defined as the MSE of the MD estimator divided by that of the CLP estimator. No clear pattern emerges regarding the bias of the estimators. Consistent with asymptotic theory, the bias diminishes as the number of observations increases. We observe more pronounced differences in the variance of the estimators. In the homogeneous case, the standard deviation of the MD estimator is four times smaller than that of the CLP estimator. This difference remains stable as the number of groups or individuals increases. The advantage of the MD estimator remains similar at the bottom of the distribution in the exogenous case, but the difference in

\footnotetext{\({ }^{24}\) With \(m=25\) in over \(40 \%\) of the draws, the F-statistics of the first stage of the IV estimations is below 10. The issue disappears when \(m=200\).
}Table 2: Coverage Probability of the \(95 \%\) Confidence Intervals

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow[b]{3}{*}{Quantile} & \multicolumn{3}{|l|}{No group-level heterogeneity} & \multicolumn{3}{|c|}{Exogenous} & \multicolumn{3}{|c|}{Endogenous} \\
\hline
 & Rel. length & \multicolumn{2}{|l|}{Coverage Rate} & Rel. length & \multicolumn{2}{|l|}{Coverage Rate} & Rel. length & \multicolumn{2}{|l|}{Coverage Rate} \\
\hline
 & MD/CLP & MD & CLP & MD/CLP & MD & CLP & MD/CLP & MD & CLP \\
\hline
\multicolumn{10}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,25)\)} \\
\hline
0.1 & 0.232 & 0.941 & 0.938 & 0.235 & 0.939 & 0.938 & 0.227 & 0.966 & 0.972 \\
\hline
0.5 & 0.244 & 0.940 & 0.942 & 0.301 & 0.943 & 0.945 & 0.262 & 0.963 & 0.972 \\
\hline
0.9 & 0.223 & 0.941 & 0.949 & 0.501 & 0.940 & 0.946 & 0.373 & 0.956 & 0.972 \\
\hline
\multicolumn{10}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,25)\)} \\
\hline
0.1 & 0.230 & 0.932 & 0.947 & 0.233 & 0.932 & 0.948 & 0.231 & 0.942 & 0.953 \\
\hline
0.5 & 0.245 & 0.946 & 0.944 & 0.296 & 0.945 & 0.946 & 0.267 & 0.952 & 0.949 \\
\hline
0.9 & 0.220 & 0.926 & 0.947 & 0.475 & 0.941 & 0.945 & 0.368 & 0.953 & 0.952 \\
\hline
\multicolumn{10}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,200)\)} \\
\hline
0.1 & 0.241 & 0.943 & 0.940 & 0.256 & 0.943 & 0.943 & 0.240 & 0.968 & 0.974 \\
\hline
0.5 & 0.242 & 0.937 & 0.944 & 0.495 & 0.938 & 0.944 & 0.370 & 0.949 & 0.971 \\
\hline
0.9 & 0.248 & 0.948 & 0.941 & 0.884 & 0.933 & 0.945 & 0.771 & 0.939 & 0.955 \\
\hline
\multicolumn{10}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,200)\)} \\
\hline
0.1 & 0.241 & 0.944 & 0.944 & 0.254 & 0.947 & 0.945 & 0.246 & 0.951 & 0.950 \\
\hline
0.5 & 0.244 & 0.946 & 0.945 & 0.483 & 0.952 & 0.948 & 0.377 & 0.957 & 0.951 \\
\hline
0.9 & 0.246 & 0.942 & 0.953 & 0.872 & 0.950 & 0.950 & 0.772 & 0.954 & 0.955 \\
\hline
\end{tabular}
\end{center}

Note:\\
Results based on 10,000 Monte Carlo simulations. The table provides the coverage rate and median length of the confidence intervals of \(\gamma(\tau)\). The relative length provides the length of the confidence interval of the MD estimator relative to that of the CLP estimator. Robust standard errors are used for the CLP estimator, and clustered standard errors at the group level are used for the MD estimator.\\
variance narrows at the upper end. We explain this with the presence of weak heterogeneity at the bottom of the distribution and strong heterogeneity at the top. At the upper end of the distribution, the variance of the estimators converges as \(n\) becomes large. Finally, in the endogenous case, the results resemble those of the exogenous case once we exclude findings affected by the weak instrument issue, precisely when \(m=25\). The differences in precision explain the large discrepancies in MSE. The MSE of the CLP estimator is up to twenty times larger than that of the MD estimator when \(\alpha_{j}(\tau)=0\) and remains substantially larger in all scenarios considered except one with a weak instrument. \({ }^{25}\)

Table 2 shows the performance of the \(95 \%\) confidence intervals suggested with our inference procedure. The table reports the coverage rate and the median length of the intervals of our estimator relative to that of the CLP estimator. \({ }^{26}\) Our suggested inference procedure has coverage close to \(95 \%\) in all cases. Compared to the CLP estimator, our confidence bands are substantially shorter. In most cases, our estimator yields confidence bands that are less than

\footnotetext{\({ }^{25}\) Although not shown here, we also computed the traditional quantile regression estimator. When there is heterogeneity, quantile regression is inconsistent for the parameter of interest; however, when \(\alpha_{j}(\tau)=0\), our estimator and quantile regression are practically indistinguishable in terms of bias and variance while the CLP estimator has a much larger variance.\\
\({ }^{26}\) For the CLP estimator, we use heteroskedasticity robust standard errors as suggested in Chetverikov et al. (2016). Recall that the CLP estimator uses only one observation per group in the second stage. Hence, we would attain the same standard errors if we kept all observations and clustered the standard errors at the group level.
}
half the length of those for the CLP estimator. This difference becomes even more pronounced in our empirical application in Section 6, where the CLP estimator produces confidence bands that are, on average, 14 times wider than those of the MD estimator.

\subsection*{4.3 Comparison of the asymptotic distributions of CLP and MD estimators}
CLP focus exclusively on \(\gamma(\tau)\), the coefficients on the group-level variables. They assume strong group-level heterogeneity, imposing that \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)>0\) uniformly in \(\tau .{ }^{27}\) Thus, their asymptotic distribution corresponds to case (i)-b of our Theorem 2, where the variance from the first stage diminishes more rapidly than that from the second stage. In this subsection, we compare the variance of the CLP and MD estimators, allowing for weak or no heterogeneity.

To simplify notation, we consider the exogenous case where \(x_{2 j}\) can serve as its own instrumental variable, though the results also extend to the endogenous case. As discussed in Section 3 , the asymptotic variance consists of two components: one accounting for first-stage error and the other for second-stage noise. We will examine each component separately. First, we consider the scenario where the true first-stage coefficients are known to isolate the variance arising in the second stage. In this scenario, the CLP point estimates can be obtained numerically within our MD framework by regressing the true first-stage fitted values on \(x_{1 i j}\) and \(x_{2 j}\), using \(\dot{x}_{1 i j}\) and \(x_{2 j}\) as instruments. Thus, the second-stage variance resulting from the randomness of \(\alpha_{j}(\tau)\) is identical for both estimators.

We can isolate the first-stage error by setting \(\operatorname{Var}\left(\alpha_{j}(\tau)\right)=0\). Under this condition, both estimators are classical MD estimators. We can express the CLP estimator as

\[
\hat{\delta}_{C L P}(\tau)=\underset{\delta \in \mathbb{R}^{K_{1}} \cdot m+K_{2}}{\arg \min } \frac{1}{m} \sum_{j=1}^{m}\left(\hat{\beta}_{j}(\tau)-\tilde{R}_{j} \delta\right)^{\prime}\left(\hat{\beta}_{j}(\tau)-\tilde{R}_{j} \delta\right)
\]

where

\[
\underset{\left(K_{1}+1\right) \times\left(K_{1} \cdot m+K_{2}\right)}{\tilde{R}_{j}}=\left(\begin{array}{cc}
0 & x_{2 j}^{\prime} \\
l_{j}^{\prime} \otimes I_{K_{1}} & 0
\end{array}\right),
\]

and \(l_{j}\) is a \(m\)-dimensional vector of zeros with a 1 in the \(j\) position. The restriction matrix \(\tilde{R}_{j}\) differs from the restriction matrix of our estimator defined in equation (7), as it does not impose equality of the first stage coefficients implied by the model. Thus, our estimator imposes \(K_{1} \cdot(m-1)\) additional correct restrictions. A second difference is that CLP use an identity weighting matrix while we use \(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\). When there is no group heterogeneity, we are in the classical MD framework, where the efficient weighting matrix is the inverse of the first-stage variance. It follows that weighting by \(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\) is efficient when the first-stage error is separable and the density of \(y\) given \(x\) at the \(\tau\) quantile is the same across groups. When, in addition, \(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\) is constant across groups (balanced panel, identical distribution of \(x_{1 i j}\) ), then equal weighting is efficient.

\footnotetext{\({ }^{27}\) Although they do not state this assumption explicitly, their asymptotic distribution would become degenerate without it, as they discuss in footnote 9 .
}Adding valid constraints within an efficient MD framework reduces variance. Therefore, if \(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\) is the efficient weighting matrix, our estimator will necessarily have lower first-stage variance than the CLP estimator. However, with an inefficient weighting matrix, adding valid constraints might increase the variance in some relatively pathological cases. \({ }^{28}\) While it would be possible to estimate the efficient weighing matrix, we prefer to avoid estimating the first-stage variance and instead opt for a more interpretable estimator in cases of misspecification.

To summarize the comparison, the MD estimator using \(\dot{x}_{1 i j}\) and \(x_{2 j}\) as instrumental variables exhibits the same variance due to the randomness of \(\alpha_{j}(\tau)\) but a lower variance due to estimation of \(\hat{\beta}_{j}(\tau)\) compared to the CLP estimator-this holds formally when the efficient weighting matrix is used. In light of these results, we can understand the simulation results. In the first DGP, all the variance arises from the first stage such that our estimator is more precise, even asymptotically. The relative MSE does not change as \(n\) increases. In the second and third DGP, the variances of both estimators will converge as \(n \rightarrow \infty\) because the second-stage variance will asymptotically dominate them. This convergence appears, however, to be relatively slow, especially at the bottom of the distribution, where heterogeneity is weak.

Note that CLP also consider a generalization of model (1) in which they assume

\[
\begin{aligned}
Q\left(\tau, y_{i j} \mid x_{1 i j}, x_{2 j}, v_{j}\right) & =\tilde{x}_{1 i j}^{\prime} \beta_{j}(\tau) \\
\beta_{j, 1}(\tau) & =x_{2 j}^{\prime} \gamma(\tau)+\alpha\left(\tau, v_{j}\right)
\end{aligned}
\]

By default, \(\beta_{j, 1}(\tau)\) is the first element of the vector \(\beta_{j}(\tau)\), but it could represent any element of this vector. In contrast to model (1), this approach allows the coefficient on \(x_{1 i j}\) to vary across groups. It also enables researchers to estimate interaction effects between group-level treatments and individual-level covariates. However, if the effect of a group-level variable varies with individual-level variables, the effect of \(x_{2 j}\) on the intercept no longer represents an average effect. Instead, it reflects the effect evaluated at \(x_{1 i j}=0\), which may not be meaningful or of interest. To address this issue, researchers would need to estimate all relevant interaction effects and combine them appropriately. This approach has not been discussed in CLP, nor has it been implemented in their simulations or in any applications of their estimator.

\section*{5 Traditional Quantile Panel Data Estimators}
\subsection*{5.1 Fixed Effects, Random Effects and Between Estimators}
In this subsection, we apply our results to derive quantile analogs of the fixed effects, between, random effects, and Hausman-Taylor estimators. As discussed in Section 2.3, we can obtain these estimators by selecting appropriate instrumental variables in the second stage. For fixed effects estimation, model (1) implies that \(\dot{x}_{1 i j}\) is a valid instrument since it varies only within groups and is uncorrelated with the group effects. This instrument automatically satisfies Assumption

\footnotetext{\({ }^{28}\) See the discussion in Section 8 of Hansen (2022b).
}
5. In this special case, the approach corresponds to the traditional MD estimator, where all variance originates in the first stage.

In the first stage, \(\beta(\tau)\) is estimated separately for each group. The second stage then averages these group-level coefficients using weights proportional to \(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\) (see equation 8). Galvao and Wang (2015) propose an alternative approach, suggesting efficient weights proportional to the inverse of the variance of the first-stage estimators. Their weights are equivalent to ours when

\[
f_{y_{i j} \mid x_{1 i j}, v_{j}}\left(Q\left(\tau, y_{i j} \mid x_{1 i j}, v\right) \mid x, v\right)=f_{y_{i j} \mid x_{1 i j}, v_{j}}\left(Q\left(\tau, y_{i j} \mid x_{1 i j}, v^{\prime}\right) \mid x, v^{\prime}\right)
\]

for any \(v\) and \(v^{\prime}\), i.e., when the conditional distribution of the group effects is the same across groups. Outside this specific case, our estimator may be less efficient but avoids the need to estimate the first-stage variance, which depends on the conditional densities in equation (32). A third approach is to take the unweighted average of \(\hat{\beta}_{j}(\tau)\), which, while not efficient when \(\beta_{j}(\tau)\) are homogeneous, remains straightforward to interpret even if the model is misspecified.

We can implement a quantile between estimator using \(\bar{x}_{1 j}\) as an instrument to exploit only the variation across groups. On the other hand, combining within and between variations is more complex for quantile models than for least squares models. Applying quantile regression to the whole population without controlling for groups identifies parameters that differ from our intended parameters (see Remark 1). Using our MD estimator with \(x_{1 i j}\) as an instrument, which corresponds to using OLS in the second stage, consistently estimates \(\beta(\tau)\) but only at the slow \(\sqrt{m}\) rate because \(x_{1 i j}\) also varies between groups. The same occurs if both \(\dot{x}_{1 i j}\) and \(\bar{x}_{1 j}\) are combined with 2 SLS, as the weights attributed to \(\bar{x}_{1 j}\) do not vanish asymptotically. Instead, we propose two efficient random effects estimators: an efficient GMM estimator and one using optimal instruments. It is worth highlighting that these estimators not only reduce the asymptotic variance of the estimator but also increase the rate of convergence from \(\sqrt{m}\) to \(\sqrt{m n}\) compared to 2SLS.

Given the first-stage estimation, we have the following moment condition:

\[
\mathbb{E}\left[g_{j}(\delta(\tau), \tau)\right]=\mathbb{E}\left[Z_{j}^{\prime}\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-X_{j} \delta(\tau)\right]=0\right.
\]

When the instrument includes both the within-group variation \(\dot{x}_{1 i j}\) and the between-group average \(\bar{x}_{1 j}\), the efficient GMM estimator will optimally combine these two sources of variation. The weighting matrix is computed as shown in equation (22). According to Propositions 3 and Theorem 2, this estimator is guaranteed to be both \(\sqrt{m n}\) consistent and efficient. This random effects estimator has the same first-order asymptotic distribution as the fixed effects estimator that uses only the within-group variation \(\dot{x}_{1 i j}\). However, the random effects estimator is expected to have a lower variance in finite samples because it also incorporates the betweengroup variation. As the number of observations \(n\) increases, the influence of the betweengroup variation diminishes, causing the random effects estimator to converge to the fixed effects estimator. This behavior is similar to what is observed in least squares models, as discussed by Baltagi (2021) and Ahn and Moon (2014).

If we impose the stronger assumption that the moment restriction in equation (33) holds conditionally on \(Z_{j}\), we can use the theory of optimal instruments to derive a more efficient random effects estimator. Optimal instruments are relevant when a researcher has a conditional moment restriction of the form \(\mathbb{E}\left[g_{j}(\delta, \tau) \mid Z_{j}\right]=0\). When a moment condition holds conditional on \(Z_{j}\), an infinite set of valid moments exist, and one could use additional moments to increase efficiency. The goal is to select the instrument that minimizes the asymptotic variance, which takes the form \(Z_{j}^{*}=\mathbb{E}\left[g_{j}(\delta, \tau) g_{j}(\delta, \tau)^{\prime} \mid Z_{j}\right]^{-1} R_{j}(\delta, \tau)\), with \(R_{j}(\delta, \tau)=\mathbb{E}\left[\left.\frac{\partial}{\partial \delta} g_{j}(\delta, \tau) \right\rvert\, Z_{j}\right]\) (see, e.g., Chamberlain, 1987 and Newey, 1993). To implement the random effect estimator with optimal instruments, we set \(Z_{j}=X_{j}\). Under the additional assumption that \(\mathbb{E}\left[\alpha_{j}^{2}(\tau) \mid X_{j}\right]=\sigma_{\alpha}^{2}(\tau),{ }^{29}\) the optimal instrument simplifies to

\[
Z_{j}^{*}(\tau)=\left(\tilde{X}_{j} \frac{V_{j}(\tau)}{n} \tilde{X}_{j}^{\prime}+\mathbf{l}_{n}^{\prime} \mathbf{1}_{n} \sigma_{\alpha}^{2}(\tau)\right)^{+} X_{j}
\]

where \(V_{j}(\tau)\) is the asymptotic variance from the first stage for a group \(j, \mathbf{l}_{n}\) is a \(n\)-dimenstional vector of ones, and \({ }^{+}\)denotes the Moore-Penrose inverse. \({ }^{30}\)

A few remarks about the optimal instruments follow. First, under standard random effects assumptions, the optimal instrument applied to least squares models is numerically identical to the FGLS estimator. Second, the optimal instrument depends on \(n\) analogously to the efficient weighting matrix of the GMM estimator. As \(n\) increases, the first stage variance converges to zero, and the generalized inverse will give infinitely more weights to the within variation and asymptotically converge to the fixed effects estimator. Third, if \(\sigma_{\alpha}(\tau)=0\), then all the variance arises in the first stage, and this estimator is identical to the efficient MD estimator (see Proposition 8 in Appendix C).

\subsection*{5.2 Hausman and Taylor Model}
The Hausman-Taylor model provides a method for finding instrumental variables within the model itself. It represents a middle ground between the random effects model, which assumes orthogonality between \(\alpha_{j}(\tau)\) and \(x_{i j}\), and the fixed effects model, which only identifies the effect of individual-level variables. To estimate the effect of group-level variables, Hausman and Taylor (1981) assume that some elements of \(x_{1 i j}\) are uncorrelated with \(\alpha_{j}(\tau)\). We consider model (1) but partition the regressors into four types of variables, \(x_{i j}=\left[x_{1 i j}^{e x} x_{1 i j}^{e n} x_{2 j}^{e x} x_{2 j}^{e n}\right]\), where the superscript \(e x\) indicates exogenous variables and the superscript en indicates potentially endogenous variables. Thus,

\[
\begin{aligned}
& \mathbb{E}\left[x_{1 i j}^{e x} \alpha_{j}(\tau)\right]=0 \\
& \mathbb{E}\left[x_{2 j}^{e x} \alpha_{j}(\tau)\right]=0
\end{aligned}
\]

\footnotetext{\({ }^{29} \mathrm{We}\) assume homoskedasticity of \(\alpha_{j}(\tau)\) to obtain a simple estimator, similar to the classical least squares random effects estimator. If we were to drop this assumption, we would need to estimate \(\mathbb{E}\left[\alpha_{j}(\tau) \mid X_{j}\right]\). Note that we do not assume homoscedasticity in the group-level model such that this assumption does not constrain the heterogeneity of \(\beta(\tau)\) across different values of \(\tau\).\\
\({ }^{30}\) Since the matrix \(\left(\tilde{X}_{j} \frac{V_{j}(\tau)}{n} \tilde{X}_{j}^{\prime}+\mathbf{l}_{n}^{\prime} \mathbf{l}_{n} \sigma_{\alpha}^{2}(\tau)\right)\) is singular, we use the Moore-Penrose inverse.
}The assumptions imply that we can estimate \(\delta(\tau)\) using the instrument \(z_{i j}=\left(\dot{x}_{1 i j}^{e x}, \dot{x}_{1 i j}^{e n}\right.\), \(\left.\bar{x}_{1 i j}^{e x}, x_{2 j}^{e x}\right)\). While \(x_{2 j}^{e n}\) is potentially endogenous, the within variation is uncorrelated with \(\alpha_{j}(\tau)\) as it varies only within \(j\). Identification requires at least as many instruments as parameters to estimate, hence \(\operatorname{dim}\left(x_{1 i j}^{e x}\right) \geq \operatorname{dim}\left(x_{2 j}^{e n}\right)\). In overidentified models, efficient GMM can be implemented, and if conditional moment restrictions are available, optimal instruments can be used. However, implementing optimal instruments is not straightforward, as it requires estimating \(\mathbb{E}\left[x_{i j} \mid z_{i j}\right]\), typically done nonparametrically (see Newey, 1993). We do not contribute to this aspect in this paper.

\subsection*{5.3 Hausman Test}
The random effects estimator's consistency relies on stronger orthogonality conditions compared to the fixed effects estimator. Under these stronger assumptions, both estimators are consistent, but the fixed effects estimator is inefficient. Hausman (1978) proposed a test for the null hypothesis of random effects against the alternative of fixed effects. Various generalizations of the Hausman test have been suggested in the literature (e.g., Chamberlain (1982); Mundlak (1978); Wooldridge (2019)). Arellano (1993) considers a heteroskedasticity and autocorrelation robust generalization based on a Wald test. Ahn and Low (1996) propose a GMM test based on a 3SLS regression as an equivalent method for the Hausman test.

This subsection explains how we can use the overidentification test presented in Section 3 as a quantile version of the Hausman test for our two-step estimator. The assumption of correct specification of the first stage is maintained under both the null and alternative hypotheses. Compared to the fixed effects estimator, consistency of the random effects estimator additionally requires that \(x_{1 i j}\) is uncorrelated with \(\alpha_{j}(\tau)\), so that \(\mathbb{E}\left[\dot{x}_{1 i j}^{\prime} \alpha_{j}(\tau)\right]=0\) and \(\mathbb{E}\left[\bar{x}_{1 j}^{\prime} \alpha_{j}(\tau)\right]=0\) are valid moment conditions. By contrast, the fixed effects estimator relies only on the moment condition \(\mathbb{E}\left[\dot{x}_{1 i j}^{\prime} \alpha_{j}(\tau)\right]=0\). Consequently, the overidentification test suggested in Proposition 4 can be used as a test of the random effects orthogonality conditions. Compared to the traditional Hausman test, our test does not rely on the assumption of conditional homoskedasticity of the errors and is robust to clustering.

Galvao and Poirier (2019) also note that a quantile regression with all observations and fixed effects quantile regression identify different parameters. They propose a Hausman test based on an auxiliary quantile regression model that incorporates both \(x_{1 i j}\) and \(\bar{x}_{1 j}\) as regressors, and tests for the significance of \(\bar{x}_{1 j}\). However, this test differs from our approach in several ways: it starts from a different model (not conditional on the groups), relies on the correct specification of the auxiliary regression, and does not directly compare random effects and fixed effects estimators.

\subsection*{5.4 Simulations}
This section presents simulation results for the different panel data estimators and the Hausman-type test presented in the previous subsections. These simulations focus on the esti-

Table 3: Bias and Standard Deviation of \(\hat{\beta}(\tau)\)

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Quantile & Pooled & BE & FE & RE-GMM & RE-OI \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,10)\)} \\
\hline
0.1 & \( \begin{aligned} & 0.009 \\ & (0.193) \end{aligned} \) & \( \begin{aligned} & 0.002 \\ & (0.235) \end{aligned} \) & \( \begin{aligned} & 0.037 \\ & (0.261) \end{aligned} \) & \( \begin{aligned} & 0.019 \\ & (0.183) \end{aligned} \) & \( \begin{aligned} & 0.044 \\ & (0.177) \end{aligned} \) \\
\hline
\multirow[t]{2}{*}{0.5} & 0.000 & 0.000 & -0.001 & -0.001 & 0.000 \\
\hline
 & (0.182) & (0.224) & (0.172) & (0.142) & (0.168) \\
\hline
\multirow[t]{2}{*}{0.9} & -0.010 & -0.003 & -0.039 & -0.021 & -0.045 \\
\hline
 & (0.195) & (0.235) & (0.259) & (0.184) & (0.181) \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,10)\)} \\
\hline
\multirow[t]{2}{*}{0.1} & 0.011 & 0.005 & 0.040 & 0.021 & 0.046 \\
\hline
 & (0.068) & (0.080) & (0.092) & (0.061) & (0.067) \\
\hline
\multirow[t]{2}{*}{0.5} & 0.001 & 0.001 & 0.001 & 0.001 & 0.001 \\
\hline
 & (0.063) & (0.076) & (0.059) & (0.047) & (0.063) \\
\hline
\multirow[t]{2}{*}{0.9} & -0.010 & -0.003 & -0.040 & -0.019 & -0.045 \\
\hline
 & (0.067) & (0.080) & (0.091) & (0.060) & (0.068) \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,25)\)} \\
\hline
\multirow[t]{2}{*}{0.1} & 0.003 & 0.000 & 0.015 & 0.011 & 0.016 \\
\hline
 & (0.175) & (0.222) & (0.141) & (0.122) & (0.120) \\
\hline
\multirow[t]{2}{*}{0.5} & -0.003 & -0.004 & 0.000 & -0.001 & -0.002 \\
\hline
 & (0.171) & (0.218) & (0.102) & (0.094) & (0.106) \\
\hline
\multirow[t]{2}{*}{0.9} & -0.009 & -0.007 & -0.017 & -0.014 & -0.018 \\
\hline
 & (0.177) & (0.223) & (0.138) & (0.121) & (0.120) \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,25)\)} \\
\hline
\multirow[t]{2}{*}{0.1} & 0.006 & 0.004 & 0.015 & 0.012 & 0.017 \\
\hline
 & (0.061) & (0.075) & (0.049) & (0.041) & (0.042) \\
\hline
\multirow[t]{2}{*}{0.5} & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
\hline
 & (0.059) & (0.073) & (0.036) & (0.032) & (0.036) \\
\hline
\multirow[t]{2}{*}{0.9} & -0.006 & -0.004 & -0.015 & -0.012 & -0.017 \\
\hline
 & (0.061) & (0.075) & (0.049) & (0.041) & (0.042) \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,200)\)} \\
\hline
\multirow[t]{2}{*}{0.1} & 0.001 & 0.002 & 0.002 & 0.002 & 0.002 \\
\hline
 & (0.163) & (0.211) & (0.049) & (0.049) & (0.047) \\
\hline
\multirow[t]{2}{*}{0.5} & 0.001 & 0.001 & 0.000 & 0.000 & 0.000 \\
\hline
 & (0.163) & (0.210) & (0.035) & (0.035) & (0.035) \\
\hline
\multirow[t]{2}{*}{0.9} & 0.000 & 0.001 & -0.002 & -0.002 & -0.002 \\
\hline
 & (0.163) & (0.211) & (0.049) & (0.048) & (0.046) \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,200)\)} \\
\hline
\multirow[t]{2}{*}{0.1} & 0.000 & 0.000 & 0.002 & 0.002 & 0.002 \\
\hline
 & (0.058) & (0.073) & (0.017) & (0.017) & (0.016) \\
\hline
\multirow[t]{2}{*}{0.5} & 0.000 & 0.000 & 0.000 & 0.000 & 0.000 \\
\hline
 & (0.058) & (0.072) & (0.013) & (0.012) & (0.012) \\
\hline
\multirow[t]{2}{*}{0.9} & -0.001 & -0.001 & -0.002 & -0.002 & -0.002 \\
\hline
 & (0.058) & (0.073) & (0.017) & (0.017) & (0.017) \\
\hline
\end{tabular}
\end{center}

Note:\\
The table reports bias and standard deviation (in parentheses) of the simulations for \(\beta(\tau)\) from 10,000 Monte Carlo simulations.

Table 4: Coverage Probability of the \(95 \%\) Confidence Invervals

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Quantile & Pooled & BE & FE & RE-GMM & RE-OI \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,10)\)} \\
\hline
0.1 & 0.948 & 0.922 & 0.946 & 0.920 & 0.914 \\
\hline
0.5 & 0.946 & 0.920 & 0.948 & 0.923 & 0.914 \\
\hline
0.9 & 0.950 & 0.924 & 0.950 & 0.918 & 0.912 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,10)\)} \\
\hline
0.1 & 0.942 & 0.941 & 0.927 & 0.930 & 0.874 \\
\hline
0.5 & 0.947 & 0.943 & 0.952 & 0.948 & 0.943 \\
\hline
0.9 & 0.946 & 0.942 & 0.932 & 0.934 & 0.877 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,25)\)} \\
\hline
0.1 & 0.949 & 0.921 & 0.949 & 0.928 & 0.933 \\
\hline
0.5 & 0.946 & 0.918 & 0.948 & 0.933 & 0.931 \\
\hline
0.9 & 0.946 & 0.917 & 0.951 & 0.931 & 0.931 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,25)\)} \\
\hline
0.1 & 0.947 & 0.945 & 0.942 & 0.939 & 0.928 \\
\hline
0.5 & 0.950 & 0.945 & 0.954 & 0.948 & 0.948 \\
\hline
0.9 & 0.948 & 0.946 & 0.938 & 0.938 & 0.927 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,200)\)} \\
\hline
0.1 & 0.950 & 0.923 & 0.950 & 0.938 & 0.947 \\
\hline
0.5 & 0.949 & 0.926 & 0.952 & 0.941 & 0.948 \\
\hline
0.9 & 0.947 & 0.925 & 0.950 & 0.938 & 0.948 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,200)\)} \\
\hline
0.1 & 0.948 & 0.943 & 0.947 & 0.946 & 0.950 \\
\hline
0.5 & 0.947 & 0.943 & 0.951 & 0.951 & 0.950 \\
\hline
0.9 & 0.948 & 0.944 & 0.949 & 0.947 & 0.949 \\
\hline
\multicolumn{6}{|l|}{Note:} \\
\hline
\multicolumn{6}{|l|}{Results based on 10,000 Monte Carlo simulations. The table reports the coverage probabilities of the confidence intervals of \(\beta(\tau)\).} \\
\hline
\end{tabular}
\end{center}

Table 5: Hausman Test

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\multirow[b]{2}{*}{Quantile} & \multicolumn{5}{|c|}{\(\lambda\)} \\
\hline
 & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,10)\)} \\
\hline
0.1 & 0.052 & 0.058 & 0.076 & 0.116 & 0.178 \\
\hline
0.5 & 0.048 & 0.060 & 0.095 & 0.162 & 0.268 \\
\hline
0.9 & 0.050 & 0.066 & 0.094 & 0.144 & 0.218 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,10)\)} \\
\hline
0.1 & 0.062 & 0.085 & 0.277 & 0.579 & 0.844 \\
\hline
0.5 & 0.050 & 0.177 & 0.532 & 0.871 & 0.987 \\
\hline
0.9 & 0.058 & 0.194 & 0.483 & 0.782 & 0.949 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,25)\)} \\
\hline
0.1 & 0.046 & 0.058 & 0.098 & 0.173 & 0.281 \\
\hline
0.5 & 0.043 & 0.060 & 0.114 & 0.204 & 0.340 \\
\hline
0.9 & 0.045 & 0.062 & 0.110 & 0.186 & 0.306 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,25)\)} \\
\hline
0.1 & 0.051 & 0.166 & 0.554 & 0.897 & 0.994 \\
\hline
0.5 & 0.050 & 0.232 & 0.689 & 0.963 & 0.999 \\
\hline
0.9 & 0.049 & 0.230 & 0.645 & 0.938 & 0.997 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(25,200)\)} \\
\hline
0.1 & 0.043 & 0.062 & 0.123 & 0.230 & 0.399 \\
\hline
0.5 & 0.043 & 0.063 & 0.125 & 0.242 & 0.412 \\
\hline
0.9 & 0.044 & 0.063 & 0.124 & 0.235 & 0.403 \\
\hline
\multicolumn{6}{|c|}{\((\mathrm{m}, \mathrm{n})=(200,200)\)} \\
\hline
0.1 & 0.054 & 0.259 & 0.771 & 0.985 & 1.000 \\
\hline
0.5 & 0.054 & 0.272 & 0.788 & 0.988 & 1.000 \\
\hline
0.9 & 0.052 & 0.270 & 0.785 & 0.987 & 1.000 \\
\hline
\end{tabular}
\end{center}

Note:\\
The table reports rejection rates of the Hausman test. The results are based on 10,000 Monte Carlo simulations. The first column shows the empirical size, while the other columns show the power of the test.\\
mation of \(\beta(\tau)\). We consider the following data-generating process

\[
y_{i j}=x_{1 i j}+\alpha_{j}+\left(1+0.1 x_{1 i j}\right) \nu_{i j} .
\]

where all variables are scalars, \(\nu_{i j} \sim \mathcal{N}(0,1)\), and \(x_{1 i j}=h_{j}+0.5 u_{i j}\), with \(u_{i j} \sim \mathcal{N}(0,1)\) and

\[
\binom{h_{j}}{\alpha_{j}} \sim \mathcal{N}\left(\binom{0}{0},\left(\begin{array}{ll}
1 & \lambda \\
\lambda & 1
\end{array}\right)\right)
\]

If \(\lambda \neq 0, x_{1 i j}\) is correlated with \(\alpha_{j}\). For the simulation of the panel data estimators, we let \(\lambda=0\) so that all estimators are consistent. In contrast, in the Monte Carlo study of the Hausman test, we set \(\lambda=\{0,0.1,0.2,0.3,0.4\}\). The true coefficient takes the values \(\beta(\tau)=1+0.1 F^{-1}(\tau)\) where \(F\) is the standard normal CDF. We consider the samples with \(n=\{10,25,200\}\) and \(m=\{25,200\}\) and focus on the set of quantiles \(\mathcal{T}=\{0.1,0.5,0.9\}\). All simulation results are based on 10,000 replications.

We compare the performance of four MD estimators, all based on the same first-stage regression but differing in their choice of instruments in the second stage. The 'Pooled' estimator\\
uses \(x_{1 i j}\) as the instrument, \({ }^{31}\) the BE estimator uses \(\bar{x}_{1 j}\) as the instrument, the RE-GMM estimator combines \(\dot{x}_{1 i j}\) and \(\bar{x}_{1 j}\) using efficient GMM, and the RE-OI estimator combines the same exogenous variation using the single optimal instrument defined in equation (34). We use the estimator of Powell (1991) for \(V_{j}(\tau)\) and the estimator of Nerlove (1971) for \(\sigma_{\alpha}^{2}(\tau)\).

Table 3 shows that the estimators perform well also when both \(m\) and \(n\) are small. The RE-GMM estimator performs similarly to the RE-OI estimator except with small \(n\) when the RE-GMM estimator outperforms the RE-OI both in terms of bias and variance. As expected, the RE-GMM, the RE-OI, and the fixed effects (FE) estimators become indistinguishable as \(n\) increases. Whereas with small \(n\), there is an apparent gain in using a random effects estimator. From the standard deviations, it is possible to see the different rates of convergence of the estimators. The precision of the fixed effects and random effects estimators increases in similar magnitude when \(m\) or \(n\) increases. In contrast, the standard deviation of the pooled and between estimators decreases only when \(m\) increases. The pooled and the between estimators have the smallest bias but, in most cases, also the largest variance.

The coverage probabilities of the \(95 \%\) confidence intervals are provided in Table 4. The confidence intervals of the pooled and the fixed effects estimator perform well in all sample sizes considered. On the other hand, the confidence bands of the random effects estimators slightly undercover the true parameter mostly when \(n\) is small. In larger samples, all the coverage probabilities are close to the theoretical level.

Table 5 shows the rejection probabilities of the overidentification test for different values of \(\lambda\). When \(\lambda=0\), the \(\mathbb{H}_{0}\) is satisfied, so we should reject the null at a rate close to \(5 \%\). If \(\lambda \neq 0, x_{i j}\) is correlated with \(\alpha_{j}\), and some moment conditions used by the RE-GMM estimator are not valid. In this case, higher rejection probabilities suggest a more powerful test. The first column shows that the empirical sizes of the test are close to the theoretical levels in most sample sizes. Columns \(2-5\) show that, as expected, the power of the test is higher in large samples and increases with the correlation between \(\bar{x}_{1 j}\) and the unobserved heterogeneity \(\alpha_{j}\). An increase in \(m\) substantially improves the power of the test, while a larger number of time periods \(n\) improves the results to a lesser extent. In general, the test performs better both in terms of size and power when \(m\) is large, which is most often the case in empirical applications. Even if the random effects estimator converges to the fixed effects estimator as \(n\) increases and the random effects estimator of \(\beta\) will be consistent even if \(\lambda \neq 0\), the size and power of the test do not deteriorate. This result is consistent with the findings in Ahn and Moon (2014).

\footnotetext{\({ }^{31}\) We refer to this estimator as 'Pooled' because the second stage involves a pooled OLS regression. It should not be confused with the one-step pooled quantile regression, which does not account for group structure.
}\section*{6 Empirical Application: The Effect of the Food Stamps Program on Birth Weight}
In this section, we apply our minimum distance approach to estimate the impact of the food stamp program on the birth weight distribution using grouped data. We complement the analysis of Almond et al. (2011) by providing distributional effects. Food stamps constitute an important means-tested program that gives entitled households coupons they can redeem at approved retail food stores. The Food Stamp Act (FSA) was introduced in 1964 and enabled counties to start their own federally funded food stamp program (FSP). In the subsequent years, counties increasingly adopted such programs, and in 1973, an amendment to the FSA required all counties to establish a FSP by 1975. Thus, the share of counties with an FSP increased steadily from 1964 to 1974, and identification exploits the variation in the timing of the adoption across counties. Almond et al. (2011) use data from 1968 (when about \(40 \%\) of the counties had introduced the program) to 1977 (two years after the FSP was implemented everywhere) to analyze the effect of the program.

Given the negative consequences of low birth weight, besides estimating the effect of the policy on average weight, Almond et al. (2011) estimate the effect on the probability that birth weight falls below a certain threshold. As discussed in Melly and Santangelo (2015), this procedure leads to biased results unless there is no time effect or group effect or the outcome is uniformly distributed.

In this section, we use the subscripts \(i, c\), and \(t\) to denote the birth, the county, and the trimester of birth, respectively. \({ }^{32}\) The variable of interest is a binary variable that is coded 1 if there was a food stamp program in place three months before birth. Hence, the treatment is assigned to county-month cells, and in around \(1 \%\) of cases, it also varies within groups.

We consider the following model separately for black and white mothers:

\[
Q\left(\tau, b w_{i c t} \mid f s p_{c t}, x_{1 i c t}, x_{2 c t}, v_{c t}\right)=f s p_{c t} \gamma_{1}(\tau)+x_{1 i c t} \beta(\tau)+x_{2 c t} \gamma_{2}(\tau)+\alpha\left(\tau, v_{c t}\right),
\]

where \(Q\left(\tau, b w_{i c t} \mid f s p_{c t}, x_{1 i c t}, x_{2 c t}, v_{c t}\right)\) is the \(\tau\) th conditional quantile function of the outcome given all the variables. The variable \(f s p_{c t}\) indicates whether there is a food stamp program in place, \(x_{1 i c t}\) are variables related to the individual births, such as gender, mother age, and its square as well as the legitimacy status of the birth. Group-level control variables \(x_{2 c t}\) include annual county-level controls (real per capita income, government transfers to individuals, medical spending, and retirement and disability payments) and 1960 county-level characteristics (county population and the shares of urban population, black population, and of farmland) interacted with a linear time trend. Further, \(x_{2 c t}\) also includes county, state-year, and time fixed effects.

For the estimation, we drop groups that have less than 25 degrees of freedom. \({ }^{33}\) The esti-

\footnotetext{\({ }^{32}\) Using the same notation as in the paper, the \(j\) units are county-trimester combinations, and the \(i\) units index individual births within a county in a given trimester. In this section, we use three subscripts for clarity.\\
\({ }^{33}\) If there are \(K_{1}\) individual level variables we drop all groups with less than \(K_{1}+1+25\) observations. Since some variables might vary at the individual level in some groups only, this threshold is group-specific.
}Figure 2: Impact of Food Stamp Introduction on the Distribution of Birth Weight\\
\includegraphics[max width=\textwidth, center]{2025_03_07_dca8d70c5c7eaa27106cg-38}

The figure shows the impact of the food stamp introduction on the conditional distribution of birth weight. The panels show point estimates and \(95 \%\) confidence bands (shaded area) computed using standard errors clustered at the county level. The panel on the left (right) shows the effects for blacks (whites). The regressions include county, time, and state-year fixed effects.\\
mations are performed using a sample of \(2,822,091\) individual observations divided into 19,482 groups for blacks and \(16,038,235\) individual births divided into 80,289 groups for whites. \({ }^{34}\) Figure 2 illustrates the results. The results for black mothers are in the left panel, while the results for white mothers are in the right panel. The effect is substantially larger among black mothers. The results suggest a positive effect of the food stamp program on the lower tail of the conditional distribution. For instance, the estimates suggest that the food stamp program is associated with an increase in birth weight by almost 30 grams for blacks at the 5 th percentile of the conditional distribution. For whites, there seems to be an effect only in the left tail of the distribution, and the effects are small. For blacks, the coefficients are large in the left tail and remain positive, albeit of small magnitude, until the \(75 \%\) percentile. However, for higher quantiles, the effects are not statistically different from zero.

To compare our MD estimator with the CLP estimator on observational data, we also perform the estimation using the CLP procedure. For this comparison, we focus on the sample of black mothers. Before implementing the CLP estimator on this data, we need to solve a few issues that affect the CLP estimator but not the MD estimator. First, the treatment variable exhibits within-group variation in around \(1 \%\) of the groups. If there are variables that vary only within some groups, the intercept is not comparable over groups, thus creating a problem for the CLP estimator. Hence, for the comparison, we drop groups for which the variable \(f s p\) is varying within the group. A second but similar issue is due to the covariate controlling for the legitimacy

\footnotetext{\({ }^{34}\) We have a different number of groups compared to Almond et al. (2011) due to multiple reasons. First, they give higher weights to births in groups where only \(50 \%\) of the births are coded in the natality data; thus, when they drop groups with less than 25 births, the number of births in these groups is inflated. Second, since they take the group average, they keep births with missing values for birth weight. We drop those births as we work with individual-level data.
}Figure 3: Impact of Food Stamp Introduction estimated with the MD and CLP estimators\\
\includegraphics[max width=\textwidth, center]{2025_03_07_dca8d70c5c7eaa27106cg-39}

The figure shows the impact of the food stamp introduction on the conditional distribution of birth weight in the sample of black mothers computed with the MD (in black) and the CLP (in blue) estimators. The sample consists of \(2,787,509\) divided into 17,141 groups. The panels show point estimates and \(95 \%\) confidence bands (shaded area) computed using standard errors clustered at the county level. For this estimation, we drop groups for which the treatment variables vary within groups and do not control for the legitimacy status of the birth.\\
status of the birth, as there are groups that do not contain all levels of the categorical variable. To solve this issue, we drop the variable from the conditioning set. Third, for one observation (out of over 2 million), one of the group-level control variables is minimally different from the value assigned to other members of the group. This is most likely due to a coding error. In this case, we replace the values with the group average. While these features of the data do not pose any problem with our estimation approach, they can lead to substantially different results using the CLP estimation method. Figure 3 shows the estimation results performed with our approach compared to the CLP approach using the sample of black mothers. Given that we estimate a slightly different model on a marginally different sample, we re-estimate the effects using the MD estimator to ensure a meaningful comparison. The results obtained using the MD estimator remain almost identical to those in Figure 2. On the other hand, using the CLP estimator, we obtain substantially different results from those attained using the MD estimator, and the confidence intervals are, on average, 14 times larger, precluding any possibility of drawing informative conclusions. Further, the figure shows that the MD point estimates and confidence intervals are always inside the confidence intervals of the CLP estimator.

\section*{7 Conclusion}
To summarize, our paper makes the following key contributions: First, we propose a new class of minimum distance estimators for quantile panel data models, applicable to both classical panel data settings, where units are observed over time, and grouped data settings, where indi-\\
viduals are divided into groups and treatment varies at the group level. This class of estimators provides quantile analogs of established panel data methods, including fixed effects, random effects, between, and Hausman-Taylor estimators. Additionally, it substantially improves upon the existing grouped instrumental variable estimator of Chetverikov et al. (2016) and remains computationally fast. Second, we establish uniform asymptotic properties under a general framework accommodating arbitrary degrees of group-level heterogeneity. We also introduce adaptive inference procedures that remain valid regardless of whether group effects are zero, bounded, or vanish at arbitrary rates. The adaptive estimator of the variance of the moments leads to a GMM estimator that is uniformly efficient across unknown relative convergence rates. Third, we demonstrate the practical relevance of our approach in an empirical application studying the impact of the Food Stamp Program on birth weight. Our results indicate that the policy's positive effects are concentrated almost entirely in the lower tail of the birth weight distribution, particularly among black mothers.

While these advancements mark significant progress, our approach has limitations that invite further exploration. First, although our Monte Carlo simulations show that our estimator and suggested standard errors perform well in finite samples, our asymptotic framework relies on both the number of groups and the number of observations per group diverging to infinity. This assumption is reasonable in settings with large groups, such as our empirical application, but it is often violated in classical panel data contexts. Future work should explore finite \(n\) inference or consider stronger assumptions for identification in such cases. Second, our estimator accommodates parametric time trends but not unrestricted time effects. Estimating time effects requires working with all units simultaneously, making a group-by-group computation strategy infeasible. Finally, while we focus on within-group heterogeneity, many applications also require understanding between-group heterogeneity. Pons (2024) develops a framework that explicitly captures both, providing a promising avenue for further research.

\section*{References}
Ahn, S. C. And S. Low (1996): "A reformulation of the Hausman test for regression models with pooled cross-section-time-series data," Journal of Econometrics, 71, 309-319.\\
Ahn, S. C. and H. R. Moon (2014): "Large-N and Large-T Properties of Panel Data Estimators and the Hausman Test," in Festschrift in Honor of Peter Schmidt, Springer.\\
Almond, D., H. W. Hoynes, and D. W. Schanzenbach (2011): "Inside the war on poverty: The impact of food stamps on birth outcomes," Review of Economics and Statistics, 93, 387403.

Alvarez, J. and M. Arellano (2003): "The time series and cross-section asymptotics of dynamic panel data estimators," Econometrica, 71, 1121-1159.\\
Angrist, J., V. Chernozhukov, and I. Fern√°ndez-Val (2006): "Quantile Regression\\
under Misspecification, with an Application to the U.S. Wage Structure," Econometrica, 74, 539-563.\\
Angrist, J. D. And K. Lang (2004): "Does school integration generate peer effects? Evidence from Boston's metco program," American Economic Review, 94, 1613-1634.

Arellano, M. (1993): "On the testing of correlated effects with panel data," Journal of Econometrics, 59, 87-97.\\
Arellano, M. and S. Bonhomme (2016): "Nonlinear panel data estimation via quantile regressions," Econometrics Journal, 19, C61-C94.\\
\_ (2017): "Quantile Selection Models With an Application to Understanding Changes in Wage Inequality," Econometrica, 85, 1-28.

Autor, D. H., D. Dorn, and G. H. Hanson (2013): "The China syndrome: Local labor market effects of import competition in the United States," American Economic Review, 103, 2121-2168.\\
‚Äî\_ (2021): "When Work Disappears: Manufacturing Decline and the Falling Marriage Market Value of Young Men," American Economic Review: Insights, 1, 161-178.\\
Autor, D. H., A. Manning, and C. L. Smith (2016): "The contribution of the minimum wage to US wage inequality over three decades: A reassessment," American Economic Journal: Applied Economics, 8, 58-99.\\
Baltagi, B. H. (2021): Econometric Analysis of Panel Data, Springer, 6 ed.\\
Brown, B. M. (1971): "Martingale central limit theorems," The Annals of Mathematical Statistics, 59-66.\\
Chamberlain, G. (1982): "Multivariate regression models for panel data," Journal of Econometrics, 18, 5-46.\\
\_ (1987): "Asymptotic efficiency in estimation with conditional moment restrictions," Journal of Econometrics, 34, 305-334.\\
(1994): "Quantile Regression, Censoring, and the Structure of Wages," Advances in econometrics, 1, 171-209.\\
Chernozhukov, V., I. Fern√°ndez-Val, J. Hahn, and W. K. Newey (2013a): "Average and Quantile Effects in Nonseparable Panel Models," Econometrica, 81, 535-580.\\
Chernozhukov, V., I. Fern√°ndez-Val, and B. Melly (2013b): "Inference on Counterfactual Distributions," Econometrica, 81, 2205-2268.

Chernozhukov, V. and C. Hansen (2005): "An IV Model of Quantile Treatment Effects," Econometrica, 73, 245-261.\\
\_ (2006): "Instrumental quantile regression inference for structural and treatment effect models," Journal of Econometrics, 132, 491-525.

Chetverikov, D., B. Larsen, and C. Palmer (2016): "IV Quantile Regression for GroupLevel Treatments, With an Application to the Distributional Effects of Trade," Econometrica, 84, 809-833.

DaI, X. and L. Jin (2021): "Minimum distance quantile regression for spatial autoregressive panel data models with fixed effects," PLoS ONE, 16, 1-13.\\
Engelhardt, G. V. and P. J. Purcell (2021): "The minimum wage and annual earnings inequality," Economics Letters, 207, 110001.\\
Fern√°ndez-Val, I., W. Y. Gao, Y. Liao, and F. Vella (2022): "Dynamic Heterogeneous Distribution Regression Panel Models, with an Application to Labor Income Processes," 1-45.\\
Galvao, A. and A. Poirier (2019): "Quantile Regression Random Effects," Annals of Economics and Statistics, 109-148.\\
Galvao, A. F., J. Gu, and S. Volgushev (2020): "On the unbiased asymptotic normality of quantile regression with fixed effects," Journal of Econometrics, 218, 178-215.

Galvao, A. F. and L. Wang (2015): "Efficient Minimum Distance Estimator for Quantile Regression Fixed Effects Panel Data," Journal of Multivariate Analysis, 133, 1-26.\\
Hahn, J. and G. Kuersteiner (2002): "Asymptotically unbiased inference for a dynamic panel model with fixed effects when both n and T are large," Econometrica, 70, 1639-1657.\\
Hansen, B. (2022a): Probability and Statistics for Economists, Princeton University Press.\\
Hansen, B. E. (2022b): Econometrics, Princeton University Press.\\
Hansen, L. P. (1982): "Large Sample Properties of Generalized Method of Moments Estimators," Econometrica, 50, 1029-1054.\\
Hausman, J. and W. E. Taylor (1981): "Panel Data and Unobservable Individual Effects," Econometrica, 49, 1377.\\
Hausman, J. A. (1978): "Specification Tests in Econometrics," Econometrica, 46, 1251-1271.\\
Im, K. S., S. C. Ahn, P. Schmidt, and J. M. Wooldridge (1999): "Efficient estimation of panel data models with strictly exogenous explanatory variables," Journal of Econometrics, 93, 177-201.\\
Kato, K., A. Galvao Jr., and G. V. Montes-Rojas (2012): "Asymptotics for Panel Quantile Regression Models with Individual Effects," Journal of Econometrics, 170, 76-91.\\
Knight, K. (1998): "Limiting Distributions for L1 Regression Estimators under General Conditions," Annals of Statistics, 26, 755-770.\\
Koenker, R. (2004): "Quantile Regression for Longitudinal Data," Journal of Multivariate Analysis, 91, 74-89.\\
Koenker, R. and G. Bassett (1978): "Regression Quantiles," Econometrica, 46, 33.\\
Liao, Y. and X. Yang (2018): "Uniform Inference for Characteristic Effects of Large Continuous-Time Linear Models," 1-52.\\
Lu, X. and L. Su (2022): "Uniform inference in linear panel data models with two-dimensional heterogeneity," Journal of Econometrics.\\
Melly, B. and G. Santangelo (2015): "The changes-in-changes model with covariates," 1-32.

Mundlak, Y. (1978): "On the Pooling of Time Series and Cross Section Data," Econometrica, 46, 69-85.

Nerlove, M. (1971): "Further Evidence on the Estimation of Dynamic Economic Relations from a Time Series of Cross Sections," Econometrica, 39, 359.

Newey, W. K. (1993): "Efficient estimation of models with conditional moment restrictions," in Handbook of Statistics, Elsevier, vol. 11, chap. 16, 419-454.\\
Newey, W. K. and J. L. Powell (1990): "Efficient Estimation of Linear and Type I Censored Regression Models Under Conditional Quantile Restrictions," Econometric Theory, 6, 295317.

Phillips, P. C. and H. R. Moon (1999): "Linear regression limit theory for nonstationary panel data," Econometrica, 67, 1057-1111.\\
Pons, M. (2024): "Quantile on Quantiles," Working Paper.\\
Powell, J. L. (1991): "Estimation of Monotonic Regression Models under Quantile Restriction," in Nonparametric and Semiparametric Model in Economics, Cambridge: Cambridge University Press.\\
SARGAN, J. D. (1958): "The Estimation of Economic Relationships using Instrumental Variables," Econometrica, 26, 393-415.

Volgushev, S., S.-K. Chao, and G. Cheng (2019): "Distributed inference for quantile regression processes," The Annals of Statistics, 47, 1634-1662.

Wooldridge, J. M. (2010): Econometric analysis of cross section and panel data, MIT press.\\
‚Äî (2019): "Correlated random effects models with unbalanced panels," Journal of Econometrics, 211, 137-150.

\section*{A Proofs}
\section*{A. 1 Proof of Lemma 1: Sampling error}
Proof of lemma 1. Starting from the definition of the estimator we obtain

\[
\begin{aligned}
\hat{\delta}(\tau) & =\left(X^{\prime} Z \hat{W}(\tau) Z^{\prime} X\right)^{-1} X^{\prime} Z \hat{W}(\tau) Z^{\prime} \hat{Y}(\tau) \\
& =\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime} \hat{\beta}_{j}(\tau) \\
& =\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j}\left(\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+\tilde{x}_{i j}^{\prime} \beta_{j}(\tau)\right) \\
& =\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j}\left(\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+x_{i j}^{\prime} \delta(\tau)+\alpha_{j}(\tau)\right) \\
& =\delta(\tau)+\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j}\left(\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+\alpha_{j}(\tau)\right) .
\end{aligned}
\]

\section*{A. 2 Proof of Theorem 1: Uniform consistency}
\section*{A.2.1 Lemma 3}
As a preliminary step to prove the uniform consistency of our estimator, we show uniform (in \(\tau\) and \(j\) ) consistency of the group-level quantile regressions.

Lemma 3 (Uniform consistency of \(\hat{\beta}_{j}(\tau)\) ). Under Assumptions 1-4 and \(7(a)\), we have

\[
\sup _{\tau \in \mathcal{T}} \max _{1 \leq j \leq m}\left\|\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right\|=o_{p}(1) .
\]

Proof of Lemma 3. Angrist et al. (2006) show uniform consistency of the quantile regression estimator in \(\tau\) but not in \(j\) (see their Theorem 3) while Galvao and Wang (2015) show uniform consistency in \(j\) but not in \(\tau\) (see their Lemma 1 ). We show uniformity in both dimensions by following the steps of the proof in Galvao and Wang (2015) and extending it.

We define \(\mathbb{Q}_{n j}(\beta, \tau)=\frac{1}{n} \sum_{i=1}^{n} \rho_{\tau}\left(y_{i j}-\tilde{x}_{i j}^{\prime} \beta\right)-\rho_{\tau}\left(y_{i j}-\tilde{x}_{i j}^{\prime} \beta_{j}(\tau)\right)\) and \(Q_{j}(\beta, \tau)=\mathbb{E}_{i \mid j}\left[\rho_{\tau}\left(y_{i j}-\right.\right.\) \(\left.\left.\tilde{x}_{i j}^{\prime} \beta\right)-\rho_{\tau}\left(y_{i j}-\tilde{x}_{i j}^{\prime} \beta_{j}(\tau)\right)\right]\). As shown in Angrist et al. (2006), the empirical processes \((\beta, \tau) \mapsto\) \(\mathbb{Q}_{n j}(\beta, \tau)\) for all groups \(j\) are stochastically equicontinuous because \(\left|\mathbb{Q}_{n j}\left(\beta^{\prime}, \tau^{\prime}\right)-\mathbb{Q}_{n j}\left(\beta^{\prime \prime}, \tau^{\prime \prime}\right)\right| \leq\) \(C_{1} \cdot\left|\tau^{\prime}-\tau^{\prime \prime}\right|+C_{2} \cdot\left\|\beta^{\prime}-\beta^{\prime \prime}\right\|\) where \(C_{1}=2 \cdot C \cdot \sup _{\beta \in \mathcal{B}}\|\beta\|\) for any compact set \(\mathcal{B}\) and \(C_{2}=2 \cdot C\). The constant \(C\) is defined in Assumption 2. Note that \(C_{1}\) and \(C_{2}\) are neither functions of \(j\) nor \(\tau\).

Fix any \(\zeta>0\). Let \(B_{j}(\zeta, \tau)=\left\{\beta:\left\|\beta-\beta_{j}(\tau)\right\| \leq \zeta\right\}\), the ball with center \(\beta_{j}(\tau)\) and radius \(\zeta\). For each \(\beta \notin B_{j}(\zeta, \tau)\), define \(\tilde{\beta}=r_{j} \beta+\left(1-r_{j}\right) \beta_{j}(\tau)\) where \(r_{j}=\frac{\zeta}{\pi \beta-\beta_{j}(\tau) \pi}\). So \(\tilde{\beta} \in \partial B_{j}(\zeta, \tau)=\left\{\beta:\left\|\beta-\beta_{j}(\tau)\right\|=\zeta\right\}\), the boundary of \(B_{j}(\zeta, \tau)\). Since \(\mathbb{Q}_{n j}(\beta, \tau)\) is convex in\\
\(\beta\) for all \(\tau\), and \(\mathbb{Q}_{n j}\left(\beta_{j}(\tau), \tau\right)=0\), we have

\[
r_{j} \mathbb{Q}_{n j}(\beta, \tau) \geq \mathbb{Q}_{n j}(\tilde{\beta}, \tau)=Q_{j}(\tilde{\beta}, \tau)+\mathbb{Q}_{n j}(\tilde{\beta}, \tau)-Q_{j}(\tilde{\beta}, \tau)>\epsilon_{\zeta}+\mathbb{Q}_{n j}(\tilde{\beta}, \tau)-Q_{j}(\tilde{\beta}, \tau)
\]

uniformly in \(j\) and \(\tau\), where\\
\(\epsilon_{\zeta}=\inf _{\tau \in \mathcal{T}} \inf _{1 \leq j \leq m} \inf _{\left\|\beta-\beta_{j}(\tau)\right\|=\zeta} \mathbb{E}_{i \mid j}\left[\int_{0}^{\tilde{x}_{i j}^{\prime}\left(\beta-\beta_{j}(\tau)\right)}\left(1\left(y_{i j}-\tilde{x}_{i j}^{\prime} \beta_{j}(\tau) \leq s\right)-1\left(y_{i j}-\tilde{x}_{i j}^{\prime} \beta_{j}(\tau) \leq 0\right)\right) d s\right]\)\\
by the identity of Knight (1998) and \(\epsilon_{\zeta}>0\) by Assumptions 3 and 4. Thus, we have that

\[
\begin{aligned}
\left\{\sup _{\tau \in \mathcal{T}} \max _{\leq j \leq m}\left\|\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right\|>\zeta\right\} & \stackrel{(a)}{\subseteq}\left\{\exists \tau_{j} \in \mathcal{T}, \exists \beta_{j} \notin B_{j}\left(\zeta, \tau_{j}\right): \mathbb{Q}_{n j}\left(\beta_{j}, \tau_{j}\right) \leq 0\right\} \\
& \stackrel{(b)}{\subseteq} \cup_{j=1}^{m}\left\{\sup _{\tau \in \mathcal{T}} \sup _{\beta_{j} \in B_{j}\left(\zeta, \tau_{j}\right)}\left|\mathbb{Q}_{n j}\left(\beta_{j}, \tau_{j}\right)-Q_{j}\left(\beta_{j}, \tau_{j}\right)\right| \geq \epsilon_{\zeta}\right\} .
\end{aligned}
\]

Relation (a) holds because, by definition, \(\hat{\beta}_{j}(\tau)\) minimizes \(\mathbb{Q}_{n j}(\beta, \tau)\), and \(\mathbb{Q}_{n j}\left(\beta_{j}(\tau), \tau\right)=0\). Relation (b) holds by the rightmost inequality of line (37). Then, it follows that

\[
\begin{aligned}
\mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \max _{1 \leq j \leq m}\left\|\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right\|>\zeta\right\} & \leq \mathrm{P}\left\{\cup_{j=1}^{m}\left\{\sup _{\tau \in \mathcal{T}} \sup _{\beta_{j} \in B_{j}(\zeta, \tau)}\left|\mathbb{Q}_{n j}\left(\beta_{j}, \tau\right)-Q_{j}\left(\beta_{j}, \tau\right)\right| \geq \epsilon_{\zeta}\right\}\right\} \\
& \leq \sum_{j=1}^{m} \mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \sup _{\beta_{j} \in B_{j}(\zeta, \tau)}\left|\mathbb{Q}_{n j}\left(\beta_{j}, \tau\right)-Q_{j}\left(\beta_{j}, \tau\right)\right| \geq \epsilon_{\zeta}\right\} \\
& \leq m \max _{1 \leq j \leq m} \mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \sup _{\beta_{j} \in B_{j}(\zeta, \tau)}\left|\mathbb{Q}_{n j}\left(\beta_{j}, \tau\right)-Q_{j}\left(\beta_{j}, \tau\right)\right| \geq \epsilon_{\zeta}\right\}
\end{aligned}
\]

Therefore, if we can show that

\[
\max _{1 \leq j \leq m} \mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \sup _{\beta_{j} \in B_{j}(\zeta, \tau)}\left|\mathbb{Q}_{n j}\left(\beta_{j}, \tau\right)-Q_{j}\left(\beta_{j}, \tau\right)\right| \geq \epsilon_{\zeta}\right\}=o\left(\frac{1}{m}\right)
\]

the proof of the lemma will be completed.\\
Without loss of generality, we assume \(\beta_{j}(\tau)=0\) for all \(j\) and \(\tau \in \mathcal{T}\). Then the balls \(B_{j}(\zeta, \tau)\) for all \(j\) and \(\tau \in \mathcal{T}\) are identical and we denote them by \(B(\delta)\). Because the closed ball \(B(\zeta)\) is compact, there exist \(K\) balls with center \(\beta^{k}, k=1, \ldots, K\), and radius \(\frac{\epsilon_{\zeta}}{3 C_{2}}\) such that the collection of them covers \(B(\zeta)\). Since \(\epsilon_{\zeta}>0\), we can find a finite \(K\) that satisfies this condition and is independent of \(j\) and \(\tau\). Therefore, for any \(\beta \in B(\delta)\), there is some \(k \in\{1, \ldots, K\}\) such that

\[
\begin{aligned}
\left|\mathbb{Q}_{n j}(\beta, \tau)-Q_{j}(\beta, \tau)\right|-\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right| & \leq\left|\mathbb{Q}_{n j}(\beta, \tau)-Q_{j}(\beta, \tau)-\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)+Q_{j}\left(\beta^{k}, \tau\right)\right| \\
& \leq\left|\mathbb{Q}_{n j}(\beta, \tau)-\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)\right|+\left|Q_{j}(\beta, \tau)-Q_{j}\left(\beta^{k}, \tau\right)\right| \\
& \leq C_{2} \frac{\epsilon_{\zeta}}{3 C_{2}}+C_{2} \frac{\epsilon_{\zeta}}{3 C_{2}}=\frac{2 \epsilon_{\zeta}}{3}
\end{aligned}
\]

uniformly in \(j\) and \(\tau \in \mathcal{T}\). Note that the third line is justified by the stochastic equicontinuity of \(\mathbb{Q}_{n j}(\beta, \tau)\).

It then follows that,

\[
\sup _{\tau \in \mathcal{T}} \sup _{\beta \in B(\zeta)}\left|\mathbb{Q}_{n j}(\beta, \tau)-Q_{j}(\beta, \tau)\right| \leq \sup _{\tau \in \mathcal{T}} \max _{1 \leq k \leq K}\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right|+\frac{2 \epsilon_{\zeta}}{3},
\]

and

\[
\begin{aligned}
\mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \sup _{\beta \in B(\delta)} \mid \mathbb{Q}_{n j}(\beta, \tau)-Q_{j}(\beta, \tau)>\epsilon_{\zeta}\right\} & \leq \mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \max _{1 \leq k \leq K}\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right|+\frac{2 \epsilon_{\zeta}}{3}>\epsilon_{\zeta}\right\} \\
& =\mathrm{P}\left\{\sup _{\tau \in \mathcal{T}} \max _{1 \leq k \leq K}\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right|>\frac{\epsilon_{\zeta}}{3}\right\} \\
& \leq \sup _{\tau \in \mathcal{T}} \sum_{k=1}^{K} \mathrm{P}\left\{\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right|>\frac{\epsilon_{\zeta}}{3}\right\} .
\end{aligned}
\]

For each \(\tau \in \mathcal{T}, \mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)\) is the sample mean of \(n\) i.i.d. terms bounded in absolute values by \(2 \cdot C \cdot \zeta\). By Hoeffding's inequality, it follows that

\[
\begin{aligned}
\sum_{k=1}^{K} \mathrm{P}\left\{\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right|>\frac{\epsilon_{\zeta}}{3}\right\} & \leq 2 K \exp \left\{-\frac{2 n \epsilon_{\zeta}^{2}}{3^{2} 2^{2} C^{2} \zeta^{2}}\right\} \\
& =2 K \exp \left\{-\frac{n \epsilon_{\zeta}^{2}}{18 C^{2} \zeta^{2}}\right\} \\
& =O(\exp (-n))
\end{aligned}
\]

Since \(\frac{\log m}{n} \rightarrow 0\) by Assumption 7(a), it follows that \(O_{p}(\exp (-n))=o_{p}(1 / m)\). Note that the bound \(\epsilon_{\zeta} / 3>0\) is uniform in \(\tau\), and \(K\) is finite. By stochastic equicontinuity of \(\mathbb{Q}_{n j}(\beta, \tau)\), as \(n \rightarrow \infty\), uniformly in \(\tau \in \mathcal{T}\),

\[
\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)=Q_{j}\left(\beta^{k}, \tau\right)+o_{p}(1) .
\]

It follows that

\[
\sup _{\tau \in \mathcal{T}} \sum_{k=1}^{K} \mathrm{P}\left\{\left|\mathbb{Q}_{n j}\left(\beta^{k}, \tau\right)-Q_{j}\left(\beta^{k}, \tau\right)\right|>\frac{\epsilon_{\zeta}}{3}\right\}=o_{p}\left(\frac{1}{m}\right) .
\]

\section*{A.2.2 Lemma 4}
Lemma 4 (Uniform consistency of \(\hat{G}(\tau)\) with a full-rank weighting matrix). Assumptions 1, 2, 5, and 8 hold. Then,

\[
\sup _{\tau \in \mathcal{T}}\left\|\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau)-\left(\Sigma_{Z X}^{\prime} W(\tau) \Sigma_{Z X}^{\prime}\right)^{-1} \Sigma_{Z X}^{\prime} W(\tau)\right\|=o_{p}(1)
\]

and \(\left(\Sigma_{Z X}^{\prime} W(\tau) \Sigma_{Z X}^{\prime}\right)^{-1} \Sigma_{Z X}^{\prime} W(\tau)\) is uniformly bounded and continuous.\\
Proof of Lemma 4. First, it follows from Assumptions 1(ii), 2(i) and 5(i) that \(\operatorname{Var}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}\right)=\) \(o_{p}\left(\frac{1}{n}\right)\) and \(\mathbb{E}\left[\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}\right]=\mathbb{E}\left[z_{i j} x_{i j}^{\prime}\right]\). Hence, by Assumption \(1(\mathrm{i}), \operatorname{Var}\left(\frac{1}{m} \sum_{j=1}^{m} \frac{1}{n} \sum_{i=1}^{n} x_{i j} z_{i j}^{\prime}\right)=\)\\
\(o_{p}\left(\frac{1}{m n}\right)\). By Chebyshev's inequality,

\[
\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}-\mathbb{E}\left[z_{i j} x_{i j}^{\prime}\right]\right) \underset{p}{\rightarrow} 0
\]

In addition, by Assumption 5 (iii), \(m^{-1} \sum_{j=1}^{m} \mathbb{E}\left[z_{i j} x_{i j}^{\prime}\right] \rightarrow \Sigma_{Z X}\). It follows that

\[
S_{Z X} \underset{p}{\rightarrow} \Sigma_{Z X}
\]

By assumption 8 , uniformly in \(\tau \in \mathcal{T}, \hat{W}(\tau) \underset{p}{\rightarrow} W(\tau)\) where \(W(\tau)\) is uniformly continuous and strictly positive definite. Since \(\Sigma_{Z X}\) is bounded and has full column rank, it implies that \(\Sigma_{Z X}^{\prime} W(\tau) \Sigma_{Z X}\) is also uniformly continuous and invertible. The result of the lemma follows.

\section*{A.2.3 Theorem 1}
Proof of Theorem 1. By Lemma 1,

\[
\hat{\delta}(\tau)-\delta(\tau)=\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j}\left(\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+\alpha_{j}(\tau)\right)
\]

By Lemma 4, the first factor converges uniformly to \(G(\tau)\) :

\[
\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \underset{p}{\rightarrow}\left(\Sigma_{Z X}^{\prime} W(\tau) \Sigma_{Z X}^{\prime}\right)^{-1} \Sigma_{Z X}^{\prime} W(\tau)=G(\tau)
\]

By lemma \(3, \hat{\beta}_{j}(\tau)\) is consistent for \(\beta_{j}(\tau)\) uniformly in \(j\) and \(\tau\). Together with the boundedness of \(x_{i j}\) in Assumption 2(i) and of \(z_{i j}\) in Assumption 5(i), it follows that

\[
\sup _{\tau \in \mathcal{T}} \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right) \underset{p}{\rightarrow} 0
\]

By Assumption 5 (ii), \(\mathbb{E}\left[z_{i j} \alpha_{j}(\tau)\right]=0\) uniformly in \(\tau\). By Assumption 5, \(\operatorname{Var}\left(z_{i j} \alpha_{j}(\tau)\right)\) is uniformly bounded. In addition, \(z_{i j}\) is bounded and \(\alpha_{j}(\tau)\) is uniformly continuous in \(\tau\). Hence,

\[
\sup _{\tau \in \mathcal{T}} \frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \alpha_{j}(\tau) \underset{p}{\rightarrow} 0
\]

The result of the Theorem follows from equations (39), (40), and (41).

\section*{A. 3 Proof of Lemma 2 - Asymptotic distribution of sample moments}
\section*{Proof of Lemma 2. Part (i)}
Lemma 3 in Galvao et al. (2020) provides the uniform Bahadur representation for the grouplevel quantile regression coefficient under our assumptions:

\[
\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)=\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)+R_{n j}^{(1)}(\tau)+R_{n j}^{(2)}(\tau)
\]

where

\[
\phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)=-B_{j, \tau}^{-1} \tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right)-\tau\right),
\]

with \(B_{j, \tau}=\mathbb{E}_{i \mid j}\left[f_{y \mid x}\left(Q_{y \mid x, \nu_{j}}\left(\tau \mid \tilde{x}_{i j}\right) \mid \tilde{x}_{i j}\right) \tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right]\) and

\[
\begin{gathered}
\sup _{j} \sup _{\tau \in \mathcal{T}}\left\|R_{n j}^{(2)}(\tau)\right\|=O_{p}\left(\frac{\log n}{n}\right), \\
\sup _{j} \sup _{\tau \in \mathcal{T}}\left\|\mathbb{E}_{i \mid j}\left[R_{n j}^{(1)}(\tau)\right]\right\|=O\left(\frac{\log n}{n}\right),
\end{gathered}
\]

\[
\sup _{j} \sup _{\tau \in \mathcal{T}}\left\|\mathbb{E}_{i \mid j}\left[\left(R_{n j}^{(1)}(\tau)-\mathbb{E}_{i \mid j}\left[R_{n j}^{(1)}(\tau)\right]\right)\left(R_{n j}^{(1)}(\tau)-\mathbb{E}_{i \mid j}\left[R_{n j}^{(1)}(\tau)\right]\right)^{\prime}\right]\right\|=O\left(\left(\frac{\log n}{n}\right)^{3 / 2}\right) .
\]

It follows that

\[
\begin{aligned}
\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)= & \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right)\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right) \\
& +\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) R_{n j}^{(1)}(\tau) \\
& +\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) R_{n j}^{(2)}(\tau) .
\end{aligned}
\]

Consider first the third term (49). By assumptions 2(i) and \(5(\mathrm{i}), x_{i j}\) and \(z_{i j}\) are bounded by \(C\) such that the sample mean of their product is also bounded. Therefore, (44) implies that

\[
\sup _{\tau \in \mathcal{T}} \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) R_{n j}^{(2)}(\tau)=O_{p}\left(\frac{\log n}{n}\right) .
\]

Consider now the second term (48). Since \(\operatorname{Var}\left(R_{n j}^{(1)}(\tau)\right)=o\left(\frac{1}{n}\right)\) by (46), \(x_{i j}\) and \(z_{i j}\) are bounded by assumptions \(2(\mathrm{i})\) and \(5(\mathrm{i})\), and observations are independent across groups, it follows that \(\operatorname{Var}\left(\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) R_{n j}^{(1)}(\tau)\right)=o_{p}\left(\frac{1}{m n}\right)\). In addition, by (45), \(\sup _{j} \sup _{\tau \in \mathcal{T}} \mathbb{E}_{i \mid j}\left[R_{n j}^{(1)}(\tau)\right]=\) \(O\left(\frac{\log n}{n}\right)\) such that \(\sup _{\tau \in \mathcal{T}} \mathbb{E}_{i \mid j}\left[\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) R_{n j}^{(1)}(\tau)\right]=O\left(\frac{\log n}{n}\right)\). Putting this together, by the Chebyshev inequality and under Assumption 7(c),

\[
\sup _{\tau \in \mathcal{T}} \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) R_{n j}^{(1)}(\tau)=o_{p}\left(\frac{1}{\sqrt{m n}}\right)
\]

It follows that both remainder terms are \(o_{p}\left(\frac{1}{\sqrt{m n}}\right)\) uniformly over \(\tau\).\\
Let \(\Sigma_{Z X j}=\mathbb{E}_{i \mid j}\left[z_{i j} \tilde{x}_{i j}^{\prime}\right]\) and consider now the term (47):

\[
\begin{aligned}
\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\right) & \left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right) \\
= & \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}-\Sigma_{Z X j}\right)\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right) \\
& +\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j}\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)
\end{aligned}
\]

By the boundedness of \(z_{i j}\) and \(x_{i j}\) and the independence of the observations over time, it follows that \(\left\|\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}-\Sigma_{Z X j}\right\|=o(1)\) uniformly in \(j\). In addition, \(\operatorname{Var}\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)=\) \(O\left(\frac{1}{n}\right)\). Hence,

\[
\operatorname{Var}\left(\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}-\Sigma_{Z X j}\right)\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{i, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)\right)=o\left(\frac{1}{m n}\right)
\]

The model in equation (1) and Assumption 5(iii) imply that \(\mathbb{E}_{i \mid j}\left[1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right) \mid \tilde{x}_{i j}, z_{i j}\right]=\tau\), which implies that

\[
\mathbb{E}\left[\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}-\Sigma_{Z X j}\right)\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)\right]=0
\]

uniformly in \(\tau\). Therefore, by Chebyshev's inequality,

\[
\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}-\Sigma_{Z X j}\right)\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)=o_{p}\left(\frac{1}{\sqrt{m n}}\right)
\]

uniformly in \(\tau\).\\
Since all other terms are \(o_{p}\left(\frac{1}{\sqrt{m n}}\right)\) uniformly over \(\tau\), the limiting distribution of the process \(\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\) is the same as the limiting distribution of

\[
\begin{aligned}
\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j} & \left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right) \\
& =\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j}\left(\frac{-B_{j, \tau}^{-1}}{n} \sum_{i=1}^{n} \tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right)-\tau\right)\right)=\frac{1}{m n} \sum_{j=1}^{m} \sum_{i=1}^{n} s_{i j}(\tau)
\end{aligned}
\]

This is a sample mean over \(m n\) independent (but not necessarily identically distributed) observations denoted by \(s_{i j}(\tau)\). The model in equation (1) and Assumption 5(iii) imply that \(\mathbb{E}\left[1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right) \mid \tilde{x}_{i j}, z_{i j}, v_{j}\right]=\tau\), which implies that \(\mathbb{E}\left[s_{i j}(\tau)\right]=0\). In addition,

\[
\operatorname{Var}\left(s_{i j}(\tau)\right)=\mathbb{E}\left[\Sigma_{Z X j} \operatorname{Var}\left(\phi_{j, \tau}\right) \Sigma_{Z X j}^{\prime}\right]=\mathbb{E}\left[\Sigma_{Z X j} B_{j, \tau}^{-1} \tau(1-\tau) \mathbb{E}_{i \mid j}\left[\tilde{x}_{i j} \tilde{x}_{i j}^{\prime}\right] B_{j, \tau}^{-1} \Sigma_{Z X j}^{\prime}\right]
\]

Pointwise asymptotic normality follows from the Lindeberg CLT.\\
Next we note that \(\left\{\Sigma_{Z X j}\left(\frac{-B_{j, \tau}^{-1}}{n} \sum_{i=1}^{n} \tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta\right)-\tau\right)\right), \tau \in \mathcal{T}, \beta \in \mathcal{B}\right\}\) is a Donsker class for any compact set \(\mathcal{B}\). This follows by noting that \(\left\{1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right), \tau \in \mathcal{T}, \beta \in \mathcal{B}\right\}\) is a VC subgraph class and hence a bounded Donsker class. Hence,

\[
\left\{\frac{1}{n} \sum_{i=1}^{n} \tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta\right)-\tau\right), \tau \in \mathcal{T}, \beta \in \mathcal{B}\right\}
\]

is also bounded Donsker with a square-integrable envelope \(2 \cdot \max _{i \in 1, \ldots, n}\left\|\tilde{x}_{i j}\right\| \leq 2 \cdot C\). The whole function is then Donsker by the boundedness of \(\Sigma_{Z X j}\) and \(B_{j, \tau}^{-1}\). The weak convergence result\\
follows by application of the functional central limit theorem for independent but not identically distributed random variables, see, for instance, Theorem 3 in Brown (1971).

Part (ii) Follows directly by Lemma 3 in Chetverikov et al. (2016).\\
Part (iii) The first moment is asymptotically equivalent to (up to a term, which is uniformly \(\left.o_{p}\left(\frac{1}{\sqrt{m n}}\right)\right)\) :

\[
\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j}\left(\frac{-B_{j, \tau}^{-1}}{n} \sum_{i=1}^{n} \tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right)-\tau\right)\right)
\]

We have already shown that both moments have a mean of zero. By Assumption 1, the observations are independent across \(i\) and \(j\) such that we only need to consider the correlation between both moments for the same individual and group:

\[
\begin{aligned}
\operatorname{Cov} & \left(\tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right)-\tau\right), z_{i j} \alpha_{j}\left(\tau^{\prime}\right)\right) \\
& =\mathbb{E}\left[\tilde{x}_{i j}\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right)-\tau\right) z_{i j}^{\prime} \alpha_{j}\left(\tau^{\prime}\right)\right] \\
& =\mathbb{E}\left[\tilde{x}_{i j} \mathbb{E}_{i \mid j}\left[\left(1\left(y_{i j} \leq \tilde{x}_{i j} \beta_{j}(\tau)\right)-\tau\right) \mid x_{i j}, z_{i j}\right] z_{i j}^{\prime} \alpha_{j}(\tau)\right]=0
\end{aligned}
\]

It then directly follows that

\[
\sup _{\tau, \tau^{\prime} \in \mathcal{T}}\left\|\operatorname{Cov}\left(\bar{g}_{m n}^{(1)}(\hat{\delta}, \tau), \bar{g}_{m n}^{(2)}\left(\hat{\delta}, \tau^{\prime}\right)\right)\right\|=o_{p}\left(\frac{1}{\sqrt{m n}}\right) .
\]

\section*{A. 4 Lemma 5: Uniform consistency of the G matrix with a heterogeneous weighting matrix}
Lemma 5. Assumptions 1, 2, 5, and \(8^{\prime}\) hold. Then, uniformly in \(\tau\), we can partition \(\hat{G}(\tau)\) into \(\hat{G}_{11}(\tau)\) (with dimensions \(\left.M_{1} \times L_{1}\right), \hat{G}_{12}(\tau)\left(M_{1} \times L_{2}\right), \hat{G}_{21}(\tau)\left(M_{2} \times L_{1}\right)\), and \(\hat{G}_{22}(\tau)\left(M_{2} \times L_{2}\right)\) such that

\[
\hat{G}(\tau)=\left(\begin{array}{cc}
\hat{G}_{11}(\tau) & \hat{G}_{12}(\tau) \\
\hat{G}_{21}(\tau) & \hat{G}_{22}(\tau)
\end{array}\right)=\left(\begin{array}{cc}
G_{m n, 11}(\tau) & G_{m n, 12}(\tau) \\
G_{m n, 21}(\tau) & G_{m n, 22}(\tau)
\end{array}\right)+\left(\begin{array}{cc}
o_{p}(1) & o_{p}\left(\sqrt{a_{n}(\tau)}\right) \\
o_{p}\left(1 / \sqrt{a_{n}(\tau)}\right) & o_{p}(1)
\end{array}\right)
\]

where \(G_{m n}(\tau)\) is defined in equation (21), \(\sup _{\tau \in \mathcal{T}} G_{m n, 12}(\tau)=O_{p}\left(a_{n}(\tau)\right)\) and the other elements of \(G_{m n}(\tau)\) are \(O_{p}(1)\) uniformly in \(\tau\).

Proof. To simplify the notation, we omit the dependency on \(\tau\). We partition \(S_{Z X}^{\prime}\) into four submatrices that have the same dimensions as the submatrices of \(\hat{G}\). As in proposition 3, we also partition \(\hat{W}\) into four submatrices such that the diagonal submatrices are square matrices of dimensions \(L_{1}\) and \(L_{2}\). Note that

\[
S_{Z X}^{\prime} \hat{W}=\left(\begin{array}{cc}
S_{11}^{\prime} & S_{21}^{\prime} \\
0 & S_{22}^{\prime}
\end{array}\right)\left(\begin{array}{cc}
\hat{W}_{11} & \hat{W}_{12} \\
\hat{W}_{21} & \hat{W}_{22}
\end{array}\right)=\left(\begin{array}{cc}
S_{11}^{\prime} \hat{W}_{11}+S_{21}^{\prime} \hat{W}_{21} & S_{11}^{\prime} \hat{W}_{12}+S_{21}^{\prime} \hat{W}_{22} \\
S_{22}^{\prime} \hat{W}_{21} & S_{22}^{\prime} \hat{W}_{22}
\end{array}\right)
\]

Furthermore,

\[
\begin{aligned}
S_{Z X}^{\prime} \hat{W} S_{Z X} & =\left(\begin{array}{cc}
S_{11}^{\prime} & S_{21}^{\prime} \\
0 & S_{22}^{\prime}
\end{array}\right)\left(\begin{array}{ll}
\hat{W}_{11} & \hat{W}_{12} \\
\hat{W}_{21} & \hat{W}_{22}
\end{array}\right)\left(\begin{array}{cc}
S_{11} & 0 \\
S_{21} & S_{22}
\end{array}\right) \\
& =\left(\begin{array}{cc}
\hat{A}_{11}+\hat{B}_{11} & \hat{B}_{12} \\
\hat{B}_{21} & \hat{B}_{22}
\end{array}\right),
\end{aligned}
\]

where \(\hat{A}_{11}=S_{11}^{\prime} \hat{W}_{11} S_{11}, \hat{B}_{11}=S_{11}^{\prime} \hat{W}_{12} S_{21}+S_{21}^{\prime} \hat{W}_{21} S_{11}+S_{21}^{\prime} \hat{W}_{22} S_{21}, \hat{B}_{12}=S_{11}^{\prime} \hat{W}_{12} S_{22}+\) \(S_{21}^{\prime} \hat{W}_{22} S_{22}, \hat{B}_{21}=S_{22}^{\prime} \hat{W}_{21} S_{11}+S_{22}^{\prime} \hat{W}_{22} S_{21}\), and \(\hat{B}_{22}=S_{22}^{\prime} \hat{W}_{22} S_{22}\).

For the non-zero elements of the matrix \(S_{Z X}\), we have

\[
\begin{aligned}
& S_{11}-\Sigma_{11}=o_{p}(1), \\
& S_{22}-\Sigma_{22}=o_{p}(1), \\
& S_{21}-\Sigma_{21}=o_{p}(1) .
\end{aligned}
\]

Together with proposition 3, this implies that uniformly over \(\tau\),

\[
\begin{aligned}
& \hat{A}_{11}=\Sigma_{11}^{\prime} W_{11} \Sigma_{11}+o_{p}(1), \\
& \hat{B}_{11}=a_{n} \Sigma_{11}^{\prime} W_{12} \Sigma_{21}+a_{n} \Sigma_{21}^{\prime} W_{21} \Sigma_{11}+a_{n} \Sigma_{21}^{\prime} W_{22} \Sigma_{21}+o_{p}\left(\sqrt{a_{n}}\right)=a_{n} B_{11}+o_{p}\left(\sqrt{a_{n}}\right), \\
& \hat{B}_{12}=a_{n} \Sigma_{11}^{\prime} W_{12} \Sigma_{22}+a_{n} \Sigma_{21}^{\prime} W_{22} \Sigma_{22}+o_{p}\left(\sqrt{a_{n}}\right)=a_{n} B_{12}+o_{p}\left(\sqrt{a_{n}}\right), \\
& \hat{B}_{21}=a_{n} \Sigma_{22}^{\prime} W_{21} \Sigma_{11}+a_{n} \Sigma_{22}^{\prime} W_{22} \Sigma_{21}+o_{p}\left(\sqrt{a_{n}}\right)=a_{n} B_{21}+o_{p}\left(\sqrt{a_{n}}\right), \\
& \hat{B}_{22}=a_{n} \Sigma_{22}^{\prime} W_{22} \Sigma_{22}+o_{p}\left(a_{n}\right)=a_{n} B_{22}+o_{p}\left(a_{n}\right) .
\end{aligned}
\]

By the inverse of a partitioned matrix

\[
\left(S_{Z X}^{\prime} \hat{W} S_{Z X}\right)^{-1}=\left(\begin{array}{cc}
\hat{C}+\hat{C} \hat{B}_{12} \hat{D} \hat{B}_{21} \hat{C} & -\hat{C} \hat{B}_{12} \hat{D} \\
-\hat{D} \hat{B}_{21} \hat{C} & \hat{D}
\end{array}\right),
\]

where \(\hat{C}=\left(\hat{A}_{11}+\hat{B}_{11}\right)^{-1}\) and \(\hat{D}=\left(\hat{B}_{22}-\hat{B}_{21} \hat{C} \hat{B}_{12}\right)^{-1}\). Hence uniformly over \(\tau\),

\[
\begin{aligned}
\hat{C} & =C+o_{p}(1), \\
\hat{D} & =a_{n}^{-1} D+o_{p}\left(a_{n}^{-1}\right), \\
\hat{D} \hat{B}_{21} \hat{C} & =D B_{21} C+o_{p}\left(a_{n}^{-1 / 2}\right),
\end{aligned}
\]

where \(C\) and \(D\) are strictly positive and bounded. Equation (62) holds because

\[
a_{n} \hat{D}=\left(a_{n}^{-1} \hat{B}_{22}-a_{n}^{-1} \hat{B}_{21} \hat{C} \hat{B}_{12}\right)^{-1}=\left(B_{22}-a_{n} B_{21} C B_{12}\right)^{-1}+o_{p}(1)=D+o_{p}(1) .
\]

Combining equation (59) with equations (61)-(62), we obtain similarly

\[
\hat{C} \hat{B}_{12} \hat{D}=C B_{12} D+o_{p}\left(a_{n}^{-1 / 2}\right) .
\]

Now, we can consider each submatrix of \(\hat{G}\) separately and derive their convergence rate to the corresponding element of the \(G_{m n}\) matrix defined in equation (21):

\[
G_{m n}(\tau)=\left(\Sigma_{Z X}^{\prime} W_{m n}(\tau) \Sigma_{Z X}\right)^{-1} \Sigma_{Z X}^{\prime} W_{m n}(\tau) .
\]

For the upper left term, we have

\[
\begin{aligned}
\hat{G}_{11}= & \left(\hat{C}+\hat{C} \hat{B}_{12} \hat{D} \hat{B}_{21} \hat{C}\right)\left(S_{11}^{\prime} \hat{W}_{11}+S_{21}^{\prime} \hat{W}_{21}\right)-\hat{C} \hat{B}_{12} \hat{D} S_{22}^{\prime} \hat{W}_{21} \\
= & \left(C+a_{n} C B_{12} D B_{21} C+o_{p}(1)\right)\left(\Sigma_{11}^{\prime} W_{11}+a_{n} \Sigma_{21}^{\prime} W_{21}+o_{p}(1)\right) \\
& +\left(C B_{12} D+o_{p}\left(a_{n}^{-1 / 2}\right)\right)\left(a_{n} \Sigma_{22}^{\prime} W_{21}+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
= & \left(C+a_{n} C B_{12} D B_{21} C\right)\left(\Sigma_{11}^{\prime} W_{11}+a_{n} \Sigma_{21}^{\prime} W_{21}\right)-a_{n} C B_{12} D \Sigma_{22}^{\prime} W_{21}+o_{p}(1) \\
= & G_{m n, 11}+o_{p}(1),
\end{aligned}
\]

uniformly over \(\tau\). For the upper right term, we have uniformly over \(\tau\),

\[
\begin{aligned}
\hat{G}_{12}= & \left(\hat{C}+\hat{C} \hat{B}_{12} \hat{D} \hat{B}_{21} \hat{C}\right)\left(S_{11}^{\prime} \hat{W}_{12}+S_{21}^{\prime} \hat{W}_{22}\right)-\hat{C} \hat{B}_{12} \hat{D} S_{22}^{\prime} \hat{W}_{22} \\
= & \left(C+a_{n} C B_{12} D B_{21} C+o_{p}(1)\right)\left(a_{n}\left(\Sigma_{11}^{\prime} W_{12}+\Sigma_{21}^{\prime} W_{22}\right)+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
& -\left(C B_{12} D+o_{p}\left(a_{n}^{-1 / 2}\right)\right)\left(a_{n} \Sigma_{22}^{\prime} W_{22}+o_{p}\left(a_{n}\right)\right) \\
= & a_{n}\left(C+a_{n} C B_{12} D B_{21} C\right)\left(\Sigma_{11}^{\prime} W_{12}+\Sigma_{21}^{\prime} W_{22}\right)-a_{n} C B_{12} D \Sigma_{22}^{\prime} W_{22}+o_{p}\left(\sqrt{a_{n}}\right) \\
= & G_{m n, 12}+o_{p}\left(\sqrt{a_{n}}\right) .
\end{aligned}
\]

For the lower right term,

\[
\begin{aligned}
\hat{G}_{22} & =-\hat{D} \hat{B}_{21} \hat{C}\left(S_{11}^{\prime} \hat{W}_{12}+S_{21}^{\prime} \hat{W}_{22}\right)+\hat{D} S_{22}^{\prime} \hat{W}_{22} \\
& =-\left(D B_{21} C+o_{p}\left(a_{n}{ }^{-1 / 2}\right)\right)\left(a_{n} \Sigma_{11}^{\prime} W_{12}+a_{n} \Sigma_{21}^{\prime} W_{22}+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
& =-a_{n} D B_{21} C\left(\Sigma_{11}^{\prime} W_{12}+\Sigma_{21}^{\prime} W_{22}\right)+D \Sigma_{22}^{\prime} W_{22}+o_{p}(1) \\
& =G_{m n, 22}+o_{p}(1),
\end{aligned}
\]

uniformly over \(\tau\). Finally, for the lower left term, uniformly of \(\tau\),

\[
\begin{aligned}
\hat{G}_{21}= & -\hat{D} \hat{B}_{21} \hat{C}\left(S_{11}^{\prime} \hat{W}_{11}+S_{21}^{\prime} \hat{W}_{21}\right)+\hat{D} S_{22}^{\prime} \hat{W}_{21} \\
= & -\left(D B_{21} C+o_{p}\left(a_{n}^{-1 / 2}\right)\right)\left(\Sigma_{11}^{\prime} W_{11}+a_{n} \Sigma_{21}^{\prime} W_{21}+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
& +\left(a_{n}^{-1} D+o_{p}\left(a_{n}^{-1}\right)\right)\left(\Sigma_{22}^{\prime} a_{n} W_{21}+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
= & -D B_{21} C\left(\Sigma_{11}^{\prime} W_{11}-\Sigma_{21}^{\prime} a_{n} W_{21}\right)+D \Sigma_{22}^{\prime} W_{21}+o_{p}\left(a_{n}{ }^{-1 / 2}\right) \\
= & G_{m n, 21}+o_{p}\left(a_{n}^{-1 / 2}\right) .
\end{aligned}
\]

Hence,

\[
\sup _{\tau}\left\|\hat{G}(\tau)-G_{m n}(\tau)\right\|=\left(\begin{array}{cc}
o_{p}(1) & o_{p}\left(\sqrt{a_{n}(\tau)}\right) \\
o_{p}\left(1 / \sqrt{a_{n}(\tau)}\right) & o_{p}(1)
\end{array}\right)
\]

where \(G_{m n, 12}(\tau)=O_{p}\left(a_{n}(\tau)\right)\) and the other elements of \(G_{m n}(\tau)\) are \(O_{p}(1)\) uniformly over \(\tau\).

\section*{A. 5 Proof of Theorem 2: Degree of heterogeneity is known}
Proof of Theorem 2. From the definition of the estimator,

\[
\hat{\delta}(\tau)-\delta(\tau)=\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \bar{g}_{m n}(\hat{\delta}, \tau)
\]

In part (i) Lemma 5 applies uniformly in \(\tau\) with \(a_{n}(\tau)=O_{p}(1 / n)\) such that

\[
\begin{aligned}
\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) & =\left(\begin{array}{ll}
\hat{G}_{11}(\tau) & \hat{G}_{12}(\tau) \\
\hat{G}_{21}(\tau) & \hat{G}_{22}(\tau)
\end{array}\right) \\
& =\left(\begin{array}{cc}
G_{11}(\tau)+o_{p}(1) & o_{p}(1 / \sqrt{n}) \\
o_{p}(\sqrt{n}) & G_{22}(\tau)+o_{p}(1)
\end{array}\right) .
\end{aligned}
\]

By the definition of the fast instruments, the first \(L_{1}\) elements of \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)\) are equal to zero. Together with Lemma 2(i), this implies that

\[
\sqrt{m n} \bar{g}_{m n, 1}(\hat{\delta}, \cdot) \rightsquigarrow \mathbb{Z}_{11}(\cdot),
\]

where \(\bar{g}_{m n, 1}(\hat{\delta}, \cdot)\) contains the first \(L_{1}\) elements of \(\bar{g}_{m n}(\hat{\delta}, \cdot)\). For the remaining \(L_{2}\) elements, we have \(\sqrt{m} \bar{g}_{m n, 2}(\hat{\delta}, \cdot)=O_{p}(1)\). It follows that

\[
\begin{aligned}
\sqrt{m n}\left(\hat{\delta}_{1}(\cdot)-\delta_{1}(\cdot)\right) & =\hat{G}_{11} \sqrt{m n} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+\hat{G}_{12} \bar{g}_{m n, 2}(\hat{\delta}, \cdot) \\
& =G_{11} \sqrt{m n} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+o_{p}(1) \sqrt{m n} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+o_{p}(1 / \sqrt{n}) \sqrt{m n} \bar{g}_{m n, 2}(\hat{\delta}, \cdot) \\
& =G_{11} \sqrt{m n} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+o_{p}(1) \sqrt{m n} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+o_{p}(1) \sqrt{m} \bar{g}_{m n, 2}(\hat{\delta}, \cdot) \\
& \rightsquigarrow G_{11} \mathbb{Z}_{11}(\cdot)+o_{p}(1),
\end{aligned}
\]

which proves part (i)-(a) of the theorem.\\
For the remaining \(M_{2}\) coefficients, applying the same results, we obtain

\[
\begin{aligned}
\sqrt{m}\left(\hat{\delta}_{2}(\cdot)-\delta_{2}(\cdot)\right) & =\hat{G}_{21} \sqrt{m} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+\hat{G}_{22} \sqrt{m} \bar{g}_{m n, 2}(\hat{\delta}, \cdot) \\
& =o_{p}(\sqrt{n}) \sqrt{m} \bar{g}_{m n, 1}(\hat{\delta}, \cdot)+G_{22} \sqrt{m} \bar{g}_{m n, 2}(\hat{\delta}, \cdot)+o_{p}(1) \sqrt{m} \bar{g}_{m n, 2}(\hat{\delta}, \cdot) \\
& \rightsquigarrow G_{22}(\cdot) \mathbb{Z}_{22}(\cdot)+o_{p}(1) .
\end{aligned}
\]

The result of part (i)-(b) of the theorem follows.\\
In part (ii) and (iii), Lemma 4 applies such that

\[
\left(S_{Z X}^{\prime} \hat{W}(\tau) S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{W}(\tau) \underset{p}{\rightarrow} G(\tau)
\]

uniformly in \(\tau \in \mathcal{T}\). In part (ii), \(\bar{g}_{m n}(\hat{\delta}, \cdot)=\bar{g}_{m n}^{(1)}(\hat{\delta}, \cdot)\). It follows from Lemma 2(i) that

\[
\sqrt{m n} \bar{g}_{m n}(\hat{\delta}, \cdot) \rightsquigarrow \mathbb{Z}_{1}(\cdot) .
\]

The result of part (ii) of the theorem follows from equations (65), (67), and (68).\\
In part (iii), \(\bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)\) and \(\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)\) converge at the same rate such that Lemma 2 implies

\[
\sqrt{m n} \bar{g}_{m n}(\hat{\delta}, \cdot) \rightsquigarrow \mathbb{Z}(\cdot),
\]

where \(\mathbb{Z}(\cdot)\) is a mean-zero Gaussian process with uniformly continuous sample paths and covariance function \(\Omega_{1}\left(\tau, \tau^{\prime}\right)+\bar{\Omega}_{1}\left(\tau, \tau^{\prime}\right)\). The result of part (iii) of the theorem follows from equations (65), (67), and (69).

\section*{A. 6 Proof of Theorem 3: Adaptive Asymptotic Distribution}
Proof of Theorem 3. We prove the theorem when Assumption \(8^{\prime}\) holds. The proof when instead Assumption 8 holds is simpler because all coefficients converge at the same rate; it can actually be considered as a special case of the proof below when we set \(a_{n}(\tau)=1\).

By Lemma 5, uniformly in \(\tau \in \mathcal{T}\),

\[
\hat{G}(\tau)=G_{m n}(\tau)+\left(\begin{array}{cc}
o_{p}(1) & o_{p}\left(\sqrt{a_{n}(\tau)}\right) \\
o_{p}\left(1 / \sqrt{a_{n}(\tau)}\right) & o_{p}(1)
\end{array}\right)
\]

where \(G_{m n, 12}(\tau)=O_{p}\left(a_{n}(\tau)\right)\) and the other elements are \(O_{p}(1)\).\\
Since the rate of convergence may differ across the quantile index and the regressors, we first consider the scalar \(\hat{\delta}_{k}(\tau)\), which is the \(k\)-th element of \(\hat{\delta}(\tau)\) for \(k \in\{1, \ldots, K\}\) and \(\tau \in \mathcal{T}\). From Lemma 1,

\[
\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)=\hat{G}_{k}(\tau) \bar{g}_{m n}(\hat{\delta}, \tau)=\hat{G}_{k}(\tau)\left(\bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)+\bar{g}_{m n}^{(2)}(\hat{\delta}, \tau)\right)
\]

where \(\hat{G}_{k}(\tau)\) is the \(k\)-th row of \(\hat{G}(\tau)\). From the proof of Lemma 2(i), we have

\[
\bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)=\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)+o_{p}\left(\frac{1}{\sqrt{m n}}\right),
\]

uniformly in \(\tau \in \mathcal{T}\).\\
We deal separately with the fast and slow coefficients and show that the result holds for both types of coefficients. For \(k \in\left\{1, \ldots, M_{1}\right\}\) (fast coefficients), \(\hat{G}_{k}(\tau)\) is uniformly bounded in probability, which implies that

\[
\hat{G}_{k}(\tau) \bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)=\hat{G}_{k}(\tau) \frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)+o_{p}\left(\frac{1}{\sqrt{m n}}\right),
\]

uniformly in \(\tau \in \mathcal{T}\). In addition, 2(i) implies that \(\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)=O_{p}\left(\frac{1}{\sqrt{m n}}\right)\) uniformly in \(\tau \in \mathcal{T}\). For these coefficients, \(\sup _{\tau \in \mathcal{T}}\left\|\hat{G}_{k}(\tau)-G_{m n, k}(\tau)\right\|=o_{p}(1)\). This, in turn, implies that

\[
\left(\hat{G}_{k}(\tau)-G_{m n, k}(\tau)\right) \frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)=o_{p}\left(\frac{1}{\sqrt{m n}}\right),
\]

uniformly in \(\tau \in \mathcal{T}\). Combining the last two displayed equations, we obtain

\[
\hat{G}_{k}(\tau) \bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)=G_{m n, k}(\tau) \frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j}\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)+o_{p}\left(\frac{1}{\sqrt{m n}}\right),
\]

uniformly in \(\tau \in \mathcal{T}\).\\
For \(k \in\left\{M_{1}+1, \ldots, K\right\}\) (slow coefficients), by Lemma 5, uniformly in \(\tau, \hat{G}_{k}(\tau)-G_{m n, k}(\tau)=\) \(o_{p}\left(1 / \sqrt{a_{n}(\tau)}\right)\) and \(G_{m n, k}=O_{p}(1)\) such that \(\hat{G}_{k}=O_{p}\left(1 / \sqrt{a_{n}(\tau)}\right)\). In addition, Lemma 2(i) implies that

\[
\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)=O_{p}\left(\frac{1}{\sqrt{m n}}\right) .
\]

It follows that

\[
\hat{G}_{k}(\tau) \bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)=\hat{G}_{k}(\tau) \frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)+o_{p}\left(\frac{1}{\sqrt{m n a_{n}(\tau)}}\right)
\]

and

\[
\left(\hat{G}_{k}(\tau)-G_{m n, k}(\tau)\right) \frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X} \frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)=o_{p}\left(\frac{1}{\sqrt{m n a_{n}(\tau)}}\right)
\]

uniformly in \(\tau \in \mathcal{T}\). Combining the last two displayed equations, we obtain

\[
\hat{G}_{k}(\tau) \bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)=G_{m n, k}(\tau) \frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X}\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)+o_{p}\left(\frac{1}{\sqrt{m n a_{n}(\tau)}}\right)
\]

uniformly in \(\tau \in \mathcal{T}\).\\
Lemma 2(ii) implies that

\[
\bar{g}_{m n, l}^{(2)}(\delta, \tau)=\frac{1}{m} \sum_{j=1}^{m} \bar{z}_{j, l} \alpha_{j}(\tau)=O_{p}\left(\frac{1}{\sqrt{m}}\right) \sqrt{\Omega_{2, l l}(\tau)}
\]

uniformly in \(\tau \in \mathcal{T}\), where \(g_{m n, l}^{(2)}(\delta, \tau)\) is the \(l\)-th element of \(g_{m n}^{(2)}(\delta, \tau), z_{j, l}\) is the \(l\)-th element of \(z_{j}\), and \(\Omega_{2, l l}(\tau)\) it the element in the \(l\)-th row and \(l\)-th column of \(\Omega_{2}(\tau)\). Let \(\hat{G}_{k l}(\tau)\) and \(G_{m n, k l}(\tau)\) be the element in the \(k\)-th row and \(l\)-th column of \(\hat{G}(\tau)\) and \(G_{m n}(\tau)\), respectively. Note that \(\hat{G}_{k l}(\tau)-G_{m n, k l}(\tau)=o_{p}\left(\sqrt{G_{m n, k l}(\tau)}\right)\) for \(l \in\left\{L_{1}+1, \ldots, L\right\}\) and \(k \in\{1, \ldots, K\}\). Thus, we have

\[
\begin{aligned}
\left(\hat{G}_{k}(\tau)-G_{m n}(\tau)\right) \bar{g}_{m n}^{(2)}(\hat{\delta}, \tau) & =\sum_{l=L_{1}+1}^{L}\left(\hat{G}_{k l}(\tau)-G_{m n, k l}(\tau)\right) \bar{g}_{m n, l}^{(2)}(\hat{\delta}, \tau) \\
& =\sum_{l=L_{1}+1}^{L} o_{p}\left(\sqrt{G_{m n, k l}}\right) O_{p}\left(\frac{1}{\sqrt{m}}\right) \sqrt{\Omega_{2, l l}(\tau)} \\
& =o_{p}\left(\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}}\right) \sum_{l=L_{1}+1}^{L} \sqrt{\left.G_{m n, k l}(\tau)\right)},
\end{aligned}
\]

uniformly in \(\tau \in \mathcal{T}\).\\
If \(k \in\left\{1, \ldots, M_{1}\right\}\), define

\[
\zeta(k, \tau)=\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}} \sum_{l=L_{1}+1}^{L} \sqrt{G_{m n, k l}(\tau)}
\]

If \(k \in\left\{M_{1}+1, \ldots, K\right\}\), define

\[
\zeta(k, \tau)=\frac{1}{\sqrt{m n a_{n}(\tau)}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}} \sum_{l=L_{1}+1}^{L} \sqrt{G_{m n, k l}(\tau)} .
\]

Thus, uniformly in \(\tau \in \mathcal{T}\),

\[
\begin{aligned}
\hat{\delta}_{k}(\tau)-\delta_{k}(\tau) & =\hat{G}_{k}(\tau) \bar{g}_{m n}^{(1)}(\hat{\delta}, \tau)+\hat{G}_{k}(\tau) \bar{g}_{m n}^{(2)}(\hat{\delta}, \tau) \\
& =G_{m n, k}(\tau)\left(\frac{1}{m} \sum_{j=1}^{m} \Sigma_{Z X j}\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)+\frac{1}{m} \sum_{j=1}^{m} \bar{z}_{j} \alpha_{j}(\tau)\right)+o_{p}(\zeta(k, \tau))
\end{aligned}
\]

Hence, we have

\[
\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)=\sum_{j=1}^{m} d_{j}(k, \tau)+o_{p}(\zeta(k, \tau)),
\]

where

\[
d_{j}(k, \tau)=G_{m n, k}(\tau)\left(\frac{1}{m} \Sigma_{Z X j}\left(\frac{1}{n} \sum_{i=1}^{n} \phi_{j, \tau}\left(\tilde{x}_{i j}, y_{i j}\right)\right)+\frac{1}{m} \bar{z}_{j} \alpha_{j}(\tau)\right) .
\]

Let \(D_{j}\) be the \(T K \times 1\) vector \(\left(\operatorname{diag}\left(\Sigma_{m n}\left(\tau_{1}\right)\right)^{-1 / 2} d_{j}\left(\tau_{1}\right), \ldots, \operatorname{diag}\left(\Sigma_{m n}\left(\tau_{T}\right)\right)^{-1 / 2} d_{j}\left(\tau_{T}\right)\right)^{\prime}\) where \(d_{j}(\tau)=\left(d_{j}(1, \tau), d_{j}(2, \tau), \ldots, d_{j}(K, \tau)\right)^{\prime}\). It follows that

\[
\operatorname{Var}\left(\sum_{j=1}^{m} D_{j}\right)=H_{m n}
\]

Then, from the proof of Lemma 2 and Assumption 9,

\[
H_{m n}^{-1 / 2} \sum_{j=1}^{m} D_{j} \underset{d}{\rightarrow} N\left(0, I_{T K}\right) .
\]

By Slutsky's theorem,

\[
H^{-1 / 2} \sum_{j=1}^{m} D_{j}=H_{m n}^{-1 / 2} \sum_{j=1}^{m} D_{j}+o_{p}(1) \underset{d}{\rightarrow} N\left(0, I_{T K}\right)
\]

In the proof of Lemma 2 we show that \(\left.\sum_{j} \operatorname{diag}\left(\Sigma_{m n}(\cdot)\right)^{-1 / 2} d_{j}(\cdot)\right)\) is asymptotically tight in \(\ell^{\infty}(\mathcal{T})\). It follows that the process \(\left.\sum_{j} \operatorname{diag}\left(\sum_{m n}(\cdot)\right)^{-1 / 2} d_{j}(\cdot)\right)\) weakly converges to \(\mathbb{Z}\), a centered Gaussian process with covariance kernel \(H\left(\tau, \tau^{\prime}\right)\).

Finally, we show that \(o_{p}(1) \sup _{\tau \in \mathcal{T}, k \in\{1, \ldots, K\}} \zeta(k, \tau) \Sigma_{m n, k}(\tau)^{-1 / 2}=o_{p}(1)\), where \(\Sigma_{m n, k}(\tau)\) is the \((k, k)\) element of \(\Sigma_{m n}(\tau)\). We consider separately the fast and slow coefficients For \(k \in\) \(\left\{1, \ldots, M_{1}\right\}\) (fast coefficients), note that

\[
\Sigma_{m n, k}(\tau)^{1 / 2}=O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{n \sqrt{m}}\right)=O_{p}\left(\frac{1}{\sqrt{m n}}\right)
\]

and

\[
\begin{aligned}
\zeta(k, \tau) & =O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}(\alpha(\tau))}}{\sqrt{m}} \sum_{l=L_{1}}^{L} \sqrt{G_{m n, k l}(\tau)}\right) \\
& =O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}} \frac{1}{\sqrt{1+\operatorname{Var}\left(\alpha_{j}(\tau)\right) n}}\right) \\
& =O_{p}\left(\frac{1}{\sqrt{m n}}\right),
\end{aligned}
\]

both uniformly in \(\tau\). It follows that \(o_{p}(1) \sup _{\tau \in \mathcal{T}} \zeta(k, \tau) \Sigma_{m n, k}(\tau)^{-1 / 2}=o_{p}(1)\).\\
For \(k \in\left\{M_{1}+1, \ldots, K\right\}\) (slow coefficients), note that

\[
\Sigma_{m n, k}(\tau)^{1 / 2}=O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}}\right)
\]

and

\[
\zeta(k, \tau)=\frac{1}{\sqrt{m n a_{n}(\tau)}}+\frac{\sqrt{\operatorname{Var}(\alpha(\tau))}}{\sqrt{m}} \sum_{l=L_{1}}^{L} \sqrt{G_{m n, k l}(\tau)} .
\]

For the first term of \(\zeta(k, \tau)\), we obtain

\[
\begin{aligned}
\frac{1}{\sqrt{m n a_{n}(\tau)}} \frac{1}{\Sigma_{m n, k}(\tau)^{1 / 2}} & =O_{p}\left(\frac{1}{\sqrt{m n a_{n}(\tau)}} \frac{1}{\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}}}\right) \\
& =O_{p}\left(\frac{1}{\sqrt{a_{n}(\tau)}} \frac{1}{1+\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right) n}}\right) \\
& =O_{p}\left(\frac{\sqrt{1+\operatorname{Var}\left(\alpha_{j}(\tau)\right) n}}{1+\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right) n}}\right) \\
& =O_{p}(1) .
\end{aligned}
\]

For the second term of \(\zeta(k, \tau)\), we obtain

\[
\begin{aligned}
\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}} \sum_{l=L_{1}}^{L} \sqrt{G_{m n, k l}(\tau)} \Sigma_{m n, k(\tau)}^{-1 / 2} & =O_{p}\left(\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}} \frac{1}{\sqrt{m n}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}}}\right) \\
& =O_{p}(1)
\end{aligned}
\]

It follows that, also in this second case, \(o_{p}(1) \sup _{\tau \in \mathcal{T}} \zeta(k, \tau) \Sigma_{m n, k}(\tau)^{-1 / 2}=o_{p}(1)\).\\
Hence, uniformly in \(\tau\),

\[
\begin{aligned}
\operatorname{diag}\left(\Sigma_{m n}(\tau)\right)^{-1 / 2}(\hat{\delta}(\tau)-\delta(\tau)) & =\operatorname{diag}\left(\Sigma_{m n}(\tau)\right)^{-1 / 2}\left(\sum_{j=1}^{m} d_{j}(\tau)+o_{p}(\zeta(\tau))\right) \\
& =\operatorname{diag}\left(\Sigma_{m n}(\tau)\right)^{-1 / 2} \sum_{j=1}^{m} d_{j}(\tau)+o_{p}(1) \\
& \rightsquigarrow \mathbb{G}(\tau),
\end{aligned}
\]

where \(\zeta(\tau)=(\zeta(1, \tau), \ldots, \zeta(K, \tau))^{\prime}\).

\section*{A. 7 Proof of Propositions 1 and \(1^{\prime}\) : Properties of \(\hat{\Omega}\left(\tau, \tau^{\prime}\right)\)}
\section*{A.7.1 Proposition 1}
Proof of Proposition 1. We use \(\hat{u}_{i j}(\tau)=\tilde{x}_{i j}^{\prime} \hat{\beta}_{j}(\tau)-x_{i j}^{\prime} \hat{\delta}(\tau)=\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+x_{i j}^{\prime}(\delta(\tau)-\) \(\hat{\delta}(\tau))+\alpha_{j}(\tau)\) to obtain

\[
\begin{aligned}
\hat{\Omega}\left(\tau, \tau^{\prime}\right)=\frac{1}{m} \sum_{j=1}^{m}\{ & \left.\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \hat{u}_{i j}(\tau)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \hat{u}_{i j}\left(\tau^{\prime}\right)\right)^{\prime}\right\} \\
=\frac{1}{m} \sum_{j=1}^{m}\{ & \left(\frac{1}{n} \sum_{i=1}^{n} z_{i j}\left[\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)+x_{i j}^{\prime}(\delta(\tau)-\hat{\delta}(\tau))+\alpha_{j}(\tau)\right]\right) \\
& \left.\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j}\left[\tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}\left(\tau^{\prime}\right)-\beta_{j}\left(\tau^{\prime}\right)\right)+x_{i j}^{\prime}\left(\delta\left(\tau^{\prime}\right)-\hat{\delta}\left(\tau^{\prime}\right)\right)+\alpha_{j}\left(\tau^{\prime}\right)\right]\right)^{\prime}\right\} \\
=\frac{1}{m} \sum_{j=1}^{m}\{ & \left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}\left(\tau^{\prime}\right)-\beta_{j}\left(\tau^{\prime}\right)\right)\right)^{\prime} \\
& +\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}(\tau)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}\left(\tau^{\prime}\right)\right)^{\prime} \\
& +\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right)^{\prime} \\
& -\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right)^{\prime} \\
& -\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}\left(\tau^{\prime}\right)-\beta_{j}\left(\tau^{\prime}\right)\right)\right)^{\prime} \\
& +\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}\left(\tau^{\prime}\right)\right)^{\prime} \\
& +\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}(\tau)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}\left(\tau^{\prime}\right)-\beta_{j}\left(\tau^{\prime}\right)\right)\right)^{\prime} \\
& -\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}\left(\tau^{\prime}\right)\right)^{\prime} \\
& \left.-\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}(\tau)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right)^{\prime}\right\}
\end{aligned}
\]

We will show that the first term converges to \(\Omega_{1}\left(\tau, \tau^{\prime}\right) / n\), the second term to \(\Omega_{2}\left(\tau, \tau^{\prime}\right)\), and the remaining terms vanish at a rate faster than the leading term.

By the proof of Lemma 2(i), it follows for the first term that

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}\left(\tau^{\prime}\right)-\beta_{j}\left(\tau^{\prime}\right)\right)\right)^{\prime} \\
& =\mathbb{E}\left[\left(\Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{i, \tau}\left(\tilde{x}_{i j}, z_{i j}\right)\right)\left(\Sigma_{Z X j} \frac{1}{n} \sum_{i=1}^{n} \phi_{i, \tau^{\prime}}\left(\tilde{x}_{i j}, z_{i j}\right)\right)^{\prime}\right]+o_{p}\left(\frac{1}{m n}\right) \\
& =\frac{\Omega_{1}\left(\tau, \tau^{\prime}\right)}{n}+O_{p}\left(\frac{1}{m n}\right)
\end{aligned}
\]

uniformly in \(\tau\). For the second term, we consider each element of the \(L \times L\) matrix separately. For \(l, l^{\prime} \in\{1, \ldots, L\}\), we have

\[
\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} \alpha_{j}(\tau)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}} \alpha_{j}\left(\tau^{\prime}\right)\right)^{\prime}=\frac{1}{m} \sum_{j=1}^{m} \bar{z}_{j l} \bar{z}_{j l^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)
\]

If either \(l\) or \(l^{\prime}\) (or both) is in \(\left\{1, \ldots, L_{1}\right\}\), then \(\Omega_{2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)=0\) and \(\sum_{j=1}^{m} \bar{z}_{j l} \bar{z}_{j l^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)=0\). Thus, we need only to consider the case that both \(\bar{z}_{j l}\) and \(\bar{z}_{j l^{\prime}}\) are not equal to 0 uniformly across all groups. We apply Theorem 9.2 in Hansen (2022a). His condition (9.3) is satisfied by the boundedness of \(z_{i j}\) in Assumption \(5(\mathrm{i})\) and the uniform boundedness of the \(4+\varepsilon_{C}\) moment of \(\alpha_{j}(\tau)\) in Assumption 6(i). Condition (9.5) in Hansen (2022a) is satisfied by the assumption in equation (23). It follows that

\[
\sqrt{m}\left(\frac{1}{m} \sum_{j=1}^{m} \bar{z}_{j l} \bar{z}_{j l^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)-\Omega_{2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)\right) \underset{d}{\rightarrow} N\left(0, C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right)\right)
\]

Then, since the condition for Theorem 18.3 in Hansen (2022a) are satisfied, we have that \(\sup _{\tau, \tau^{\prime}}\left\|\frac{1}{m} \sum_{j=1}^{m} \bar{z}_{j l} \bar{z}_{j l^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)-\Omega_{2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)\right\|=O_{p}\left(\frac{\sqrt{C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right)}}{\sqrt{m}}\right)\). Let \(C_{l}\) denotes a uniform bound on \(\left|z_{i j l}\right|\). From the definition of \(C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right)\) we have

\[
C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right) \leq C_{l}^{2} C_{l^{\prime}}^{2} \operatorname{Var}\left(\alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)\right) \leq C_{l}^{2} C_{l^{\prime}}^{2} \mathbb{E}\left[\alpha_{j}(\tau)^{2} \alpha_{j}\left(\tau^{\prime}\right)^{2}\right] \leq C_{l}^{2} C_{l^{\prime}}^{2} \sqrt{\mathbb{E}\left[\alpha_{j}(\tau)^{4}\right] \mathbb{E}\left[\alpha_{j}\left(\tau^{\prime}\right)^{4}\right]}
\]

Finally, note that \(\frac{\mathbb{E}\left[\alpha_{j}(\tau)^{4}\right]}{\operatorname{Var}\left(\alpha_{j}(\tau)\right)^{2}}\) is bounded. It follows that \(\frac{1}{m} \sum_{j=1}^{m} \bar{z}_{j l} \bar{z}_{j l^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)=\Omega_{2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)+\) \(O_{p}\left(\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right) \operatorname{Var}\left(\alpha_{j}\left(\tau^{\prime}\right)\right)}}{\sqrt{m}}\right)=\Omega_{2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)+O_{p}\left(\frac{\sqrt{\Omega_{2, l l}(\tau, \tau) \Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime} \tau^{\prime}\right)}}{\sqrt{m}}\right)\).

For the third term, we also consider each element \(l, l^{\prime}\) of the matrix separately:

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right)= \\
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} z_{i j l} x_{i j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} \sum_{k=1}^{K} z_{i j l^{\prime}} x_{i j k}\left(\hat{\delta}_{k}\left(\tau^{\prime}\right)-\delta_{k}\left(\tau^{\prime}\right)\right)\right)
\end{aligned}
\]

We split \(\sum_{k=1}^{K} z_{i j l^{\prime}} x_{i j k}\left(\hat{\delta}_{k}\left(\tau^{\prime}\right)-\delta_{k}\left(\tau^{\prime}\right)\right)\) into the the individual-level and group-level variables. Since the estimation error of the coefficients on the individual-level variables is \(O_{p}(1 / \sqrt{m n})\) and\\
\(z_{i j l}\) and \(x_{i j k}\) are bounded, we obtain

\[
\sum_{k=1}^{K_{1}} \frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{1 i j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)=O_{p}\left(\frac{1}{\sqrt{m n}}\right)
\]

uniformly in \(\tau\). For the group-level variables, we obtain

\[
\sum_{k=K_{1}+1}^{K} \frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{2 j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)=\sum_{k=K_{1}+1}^{K} \bar{z}_{j l} x_{2 j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)
\]

It follows that \(\sum_{k=K_{1}+1}^{K} \frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)=0\) if \(\bar{z}_{j l}=0\) for all \(j=1, \ldots, m\), i.e. if \(l \in\left\{1, \ldots, L_{1}\right\}\). If \(l \in\left\{L_{1}+1, \ldots, L\right\}\), uniformly in \(\tau, \sum_{k=K_{1}+1}^{K} \frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)=\) \(O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}}\right)\). Since \(\Omega_{2, l l}(\tau)=\operatorname{Var}\left(\bar{z}_{j l} \alpha_{j}(\tau)\right)\), in both cases we can write

\[
\sum_{k=1}^{K} \frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{j k}\left(\hat{\delta}_{k}(\tau)-\delta_{k}(\tau)\right)=O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\Omega_{2, l l}(\tau)}}{\sqrt{m}}\right)
\]

uniformly in \(\tau\). Combining these results, we get uniformly in \(\tau, \tau^{\prime}\)

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right) \\
& \quad=O_{p}\left(\frac{\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m}\right)
\end{aligned}
\]

For the fourth term, similar arguments and the fact that \(\sup _{\tau}\left\|\beta_{j}(\tau)-\beta_{j}(\tau)\right\|=O_{p}\left(\frac{1}{\sqrt{n}}\right)\) imply that uniformly in \(\tau, \tau^{\prime}\)

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right) \\
& \quad=O_{p}\left(\frac{1}{\sqrt{m} n}+\frac{\sqrt{\Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{\sqrt{m n}}\right)
\end{aligned}
\]

Similarly, for the fifth term

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right) \\
& \quad=O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\Omega_{2, l l}(\tau)}}{\sqrt{m n}}\right)
\end{aligned}
\]

uniformly in \(\tau, \tau^{\prime}\). Thus, the sum of the fourth and fifth terms is

\[
O_{p}\left(\frac{1}{\sqrt{m} n}+\frac{\sqrt{\Omega_{2, l l}(\tau)}}{\sqrt{m} \sqrt{n}}+\frac{\sqrt{\Omega_{2, l^{\prime} l^{\prime}\left(\tau^{\prime}\right)}}}{\sqrt{m} \sqrt{n}}\right)=O_{p}\left(\frac{\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{\sqrt{m}}\right)
\]

uniformly in \(\tau\).

For the sixth term, we obtain, uniformly in \(\tau, \tau^{\prime}\)

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}} \alpha_{j}\left(\tau^{\prime}\right)\right) \\
&=\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}(\tau)-\beta_{j}(\tau)\right)\right) \bar{z}_{j l^{\prime}} \alpha_{j}\left(\tau^{\prime}\right) \\
&= O_{p}\left(\frac{\sqrt{\Omega_{2, l^{\prime} \prime^{\prime}\left(\tau^{\prime}\right)}}}{\sqrt{m n}}\right)
\end{aligned}
\]

and similarly, we can show that the seventh term is \(O_{p}\left(\frac{\sqrt{\Omega_{2, l l}(\tau)}}{\sqrt{m n}}\right)\).\\
For the eighth term, we obtain

\[
\begin{aligned}
& \frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}}^{\prime} \alpha_{j}\left(\tau^{\prime}\right)\right) \\
&=\frac{1}{m} \sum_{j=1}^{m}\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right) \bar{z}_{j l^{\prime}} \alpha_{j}\left(\tau^{\prime}\right) \\
&=O_{p}\left(\frac{\sqrt{\Omega_{2, l l}(\tau) \Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m}+\frac{\sqrt{\Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m \sqrt{n}}\right)
\end{aligned}
\]

and the ninth term is \(O_{p}\left(\frac{\sqrt{\Omega_{2, l}(\tau) \Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m}+\frac{\sqrt{\Omega_{m n, l^{\prime} \nu^{\prime}}\left(\tau^{\prime}\right)}}{m \sqrt{n}}\right)\) such that the sum of the eight and ninth term is \(O_{p}\left(\frac{\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m}\right)\), where both results are uniformly in \(\tau, \tau^{\prime}\).

Combining all the terms, we find that for each \(l, l^{\prime}\) entry

\[
\begin{aligned}
\hat{\Omega}\left(\tau, \tau^{\prime}\right) & =\frac{\Omega_{1, l l^{\prime}}\left(\tau, \tau^{\prime}\right)}{n}+\Omega_{2, l l^{\prime}}\left(\tau, \tau^{\prime}\right)+O_{p}\left(\frac{\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}}(\tau)}}{\sqrt{m}}\right) \\
& =\Omega_{m n, l l^{\prime}}\left(\tau, \tau^{\prime}\right)+o_{p}\left(\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}\right)
\end{aligned}
\]

uniformly in \(\tau, \tau^{\prime}\).

\section*{A.7.2 Proposition \(1^{\prime}\)}
When the coefficients of some individual-level variables converge at a slow rate, while those of others converge at a fast rate, the third term in the proof of Proposition 1 may not be \(O_{p}\left(\frac{\sqrt{\Omega_{m n, l l}(\tau) \Omega_{m n, l^{\prime} l^{\prime}\left(\tau^{\prime}\right)}}}{m}\right)\). The slowly converging coefficients can introduce an estimation error in the variance of the faster moments that diminishes more slowly than the true value. Proposition \(1^{\prime}\), stated below, provides a more general result that does not assume the coefficients of individual-level variables converge at the \(O_{p}(1 / \sqrt{m n})\) rate. Consequently, \(\hat{\Omega}\left(\tau, \tau^{\prime}\right)\) is consistent as long as all individual-level variable coefficients converge at the same rate. One example of this is the between estimator for individual-level variables. Another example is the 2SLS estimator\\
applied to the random effects model. In this case, the weighting matrix is full rank, and all coefficients converge at the rate of \(1 / \sqrt{m n}+\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)} / \sqrt{m}\).

Proposition 1'. Let assumptions 1-6 and \(7(c)\) hold. As \(m \rightarrow \infty\), for each \(l, l^{\prime} \in\{1, \ldots, L\}\) and uniformly in \(\tau, \tau^{\prime} \in \mathcal{T}^{2}\),

\[
m^{-1} \sum_{j=1}^{m} \mathbb{E}\left[\left(\bar{z}_{j l} \bar{z}_{j^{\prime}} \alpha_{j}(\tau) \alpha_{j}\left(\tau^{\prime}\right)-\Omega_{2 l l^{\prime}}\left(\tau, \tau^{\prime}\right)\right)^{2}\right] \rightarrow C_{l, l^{\prime}}\left(\tau, \tau^{\prime}\right)<\infty
\]

The estimator used to compute \(\hat{u}_{i j}(\tau)\) satisfies

\[
\hat{\delta}(\tau)-\delta(\tau)=O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{m}}\right)
\]

uniformly in \(\tau\). Then, for any \(l l^{\prime}\) entry of the \(\hat{\Omega}\left(\tau, \tau^{\prime}\right)\) matrix with \(l, l^{\prime} \in\{1, \ldots, L\}\) we have\\
\(\hat{\Omega}\left(\tau, \tau^{\prime}\right)=\Omega_{m n, l l^{\prime}}\left(\tau, \tau^{\prime}\right)+o_{p}\left(\frac{1}{n}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}\left(\tau^{\prime}\right)\right)}}{\sqrt{n}}+\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right) \operatorname{Var}\left(\alpha_{j}\left(\tau^{\prime}\right)\right)}\right)\)\\
Proof. The proof follows the same steps as the proof of Proposition 1, but requires some modifications each time \(\hat{\delta}(\tau)-\delta(\tau)\) is involved. This term is now \(O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}{\sqrt{m}}\right)\) instead of \(O_{p}\left(\frac{1}{\sqrt{m n}}+\frac{\Omega_{2}(\tau)}{\sqrt{m}}\right)\). For the third term, we obtain

\[
\begin{aligned}
\frac{1}{m} \sum_{j=1}^{m} & \left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l} x_{i j}^{\prime}(\hat{\delta}(\tau)-\delta(\tau))\right)\left(\frac{1}{n} \sum_{i=1}^{n} z_{i j l^{\prime}}^{\prime} x_{i j}^{\prime}\left(\hat{\delta}\left(\tau^{\prime}\right)-\delta\left(\tau^{\prime}\right)\right)\right) \\
& =O_{p}\left(\frac{1}{m n}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right) \operatorname{Var}\left(\alpha_{j}\left(\tau^{\prime}\right)\right)}}{m}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{n} m}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}\left(\tau^{\prime}\right)\right)}}{\sqrt{n} m}\right) .
\end{aligned}
\]

The sum of the fourth and fifth terms is now

\[
O_{p}\left(\frac{1}{n \sqrt{m}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}(\tau)\right)}}{\sqrt{n m}}+\frac{\sqrt{\operatorname{Var}\left(\alpha_{j}\left(\tau^{\prime}\right)\right)}}{\sqrt{n m}}\right) .
\]

The sum of the eight and ninth terms is now

\[
O_{p}\left(\frac{\sqrt{\Omega_{2, l l}(\tau)}}{m \sqrt{n}}+\frac{\sqrt{\Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m \sqrt{n}}+\frac{\sqrt{\operatorname{Var}\left(\alpha\left(\tau^{\prime}\right)\right) \Omega_{2, l l}(\tau)}}{m}+\frac{\sqrt{\operatorname{Var}(\alpha(\tau)) \Omega_{2, l^{\prime} l^{\prime}}\left(\tau^{\prime}\right)}}{m}\right)
\]

The result of the lemma follows.

\section*{A. 8 Proof of Proposition 2: Adaptive Inference}
Proof of Proposition 2. We prove the results for \(T=2\) as the proof trivially extends to \(T>2\). We consider the \(2 K \times 1\) coefficient vector \(\hat{\delta}\left(\tau, \tau^{\prime}\right)=\left(\hat{\delta}(\tau)^{\prime}, \hat{\delta}\left(\tau^{\prime}\right)^{\prime}\right)^{\prime}\) whose asymptotic covariance matrix is

\[
\Sigma_{m n}=\left(\begin{array}{cc}
\Sigma_{m n}(\tau) & \Sigma_{m n}\left(\tau, \tau^{\prime}\right) \\
\Sigma_{m n}\left(\tau, \tau^{\prime}\right) & \Sigma_{m n}\left(\tau^{\prime}\right)
\end{array}\right)
\]

This covariance matrix is estimated by

\[
\begin{aligned}
\hat{\Sigma} & =\frac{1}{m}\left(\begin{array}{cc}
\hat{G}(\tau) & 0 \\
0 & \hat{G}\left(\tau^{\prime}\right)
\end{array}\right)\left(\begin{array}{cc}
\hat{\Omega}(\tau, \tau) & \hat{\Omega}\left(\tau, \tau^{\prime}\right) \\
\hat{\Omega}\left(\tau^{\prime}, \tau\right) & \hat{\Omega}\left(\tau^{\prime}\right)
\end{array}\right)\left(\begin{array}{cc}
\hat{G}(\tau) & 0 \\
0 & \hat{G}\left(\tau^{\prime}\right)
\end{array}\right)^{\prime} \\
& =\frac{1}{m}\left(\begin{array}{cc}
\hat{G}(\tau) \hat{\Omega}(\tau) \hat{G}(\tau) & \hat{G}(\tau) \hat{\Omega}\left(\tau, \tau^{\prime}\right) \hat{G}\left(\tau^{\prime}\right) \\
\hat{G}\left(\tau^{\prime}\right) \hat{\Omega}\left(\tau^{\prime}, \tau\right) \hat{G}(\tau) & \hat{G}\left(\tau^{\prime}\right) \hat{\Omega}\left(\tau^{\prime}\right) \hat{G}\left(\tau^{\prime}\right)
\end{array}\right) .
\end{aligned}
\]

For each \(k, k^{\prime} \in\{1, \ldots, 2 K\}\), we will show that

\[
\hat{\Sigma}_{k k^{\prime}}=\Sigma_{m n, k k^{\prime}}+o_{p}\left(\sqrt{\Sigma_{m n, k k} \Sigma_{m n, k^{\prime} k^{\prime}}}\right) .
\]

To simplify the notation, we show this result for the \(K \times K\) top-left submatrix of \(\Sigma_{m n}\) such that we can drop the dependence on \(\tau\). The proof for the other parts is similar. Our analysis relies on two key ingredients:

First, by Proposition 1 , for any \(l l^{\prime}\) entry of the \(\hat{\Omega}\) matrix with \(l, l^{\prime} \in\{1, \ldots, L\}\), we have

\[
\hat{\Omega}_{l l^{\prime}}=\Omega_{m n, l l^{\prime}}+o_{p}\left(\sqrt{\Omega_{m n, l l} \Omega_{m n, l^{\prime l^{\prime}}}}\right)
\]

where \(\Omega\) can be split into four submatrices where the dimensions of the top-left part are \(L_{1} \times L_{1}\) and the bottom-right are \(L_{2} \times L_{2}\) :

\[
\Omega=\left(\begin{array}{ll}
\Omega_{m n, 11} & \Omega_{m n, 12} \\
\Omega_{m n, 21} & \Omega_{m n, 22}
\end{array}\right)
\]

such that \(\Omega_{m n, 11}, \Omega_{m n, 12}\), and \(\Omega_{m n, 21}\) are \(O_{p}\left(\frac{1}{n}\right)\) while \(\Omega_{m n, 22}=O_{p}\left(\frac{1}{n a_{n}}\right)\).\\
Second, by Lemma 5,

\[
\hat{G}=\left(\begin{array}{ll}
G_{m n, 11} & G_{m n, 12} \\
G_{m n 21} & G_{m n, 22}
\end{array}\right)+\left(\begin{array}{cc}
o_{p}(1) & o_{p}\left(\sqrt{a_{n}}\right) \\
o_{p}\left(1 / \sqrt{a_{n}}\right) & o_{p}(1)
\end{array}\right),
\]

where \(G_{m n, 12}=O_{p}\left(a_{n}\right)\) and the other elements of \(G_{m n}\) are \(O_{p}(1)\).\\
With this notation,

\[
\begin{aligned}
\Sigma_{m n} & =\frac{1}{m}\left(\begin{array}{ll}
G_{m n, 11} & G_{m n, 12} \\
G_{m n, 21} & G_{m n, 22}
\end{array}\right)\left(\begin{array}{ll}
\Omega_{m n, 11} & \Omega_{m n, 12} \\
\Omega_{m n, 21} & \Omega_{m n, 22}
\end{array}\right)\left(\begin{array}{ll}
G_{m n, 11}^{\prime} & G_{m n, 21}^{\prime} \\
G_{m n, 12}^{\prime} & G_{m n, 22}^{\prime}
\end{array}\right) \\
& =\left(\begin{array}{ll}
\Sigma_{m n, 11} & \Sigma_{m n, 12} \\
\Sigma_{m n, 21} & \Sigma_{m n, 22}
\end{array}\right),
\end{aligned}
\]

where

\[
\begin{aligned}
& \Sigma_{m n, 11}=\frac{1}{m}\left(G_{m n, 11} \Omega_{m n, 11} G_{m n, 11}^{\prime}+G_{m n, 11} \Omega_{m n, 12} G_{m n, 12}^{\prime}+G_{m n, 12} \Omega_{m n, 21} G_{m n, 11}^{\prime}+G_{m n, 12} \Omega_{m n, 22} G_{m n, 12}^{\prime}\right), \\
& \Sigma_{m n, 12}=\frac{1}{m}\left(G_{m n, 11} \Omega_{m n, 11} G_{m n, 21}^{\prime}+G_{m n, 11} \Omega_{m n, 12} G_{m n, 22}^{\prime}+G_{m n, 12} \Omega_{m n, 21} G_{m n, 21}^{\prime}+G_{m n, 12} \Omega_{m n, 22} G_{m n, 22}^{\prime}\right), \\
& \Sigma_{m n, 21}=\frac{1}{m}\left(G_{m n, 21} \Omega_{m n, 11} G_{m n, 11}^{\prime}+G_{m n, 21} \Omega_{m n, 12} G_{m n, 12}^{\prime}+G_{m n, 22} \Omega_{m n, 21} G_{m n, 11}^{\prime}+G_{m n, 22} \Omega_{m n, 22} G_{m n, 12}^{\prime}\right), \\
& \Sigma_{m n, 22}=\frac{1}{m}\left(G_{m n, 21} \Omega_{m n, 11} G_{m n, 21}^{\prime}+G_{m n, 21} \Omega_{m n, 12} G_{m n, 22}^{\prime}+G_{m n, 22} \Omega_{m n, 21} G_{m n, 21}^{\prime}+G_{m n, 22} \Omega_{m n, 22} G_{m n, 22}^{\prime}\right) .
\end{aligned}
\]

It follows that \(\Sigma_{m n, 11}=O_{p}\left(\frac{1}{m n}\right), \Sigma_{m n, 12}=O_{p}\left(\frac{1}{m n}\right), \Sigma_{m n, 21}=O_{p}\left(\frac{1}{m n}\right)\), and \(\Sigma_{m n, 22}=\) \(O_{p}\left(\frac{1}{m n a_{n}}\right)\).

We consider the estimation error of these four terms separately, each of them being composed of four parts. For the first term,

\[
\begin{aligned}
\hat{G}_{11} \hat{\Omega}_{11} \hat{G}_{11}^{\prime} & =\left(G_{m n, 11}+o_{p}(1)\right)\left(\Omega_{m n, 11}+o_{p}\left(\Omega_{m n, 11}\right)\right)\left(G_{m n, 11}^{\prime}+o_{p}(1)\right) \\
& =G_{m n, 11} \Omega_{m n, 11} G_{m n, 11}^{\prime}+o_{p}\left(\Omega_{m n, 11}\right) \\
& =G_{m n, 11} \Omega_{m n, 11} G_{m n, 11}^{\prime}+o_{p}\left(\frac{1}{n}\right), \\
\hat{G}_{11} \hat{\Omega}_{12} \hat{G}_{12}^{\prime} & =\left(G_{m n, 11}+o_{p}(1)\right)\left(\Omega_{m n, 12}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right)\right)\left(G_{m n, 12}^{\prime}+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
& =G_{m n, 11} \Omega_{m n, 12} G_{m n 12}^{\prime}+o_{p}\left(\Omega_{m n, 12}\right)+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22} a_{n}}\right) \\
& =G_{m n, 11} \Omega_{m n, 12} G_{m n, 12}^{\prime}+o_{p}\left(\frac{1}{n}\right), \\
\hat{G}_{12} \hat{\Omega}_{21} \hat{G}_{11}^{\prime} & =\left(\hat{G}_{11} \hat{\Omega}_{12} \hat{G}_{12}^{\prime}\right)^{\prime} \\
& =G_{m n, 12} \Omega_{m n, 21} G_{m n, 11}^{\prime}+o_{p}\left(\frac{1}{n}\right), \\
\hat{G}_{12} \hat{\Omega}_{22} \hat{G}_{12}^{\prime} & =\left(G_{m n, 12}+o_{p}\left(\sqrt{a_{n}}\right)\right)\left(\Omega_{m n, 22}+o_{p}\left(\Omega_{m n, 22}\right)\right)\left(G_{m n, 12}^{\prime}+o_{p}\left(\sqrt{a_{n}}\right)\right) \\
& =G_{m n, 12} \Omega_{m n, 22} G_{m n, 12}^{\prime}+o_{p}\left(a_{n} \Omega_{m n, 22}\right) \\
& =G_{m n, 12} \Omega_{m n, 22} G_{m n, 12}^{\prime}+o_{p}\left(\frac{1}{n}\right) .
\end{aligned}
\]

It follows that \(\hat{\Sigma}_{11}=\Sigma_{m n, 11}+o_{p}\left(\frac{1}{m n}\right)=\Sigma_{m n, 11}+o_{p}\left(\Sigma_{m n, 11}\right)\).\\
For the second part,

\[
\begin{aligned}
\hat{G}_{11} \hat{\Omega}_{11} \hat{G}_{21}^{\prime} & =\left(G_{m n, 11}+o_{p}(1)\right)\left(\Omega_{m n, 11}+o_{p}\left(\Omega_{m n, 11}\right)\right)\left(G_{m n, 21}^{\prime}+o_{p}\left(1 / \sqrt{a_{n}}\right)\right) \\
& =G_{m n, 11} \Omega_{m n, 11} G_{m n, 21}^{\prime}+o_{p}\left(\Omega_{m n, 11} / \sqrt{a_{n}}\right) \\
& =G_{m n, 11} \Omega_{m n, 11} G_{m n, 21}^{\prime}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right), \\
\hat{G}_{11} \hat{\Omega}_{12} \hat{G}_{22}^{\prime} & =\left(G_{m n, 11}+o_{p}(1)\right)\left(\Omega_{m n, 12}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right)\right)\left(G_{m n, 22}^{\prime}+o_{p}(1)\right) \\
& =G_{m n, 11} \Omega_{m n, 12} G_{m n, 22}^{\prime}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right), \\
\hat{G}_{12} \hat{\Omega}_{21} \hat{G}_{21}^{\prime} & =\left(G_{m n, 12}+o_{p}\left(\sqrt{a_{n}}\right)\right)\left(\Omega_{m n, 21}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right)\left(G_{m n, 21}^{\prime}+o_{p}\left(1 / \sqrt{a_{n}}\right)\right)^{\prime}\right. \\
& =G_{m n, 12} \Omega_{m n, 21} G_{m n, 21}^{\prime}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right), \\
\hat{G}_{12} \hat{\Omega}_{22} \hat{G}_{22}^{\prime} & =\left(G_{m n, 12}+o_{p}\left(\sqrt{a_{n}}\right)\right)\left(\Omega_{m n, 22}+o_{p}\left(\Omega_{m n, 22}\right)\right)\left(G_{m n, 22}^{\prime}+o_{p}(1)\right) \\
& =G_{m n, 12} \Omega_{m n, 22} G_{m n, 22}^{\prime}+o_{p}\left(\sqrt{a_{n}} \Omega_{m n, 22}\right) \\
& =G_{m n, 12} \Omega_{m n, 22} G_{m n, 22}^{\prime}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right) .
\end{aligned}
\]

It follows that \(\hat{\Sigma}_{12}=\Sigma_{m n, 12}+o_{p}\left(m^{-1} \sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right)=\Sigma_{m n, 12}+o_{p}\left(\sqrt{\Sigma_{m n, 11} \Sigma_{m n, 22}}\right)\). The third part, \(\hat{\Sigma}_{21}\), is the transpose of \(\hat{\Sigma}_{12}\).

For the fourth part,

\[
\begin{aligned}
\hat{G}_{21} \hat{\Omega}_{11} \hat{G}_{21}^{\prime} & =\left(G_{m n, 21}+o_{p}\left(1 / \sqrt{a_{n}}\right)\right)\left(\Omega_{m n, 11}+o_{p}\left(\Omega_{m n, 11}\right)\right)\left(G_{m n, 21}^{\prime}+o_{p}\left(1 / \sqrt{a_{n}}\right)\right) \\
& =G_{m n, 21} \Omega_{m n, 11} G_{m n, 21}^{\prime}+o_{p}\left(\Omega_{m n, 11} / a_{n}\right) \\
& =G_{m n, 21} \Omega_{m n, 11} G_{m n, 21}^{\prime}+o_{p}\left(\Omega_{m n, 22}\right), \\
\hat{G}_{21} \hat{\Omega}_{12} \hat{G}_{22}^{\prime} & =\left(G_{m n, 21}+o_{p}\left(1 / \sqrt{a_{n}}\right)\right)\left(\Omega_{m n, 12}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right)\right)\left(G_{m n, 22}^{\prime}+o_{p}(1)\right) \\
& =G_{m n, 21} \Omega_{m n, 12} G_{m n, 22}^{\prime}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22} / a_{n}}\right) \\
& =G_{m n, 21} \Omega_{m n, 12} G_{m n, 22}^{\prime}+o_{p}\left(\Omega_{m n, 22}\right), \\
\hat{G}_{22} \hat{\Omega}_{21} \hat{G}_{21}^{\prime} & =\left(G_{m n, 22}+o_{p}(1)\right) \Omega_{m n, 21}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22}}\right)\left(G_{m n, 21}^{\prime}+o_{p}\left(1 / \sqrt{a_{n}}\right)\right)^{\prime} \\
& =G_{m n, 22} \Omega_{m n, 21} G_{m n, 21}^{\prime}+o_{p}\left(\sqrt{\Omega_{m n, 11} \Omega_{m n, 22} / a_{n}}\right) \\
& =G_{m n, 22} \Omega_{m n, 21} G_{m n, 21}^{\prime}+o_{p}\left(\Omega_{m n, 22}\right) \\
\hat{G}_{22} \hat{\Omega}_{22} \hat{G}_{22}^{\prime} & =\left(G_{m n, 22}+o_{p}(1)\right)\left(\Omega_{m n, 22}+o_{p}\left(\Omega_{m n, 22}\right)\right)\left(G_{m n, 22}^{\prime}+o_{p}(1)\right) \\
& =G_{m n, 22} \Omega_{m n, 22} G_{m n, 22}^{\prime}+o_{p}\left(\Omega_{m n, 22}\right) .
\end{aligned}
\]

It follows that \(\hat{\Sigma}_{22}=\Sigma_{m n, 22}+o_{p}\left(m^{-1} \Omega_{m n, 22}\right)=\Sigma_{m n, 22}+o_{p}\left(\Sigma_{m n, 22}\right)\). The results for these four submatrices imply that

\[
\hat{\Sigma}_{k k^{\prime}}=\Sigma_{m n, k k^{\prime}}+o_{p}\left(\sqrt{\Sigma_{m n, k k} \Sigma_{m n, k^{\prime} k^{\prime}}}\right),
\]

which implies that

\[
\eta^{\prime} \hat{\Sigma} \eta=\eta^{\prime} \Sigma_{m n} \eta+o_{p}\left(\eta^{\prime} \Sigma_{m n} \eta\right) .
\]

The proposition follows from Theorem 3.

\section*{A. 9 Proof of Proposition 3: Properties of the weighting matrix}
Proof. Since this proof focuses on a case where \(\tau=\tau^{\prime}\), we suppress the dependency on \(\tau\) for simplicity. We partition the \(\Omega\) matrix as follows:

\[
\Omega_{m n}=\left(\begin{array}{ll}
\Omega_{m n, 11} & \Omega_{m n, 12} \\
\Omega_{m n, 21} & \Omega_{m n, 22}
\end{array}\right)
\]

where \(\Omega_{m n, 11}\) is \(L_{1} \times L_{1}, \Omega_{m n, 12}\) is \(L_{1} \times L_{2}, \Omega_{m n, 21}\) is \(L_{2} \times L_{1}\) and \(\Omega_{m n, 22}\) is \(L_{2} \times L_{2}\).\\
From Proposition 1 we have, uniformly over \(\tau\)

\[
\hat{\Omega}=\left(\begin{array}{cc}
\frac{\Omega_{1,11}^{n}}{\Omega_{1,21}} & \frac{\Omega_{1,12}^{n}}{n} \\
\frac{\Omega_{m n, 22}}{n} & \Omega_{m}
\end{array}\right)+\left(\begin{array}{cc}
o_{p}\left(\left\|\Omega_{m n, 11}\right\|\right) & +o_{p}\left(\frac{\sqrt{\| \Omega_{m n, 22 \mid}}}{\sqrt{n}}\right) \\
o_{p}\left(\frac{\sqrt{\| \Omega_{m n, 22 \|}}}{\sqrt{n}}\right) & o_{p}\left(\left\|\Omega_{m n, 22}\right\|\right)
\end{array}\right) .
\]

By the inverse of a partitioned matrix

\[
\hat{W}=\frac{1}{n} \cdot\left(\begin{array}{ll}
\hat{\Omega}_{11} & \hat{\Omega}_{12} \\
\hat{\Omega}_{21} & \hat{\Omega}_{22}
\end{array}\right)^{-1}=\left(\begin{array}{cc}
\hat{\Psi} & -\hat{\Psi} \hat{\Omega}_{12} \hat{\Omega}_{22}^{-1} \\
-\hat{\Omega}_{22}^{-1} \hat{\Omega}_{21} \hat{\Psi} & n^{-1} \cdot \hat{\Omega}_{22}^{-1}+\hat{\Omega}_{22}^{-1} \hat{\Omega}_{21} \Psi \hat{\Omega}_{12} \hat{\Omega}_{22}^{-1}
\end{array}\right)
\]

where \(\hat{\Psi}=\left(n \cdot \hat{\Omega}_{11}-n \cdot \hat{\Omega}_{12} \hat{\Omega}_{22}^{-1} \hat{\Omega}_{21}\right)^{-1}\). To address the possibility that \(\Omega_{m n, 22}\) could converge to zero, we rescale both elements in \(\hat{\Omega}_{12} \hat{\Omega}_{22}^{-1}\), which ensures that both elements are well behaved, and show that the expression is self-normalizing. Hence, uniformly in \(\tau\),

\[
\begin{aligned}
\hat{\Omega}_{12} \hat{\Omega}_{22}^{-1} & =\left(\left\|\Omega_{m n, 22}\right\|^{-1} \hat{\Omega}_{12}\right)\left(\left\|\Omega_{m n, 22}\right\|^{-1} \hat{\Omega}_{m n, 22}\right)^{-1} \\
& =\Omega_{m n, 12} \Omega_{m n, 22}^{-1}+o_{p}\left(\sqrt{n^{-1}\left\|\Omega_{m n, 22}\right\|^{-1}}\right) \\
& =n^{-1} \Omega_{1,12} \Omega_{m n, 22}^{-1}+o_{p}\left(\sqrt{a_{n}}\right)
\end{aligned}
\]

Note that the second line follows as

\[
\begin{aligned}
& \left\|\Omega_{m n, 22}\right\|^{-1} \hat{\Omega}_{12}=n^{-1}\left\|\Omega_{m n, 22}\right\|^{-1} \Omega_{1,12}+o_{p}\left(\sqrt{n^{-1}\left\|\Omega_{m n, 22}\right\|^{-1}}\right) \\
& \left\|\Omega_{m n, 22}\right\|^{-1} \hat{\Omega}_{22}=\left\|\Omega_{m n, 22}\right\|^{-1} \Omega_{m n, 22}+o_{p}(1)
\end{aligned}
\]

and \(\left\|\Omega_{m n, 22}\right\|^{-1} \hat{\Omega}_{22}\) is invertible. It then follows that, uniformly in \(\tau\),

\[
\begin{aligned}
\hat{\Omega}_{11}-\hat{\Omega}_{12} \hat{\Omega}_{22}^{-1} \hat{\Omega}_{21}= & \frac{\Omega_{1,11}}{n}+o_{p}\left(\frac{1}{n}\right)-\left(n^{-1} \Omega_{m n, 12} \Omega_{m n, 22}^{-1}+o_{p}\left(\sqrt{n^{-1}\left\|\Omega_{m n, 22}\right\|^{-1}}\right)\right) \\
& \left(\frac{\Omega_{1,21}}{n}+o_{p}\left(\frac{\sqrt{\left\|\Omega_{m n, 22}\right\|}}{\sqrt{n}}\right)\right) \\
& =\frac{\Omega_{1,11}}{n}-\frac{\Omega_{1,12}}{n} \Omega_{m n, 22}^{-1} \frac{\Omega_{1,21}}{n}+o_{p}\left(\frac{1}{n}\right) .
\end{aligned}
\]

Hence, uniformly in \(\tau\),

\[
\hat{\Psi}^{-1}=n \cdot \hat{\Omega}_{11}-n \cdot \hat{\Omega}_{12} \hat{\Omega}_{22}^{-1} \hat{\Omega}_{21}=\Omega_{1,11}-\frac{1}{n} \Omega_{1,12} \Omega_{m n, 22}^{-1} \Omega_{1,21}+o_{p}(1)
\]

so that for the submatrix in the upper left of equation (80), we have

\[
\sup _{\tau}\|\hat{\Psi}-\Psi\|=o_{p}(1)
\]

where \(\Psi\) is strictly positive definite.\\
For the upper right term of \(\hat{W}\), we have uniformly in \(\tau\)

\[
-\hat{\Psi} \hat{\Omega}_{12} \hat{\Omega}_{22}^{-1}=-\Psi \frac{\Omega_{1,12}}{n} \Omega_{m n, 22}^{-1}+o_{p}\left(\sqrt{a_{n}}\right)
\]

where \(\Psi \frac{\Omega_{1,12}}{n} \Omega_{m n, 22}^{-1}=O_{p}\left(a_{n}\right)\).\\
Similarly,

\[
\hat{\Omega}_{22}^{-1} \hat{\Omega}_{21} \hat{\Psi}=\Omega_{m n, 22}^{-1} \frac{\Omega_{1,21}}{n} \Psi+o_{p}\left(\sqrt{a_{n}}\right)
\]

Finally, for the lower right term, using an appropriate normalization, we find that uniformly in \(\tau\)

\[
n^{-1} \hat{\Omega}_{22}^{-1}=n^{-1} \Omega_{m n, 22}^{-1}+o_{p}\left(\frac{\left\|\Omega_{m n, 22}\right\|^{-1}}{n}\right)=n^{-1} \Omega_{m n, 22}^{-1}+o_{p}\left(a_{n}\right)
\]

and combining this with the results for the other components for the lower right submatrix, we obtain

\[
\sup _{\tau}\left\|n^{-1} \cdot \hat{\Omega}_{22}^{-1}+\hat{\Omega}_{22}^{-1} \hat{\Omega}_{21} \Psi \hat{\Omega}_{12} \hat{\Omega}_{22}^{-1}-n^{-1} \cdot \Omega_{m n, 22}^{-1}+\Omega_{m n, 22}^{-1} \frac{\Omega_{1,21}}{n} \Psi \frac{\Omega_{1,12}}{n} \Omega_{m n, 22}^{-1}\right\|=o_{p}\left(a_{n}\right)
\]

Hence, uniformly in \(\tau \in \mathcal{T}\),

\[
\hat{W}(\tau)=\left(\begin{array}{cc}
W_{11}(\tau) & a_{n}(\tau) W_{12}(\tau) \\
a_{n}(\tau) W_{21}(\tau) & a_{n}(\tau) W_{22}(\tau)
\end{array}\right)+\left(\begin{array}{cc}
o_{p}(1) & o_{p}\left(\sqrt{a_{n}(\tau)}\right) \\
o_{p}\left(\sqrt{a_{n}(\tau)}\right) & o_{p}\left(a_{n}(\tau)\right)
\end{array}\right) .
\]

\section*{A. 10 Proof of Proposition 4: Overidentification Test}
Proof of Proposition 4. We prove this result for \(T=1\) as the proof trivially extends to \(T>1\). We suppress the dependency on \(\tau\) for simplicity as this proof focuses on a case where \(\tau=\tau^{\prime}\). First, we want to rewrite the J-statistics in a way that accounts for the different convergence rates of the moment conditions. Let \(d_{\Omega, m n}=I_{L} \cdot \operatorname{diag}\left(\Omega_{m n}\right)\). Then we can write

\[
\begin{aligned}
J(\hat{\delta}) & =m \bar{g}_{m n}(\hat{\delta})^{\prime} \hat{\Omega}^{-1} \bar{g}_{m n}(\hat{\delta}) \\
& =\left(d_{\Omega, m n}^{-1 / 2} \sqrt{m} \bar{g}_{m n}(\hat{\delta})\right)^{\prime}\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]^{-1}\left(d_{\Omega, m n}^{-1 / 2} \sqrt{m} \bar{g}_{m n}(\hat{\delta}) .\right)
\end{aligned}
\]

Second, we want to show that for some matrix \(\hat{B}, \bar{g}_{m n}(\hat{\delta})=\hat{B} \bar{g}_{m n}(\delta)\). Recall that \(\hat{Y}_{j}=\) \(X_{j} \delta+\alpha_{j}+\tilde{X}_{j}\left(\hat{\beta}_{j}-\beta_{j}\right)\). Hence, we can write

\[
\begin{aligned}
Z_{j}^{\prime} \hat{Y}_{j} & =Z_{j}^{\prime} X_{j} \delta+Z_{j}^{\prime} \alpha_{j}+Z_{j}^{\prime} \tilde{X}_{j}\left(\hat{\beta}_{j}-\beta\right) \\
S_{Z \hat{Y}} & =S_{Z X} \delta+\frac{1}{m} \sum_{j=1}^{m} \frac{1}{n} \sum_{i=1}^{n} z_{i j} \alpha_{j}+\frac{1}{m} \sum_{j=1}^{m} \frac{1}{n} \sum_{i=1}^{n} z_{i j} \tilde{x}_{i j}^{\prime}\left(\hat{\beta}_{j}-\beta_{j}\right) \\
& =S_{Z X} \delta+\bar{g}_{m n}(\delta)
\end{aligned}
\]

Then, note that

\[
\begin{aligned}
\bar{g}_{m n}(\hat{\delta}) & =\frac{1}{m} \sum_{j=1}^{m} \frac{1}{n} \sum_{i=1}^{n} z_{i j}\left(\hat{y}_{i j}-x_{i j}^{\prime} \hat{\delta}\right) \\
& =S_{Z \hat{Y}}-S_{Z X} \hat{\delta} \\
& =S_{Z \hat{Y}}-S_{Z X}\left(S_{Z X} \hat{\Omega}^{-1} S_{Z X}\right)^{-1} S_{Z X} \hat{\Omega}^{-1} S_{Z \hat{Y}}=\hat{B} S_{Z \hat{Y}}
\end{aligned}
\]

where \(\hat{B}=\left(I_{L}-S_{Z X}\left(S_{Z X}^{\prime} \hat{\Omega}^{-1} S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{\Omega}^{-1}\right)\).\\
Thus,

\[
\begin{aligned}
\bar{g}_{m n}(\hat{\delta}) & =\hat{B} S_{Z \hat{Y}} \\
& =\left(I_{L}-S_{Z X}\left(S_{Z X}^{\prime} \hat{\Omega}^{-1} S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{\Omega}^{-1}\right)\left(S_{Z X} \delta+\bar{g}_{m n}(\delta)\right) \\
& =\hat{B} \bar{g}_{m n}(\delta) .
\end{aligned}
\]

Since \(\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]\) is positive definite there exist a matrix \(\hat{Q}\) such that

\[
\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]^{-1}=\hat{Q}^{\prime} \hat{Q}
\]

We define \(\hat{A}=\hat{Q} d_{\Omega, m n}^{-1 / 2} S_{Z X}^{\prime}\) and \(\hat{M}=I_{L}-\hat{A}\left(\hat{A}^{\prime} \hat{A}\right)^{-1} \hat{A}^{\prime}\).\\
In this third part, we show that

\[
\hat{B}^{\prime} d_{\Omega, m n}^{-1 / 2}\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]^{-1} d_{\Omega, m n}^{-1 / 2} \hat{B}=d_{\Omega, m n}^{-1 / 2} \hat{Q}^{\prime} \hat{M} \hat{Q} d_{\Omega, m n}^{-1 / 2},
\]

where \(\hat{B}^{\prime} d_{\Omega, m n}^{-1 / 2}\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]^{-1} d_{\Omega, m n}^{-1 / 2} \hat{B}=\hat{B}^{\prime} d_{\Omega, m n}^{-1 / 2} \hat{Q}^{\prime} \hat{Q} d_{\Omega, m n}^{-1 / 2} \hat{B}\). Note that

\[
\begin{aligned}
\hat{Q} d_{\Omega, m n}^{-1 / 2} \hat{B} & =\hat{Q} d_{\Omega, m n}^{-1 / 2}\left(I_{L}-S_{Z X}\left(S_{Z X}^{\prime} \hat{\Omega}^{-1} S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{\Omega}^{-1}\right) \\
& =\left(\hat{Q} d_{\Omega, m n}^{-1 / 2}-\hat{Q} d_{\Omega, m n}^{-1 / 2} S_{Z X}\left(S_{Z X}^{\prime} \hat{\Omega}^{-1} S_{Z X}\right)^{-1} S_{Z X}^{\prime} \hat{\Omega}^{-1}\right) \\
& =\left(\hat{Q} d_{\Omega, m n}^{-1 / 2}-\hat{Q} d_{\Omega, m n}^{-1 / 2} S_{Z X}\left(S_{Z X}^{\prime} d_{\Omega, m n}^{-1 / 2} \hat{Q}^{\prime} \hat{Q} d_{\Omega, m n}^{-1 / 2} S_{Z X}\right)^{-1} S_{Z X}^{\prime} d_{\Omega, m n}^{-1 / 2} \hat{Q}^{\prime} \hat{Q} d_{\Omega, m n}^{-1 / 2}\right) \\
& =\left(I_{L}-\hat{A}\left(\hat{A}^{\prime} \hat{A}\right)^{-1} \hat{A}^{\prime}\right) \hat{Q} d_{\Omega, m n}^{-1 / 2} \\
& =\hat{M} \hat{Q} d_{\Omega, m n}^{-1 / 2} .
\end{aligned}
\]

Where the third line uses \(d_{\Omega, m n}^{1 / 2} \hat{\Omega}^{-1} d_{\Omega, m n}^{1 / 2}=\hat{Q}^{\prime} \hat{Q}\) and in the last two lines, we use the definitions of \(\hat{A}\) and \(\hat{M}\).\\
\(\hat{M}\) is symmetric and idempotent. Thus

\[
\begin{aligned}
\hat{B}^{\prime} d_{\Omega, m n}^{-1 / 2} \hat{\Omega}^{-1} d_{\Omega, m n}^{-1 / 2} \hat{B} & =\hat{B}^{\prime} d_{\Omega, m n}^{-1 / 2} \hat{Q}^{\prime} \hat{Q} d_{\Omega, m n}^{-1 / 2} \hat{B} \\
& =\left(\hat{Q} d_{\Omega, m n}^{-1 / 2} \hat{B}\right)^{\prime} \hat{Q} d_{\Omega, m n}^{-1 / 2} \hat{B} \\
& =\left(\hat{M} \hat{Q} d_{\Omega, m n}^{-1 / 2}\right)^{\prime} \hat{M} \hat{Q} d_{\Omega, m n}^{-1 / 2} \\
& =d_{\Omega, m n}^{-1 / 2} \hat{Q^{\prime}} \hat{M} \hat{Q} d_{\Omega, m n}^{-1 / 2} .
\end{aligned}
\]

The rank of \(\hat{M}\) is the trace of \(\hat{M}\), which is \(L-K\).\\
Since \(d_{\Omega, m n}^{-1 / 2} \Omega_{m n} d_{\Omega, m n}^{-1 / 2}\) is positive definite, there exist a matrix \(Q_{m n}\) such that

\[
Q_{m n}^{\prime} Q_{m n}=\left[d_{\Omega, m n}^{-1 / 2} \Omega_{m n} d_{\Omega, m n}^{-1 / 2}\right]^{-1}
\]

It is easy to show that

\[
d_{\Omega}^{1 / 2} \hat{\Omega}^{-1} d_{\Omega}^{1 / 2}=d_{\Omega}^{1 / 2} \Omega_{m n}^{-1} d_{\Omega}^{1 / 2}+o_{p}(1),
\]

where

\[
\left(d_{\Omega}^{-1 / 2} \Omega_{m n} d_{\Omega}^{-1 / 2}\right)^{-1}=\left(\begin{array}{cc}
1 & \frac{\Omega_{1,12}}{\sqrt{n} \sqrt{\Omega_{1,11} \Omega_{22}}} \\
\frac{\Omega_{1,21}}{\sqrt{n} \sqrt{\Omega_{22} \Omega_{1,11}}} & 1
\end{array}\right) .
\]

It then follows directly that \(\hat{Q}=Q_{m n}+o_{p}(1)\).\\
Further, we have that

\[
d_{\Omega, m n}^{-1 / 2} \sqrt{m} \bar{g}_{m n}(\delta) \xrightarrow{d} N\left(0, d_{\Omega, m n}^{-1 / 2} \Omega_{m n} d_{\Omega, m n}^{-1 / 2}\right) .
\]

Define \(\hat{v}_{m n}=\sqrt{m} \hat{Q} d_{\Omega, m n}^{-1 / 2} \bar{g}_{m n}(\delta)\) and note that

\[
\hat{v}_{m n} \xrightarrow{d} N\left(0, Q_{m n} d_{\Omega, m n}^{-1 / 2} \Omega_{m n} d_{\Omega, m n}^{-1 / 2} Q_{m n}^{\prime}\right)=N\left(0, Q_{m n}\left(Q_{m n}^{\prime} Q_{m n}\right)^{-1} Q_{m n}^{\prime}\right)=N\left(0, I_{L}\right) .
\]

Now we can come back to our test statistic:

\[
\begin{aligned}
J(\hat{\delta}) & =m \bar{g}_{m n}(\hat{\delta})^{\prime} \hat{\Omega}^{-1} \bar{g}_{m n}(\hat{\delta}) \\
& =\left(\sqrt{m} d_{\Omega, m n}^{-1 / 2} \bar{g}_{m n}(\hat{\delta})\right)^{\prime}\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]^{-1}\left(\sqrt{m} d_{\Omega, m n}^{-1 / 2} \bar{g}_{m n}(\hat{\delta})\right) \\
& =\left(\sqrt{m} d_{\Omega, m n}^{-1 / 2} \hat{B} \bar{g}_{m n}(\delta)\right)^{\prime}\left[d_{\Omega, m n}^{-1 / 2} \hat{\Omega} d_{\Omega, m n}^{-1 / 2}\right]^{-1}\left(\sqrt{m} d_{\Omega, m n}^{-1 / 2} \hat{B} \bar{g}_{m n}(\delta)\right) \\
& =\left(\sqrt{m} \bar{g}_{m n}(\delta)\right)^{\prime} \hat{B}^{\prime} \hat{\Omega}^{-1} \hat{B}\left(\sqrt{m} \bar{g}_{m n}(\delta)\right) \\
& =\left(\sqrt{m} \bar{g}_{m n}(\delta)\right)^{\prime} d_{\Omega, m n}^{-1 / 2} \hat{Q}^{\prime} \hat{M} \hat{Q} d_{\Omega, m n}^{-1 / 2}\left(\sqrt{m} \bar{g}_{m n}(\delta)\right) \\
& =\left(\sqrt{m} \hat{Q} d_{\Omega, m n}^{-1 / 2} \bar{g}_{m n}(\delta)\right)^{\prime} \hat{M}\left(\sqrt{m} \hat{Q} d_{\Omega, m n}^{-1 / 2} \bar{g}_{m n}(\delta)\right) \\
& =\hat{v}_{m n}^{\prime} \hat{M} \hat{v}_{m n} .
\end{aligned}
\]

Since \(\hat{M}\) is idempotent with rank \(L-K\), it immediately follows that

\[
J(\hat{\delta}) \xrightarrow{d} \chi_{L-K}^{2} .
\]

\section*{B Least Squares Panel Data Models}
\section*{B. 1 Formal results}
This section complements subsection 2.3 by discussing more in detail the relationship between the least squares estimator and the minimum distance approach. Throughout the section, we define the \(n \times\left(K_{1}+1\right)\) matrix of first-stage regressors \(\tilde{X}_{1 j}=\left(\tilde{x}_{1 j}, \tilde{x}_{2 j}, \ldots, \tilde{x}_{n j}\right)^{\prime}\), and the \(m n \times K_{1}\) matrix of individual-level regressors \(X_{1}=\left(X_{1 j}^{\prime}, \ldots, X_{1 m}^{\prime}\right)^{\prime}\). Further, we use the matrices \(P_{j}=\)\\
\(l\left(l^{\prime} l\right)^{-1} l^{\prime}\) and \(Q_{j}=I_{j}-P_{j}\), where \(l\) is a \(n \times 1\) vector of ones. Thus, \(P_{j} X_{j}=\bar{X}_{j}\) and \(Q_{j} X_{1 j}=\dot{X}_{1 j}\). We consider a linear version of our estimator, where OLS instead of quantile regression is used in the first stage and we focus on model (10). In this section, we show that mean models can be estimated using a two-step procedure. Notation is the same as in the paper, except that the fitted values are computed using an OLS regression. More precisely, the vector of fitted values of group \(j\) is

\[
\hat{Y}_{j}=\tilde{X}_{j} \hat{\beta}_{j}=\tilde{X}_{j}\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime} Y_{j} .
\]

The following Proposition states the equivalence of the two-step procedure using the fitted values and the conventional one-step estimator in mean models.

Proposition 5. Denote \(\hat{\delta}_{G M M}^{M D}\) the coefficient vector of a linear GMM regression of \(\hat{Y}\) on \(X\) with instrument \(Z\). Let \(\hat{\delta}_{G M M}\) be the coefficient vector of the same GMM regression but with regressand \(Y\). Assume that for each \(j, Z_{j}\) lies in the column space of \(\tilde{X}_{j}\), then \(\hat{\delta}_{G M M}^{M D}=\hat{\delta}_{G M M}\).

The proof of this Proposition and all subsequent proofs are in Appendix B.2. Proposition 5 implies that any linear model can be computed by a two-step estimator as long as the matrix of instruments of each group \(j, Z_{j}\) lies in the column space of the matrix of first-stage regressors of group \(j, \tilde{X}_{j} .{ }^{35}\) This result applies to a wide range of estimators. Since OLS is a special case of GMM, the result for pooled OLS follows directly, while the result for the within estimator is summarized in the following Corollary.

Corollary 1. Denote \(\hat{\delta}_{F E}^{M D}\) the coefficient vector of an IV regression of \(\hat{Y}\) on \(X_{1}\) with instruments \(\dot{X}_{1}\). Let \(\hat{\delta}_{F E}\) be the coefficient vector of the within estimator, that is, of a regression of \(\dot{Y}\) on \(\dot{X}_{1}\). Then \(\hat{\delta}_{F E}^{M D}=\hat{\delta}_{F E}\).

The between estimator is usually computed by regressing \(\bar{Y}\) on \(\bar{X}\). Alternatively, it can be estimated by an IV regression of \(Y\) (or \(\hat{Y}\) ) on \(X\) using \(\bar{X}\) as an instrument, where it exploits only the variation between individuals.

Corollary 2. Denote \(\hat{\delta}_{B E}^{M D}\) the coefficient vector of an IV regression of \(\hat{Y}\) on \(X\) with instruments \(\bar{X}\). Let \(\hat{\delta}_{B E}\) be the coefficient vector of the between estimator, that is, of a regression of \(\bar{Y}\) on \(\bar{X}\). Then \(\hat{\delta}_{B E}^{M D}=\hat{\delta}_{B E}\).

It is worth noting that the IV approach to these panel data estimators also works in one stage with \(Y\) as the dependent variable. Further, it is clearly possible to estimate between (within) models using average (demeaned) fitted values and regressors.

The pooled OLS and the between estimators can estimate both \(\beta\) and \(\gamma\) but are not efficient. The random effects estimator optimally combines between and the within variation to find a more efficient estimator. While FGLS is the most common estimator for the random effects model, Im et al. (1999) show that the overidentified 3SLS estimator, with instruments \(Z_{j}=\left(\dot{X}_{1 j}, \bar{X}_{j}\right)\),

\footnotetext{\({ }^{35}\) Since \(\tilde{X}_{j}\) includes a constant, the presence of group-level variables in \(Z_{j}\) will not affect its column space.
}
is identical to the random effects estimator. The 3SLS estimator is a special case of GMM with weighting matrix \(W=\mathbb{E}\left[Z_{j}^{\prime} \tilde{\Omega} Z_{j}\right]\) where \(\tilde{\Omega}\) follows the usual random effects covariance structure. Thus, by Proposition 5, the random effects estimator can also be computed in two steps using the fitted values in the second stage.

Corollary 3. Denote \(\hat{\delta}_{R E}^{M D}\) the coefficient vector of a 3SLS regression of \(\hat{Y}\) on \(X\) with instruments \(\left(\dot{X}_{1 j}, \bar{X}_{j}\right)\). Let \(\hat{\delta}_{R E}\) be the coefficient vector of a random effects regression of \(Y\) on \(X\). Then \(\hat{\delta}_{R E}^{M D}=\hat{\delta}_{R E}\).

Alternatively, the random effects estimator can be implemented using the theory of optimal instruments and a just identified 2SLS regression. Starting from a conditional moment restriction, the idea of optimal instruments is to select an instrument and weights that minimize the asymptotic variance (see, e.g. Newey, 1993). Relevant to our two-step procedure, under homoskedasticity of the errors, the conditional moments \(\mathbb{E}\left[Y_{j}-X_{j} \delta \mid X_{j}\right]=0\) and \(\mathbb{E}\left[\hat{Y}_{j}-X_{j} \delta \mid X_{j}\right]=0\) imply the same optimal instrument and the problem simplifies to the usual random effects estimator.

Proposition 6. Assume \(\mathbb{E}\left[\varepsilon_{i j}^{2} \mid X_{j}\right]=\sigma_{\varepsilon}^{2}\) and \(\mathbb{E}\left[\alpha_{j}^{2} \mid X_{j}\right]=\sigma_{\alpha}^{2}\). The conditional moments \(\mathbb{E}\left[\hat{Y}_{j}\right.\) \(\left.X_{j} \delta \mid X_{j}\right]=0\) and \(\mathbb{E}\left[Y_{j}-X_{j} \delta \mid X_{j}\right]=0\) imply the same optimal instrument.

The Hausman-Taylor model (Hausman and Taylor, 1981) is a middle ground between the fixed effects and the random effects models where some regressors are assumed to be uncorrelated with \(\alpha_{j}\). The matrix of regressors \(X\) is partitioned as \(X=\left[X_{1}^{e x} X_{1}^{e n} X_{2}^{e x} X_{2}^{e n}\right]\) where \(X_{1}^{e x}\) and \(X_{2}^{e x}\) are orthogonal to \(\alpha_{j}\). No assumption is placed on the relationship between \(\alpha_{j}\) and \(X_{1}^{e n}\) and \(X_{2}^{e n}\). The model can be estimated by IV using instruments \(Z=\left(\dot{X}_{1}^{e x}, \dot{X}_{1}^{e n}, \bar{X}_{1}^{e x}, X_{2}^{e x}\right)\) (see, e.g., Hansen, 2022b). Thus, it follows by Proposition 5 that the Hausman-Taylor model can be estimated in two stages.

Corollary 4. Denote \(\hat{\delta}_{H T}^{M D}\) the coefficient vector of a \(2 S L S\) regression of \(\hat{Y}\) on \(X\) with instruments \(\left(\dot{X}_{1}^{e x}, \dot{X}_{1}^{e n}, \bar{X}_{1}^{e x}, X_{2}^{e x}\right)\). Let \(\hat{\delta}_{H T}\) be the coefficient vector of the Hausman-Taylor Estimator based on a regression \(Y\) on \(X\). Then \(\hat{\delta}_{H T}^{M D}=\hat{\delta}_{H T}\).

Finally, we show that not only the point estimates but also the standard errors can be obtained using the two-stage minimum distance approach. This requires clustering the standard errors in the second stage at a level weakly higher than the group \(j\). Let \(g=1, \ldots, G\) index the clusters and assume that each of the clusters has \(N_{g}\) observations. This nests the case where one wishes to cluster at the individual level or at a higher level. For example, if \(j\) are county-year combinations, one might cluster at the county level. For an estimator \(\hat{\delta}\), the clustered covariance\\
matrix is estimated by

\[
\begin{aligned}
\hat{V}_{\delta}= & \left(\frac{1}{G} \sum_{g=1}^{G} X_{g}^{\prime} Z_{g} \hat{W} \frac{1}{G} \sum_{g=1}^{G} Z_{g}^{\prime} X_{g}\right)^{-1} \frac{1}{G} \sum_{g=1}^{G} X_{g}^{\prime} Z_{g} \hat{W}\left(\frac{1}{G} \sum_{g=1}^{G} Z_{g}^{\prime} \tilde{u}_{g} \tilde{u}_{g}^{\prime} Z_{g}\right) \\
& \cdot \hat{W} \frac{1}{G} \sum_{g=1}^{G} Z_{g}^{\prime} X_{g}\left(\frac{1}{G} \sum_{g=1}^{G} X_{g}^{\prime} Z_{g} \hat{W} \frac{1}{G} \sum_{g=1}^{G} Z_{g}^{\prime} X_{g}\right)^{-1},
\end{aligned}
\]

where \(\tilde{u}_{g}\) is a \(N_{g}\)-dimensional vector of estimated errors for the observations in cluster \(g, X_{g}\) is the \(K \times N_{g}\) matrix of regressors of cluster \(g\), and \(Z_{g}\) is the \(L \times N_{g}\) matrix of instruments.

Proposition 7. Denote \(\hat{V}_{\delta}\) the clustered covariance matrix of \(\hat{\delta}\) estimated by a GMM regression of \(Y\) on \(X\) with instrument \(Z\). Let \(\hat{V}_{\delta}\) bD be the clustered covariance matrix of \(\hat{\delta}^{M D}\) estimated by GMM regression of \(\hat{Y}\) on \(X\) with instrument \(Z\), where \(\hat{Y}\) are estimated by an OLS first-stage. Let the clusters be at weakly higher level than \(j\). Then, \(\hat{V}_{\delta M D}=\hat{V}_{\delta}\).

\section*{B. 2 Proofs of the least squares results}
Proof of Proposition 5. Define the projection matrix \(\tilde{P}_{j}=\tilde{X}_{j}\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime}\). Since \(Z_{j}\) is in the column space of \(\tilde{X}_{j}\),

\[
\tilde{P}_{j} Z_{j}=Z_{j}
\]

The MD estimator with a GMM second stage is:

\[
\hat{\delta}_{G M M}^{M D}=\left(X^{\prime} Z \hat{W} Z^{\prime} X\right)^{-1} X^{\prime} Z \hat{W} Z^{\prime} \hat{Y}
\]

For \(\hat{\delta}_{G M M}^{M D}\) to be equal to \(\hat{\delta}_{G M M}\), it suffices that \(Z^{\prime} \hat{Y}=Z^{\prime} Y\). Note that

\[
\begin{aligned}
Z^{\prime} \hat{Y} & =\sum_{j=1}^{m} Z_{j}^{\prime} \hat{Y}_{j} \\
& =\sum_{j=1}^{m} Z_{j}^{\prime} \tilde{X}_{j} \hat{\beta}_{j} \\
& =\sum_{j=1}^{m} Z_{j}^{\prime} \tilde{X}_{j}\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime} Y_{j} \\
& =\sum_{j=1}^{m}\left(\tilde{P}_{j} Z_{j}\right)^{\prime} Y_{j} \\
& =\sum_{j=1}^{m} Z_{j}^{\prime} Y_{j}=Z^{\prime} Y,
\end{aligned}
\]

where the third line uses \(\hat{Y}_{j}=\tilde{X}_{j} \hat{\beta}_{j}\), the fourth line uses the definition of the first-step OLS estimator, and the last line uses equation (82). It then follows directly that \(\hat{\delta}_{M D}\) equals \(\hat{\delta}_{G M M}\).

Proof of Corollary 1. First, note that since \(Q_{j} X_{1 j}=\dot{X}_{1 j}, \dot{X}_{1 j}\) lies in the column space of \(X_{1 j}\). Then, we apply Proposition 5. It follows that a IV regression of \(\hat{Y}\) on \(X_{1 j}\) with instrument \(Z_{j}\) is algebraically identical to a IV regression with \(Y_{j}\) as dependent variable. Then,

\[
\begin{aligned}
\hat{\delta}_{F E}^{M D} & =\left(\sum_{j=1}^{m} Z_{j}^{\prime} X_{1 j}\right)^{-1} \sum_{j=1}^{m} Z_{j}^{\prime} Y_{j} \\
& =\left(\sum_{j=1}^{m} \dot{X}_{1 j}^{\prime} X_{1 j}\right)^{-1} \sum_{j=1}^{m} \dot{X}_{1 j}^{\prime} Y_{j} \\
& =\left(\sum_{j=1}^{m} X_{1 j}^{\prime} Q_{j} X_{1 j}\right)^{-1} \sum_{j=1}^{m} X_{1 j}^{\prime} Q_{j} Y_{j} \\
& =\left(\sum_{j=1}^{m} \dot{X_{1 j}^{\prime}} \dot{X_{1 j}}\right)^{-1} \sum_{j=1}^{m} \dot{X_{1 j}^{\prime}} \dot{Y}=\hat{\delta}_{F E},
\end{aligned}
\]

where the second line follows since \(Z_{j}=\dot{X}_{1 j}\), the third and last line by \(Q_{j} X_{1 j}=\dot{X}_{1 j}, Q_{j} Y_{j}=\dot{Y}_{j}\) and since \(Q_{j}\) is idempotent.

Proof of Corollary 2. First, note that since \(P_{j} \tilde{X}_{j}=\bar{X}_{j}, \bar{X}_{j}\) lies in the column space of \(\tilde{X}_{j}\). Then, we apply Proposition 5. It follows that an IV regression of \(\hat{Y}_{j}\) on \(X_{j}\) with instrument \(Z_{j}\) is algebraically identical to an IV regression with \(Y_{j}\) as dependent variable. Then,

\[
\begin{aligned}
\hat{\delta}_{B E}^{M D} & =\left(\sum_{j=1}^{m} Z_{j}^{\prime} X_{j}\right)^{-1} \sum_{j=1}^{m} Z_{j}^{\prime} Y_{j} \\
& =\left(\sum_{j=1}^{m} \bar{X}_{j}^{\prime} X_{j}\right)^{-1} \sum_{j=1}^{m} \bar{X}_{j}^{\prime} Y_{j} \\
& =\left(\sum_{j=1}^{m} X_{j}^{\prime} P_{j} X_{j}\right)^{-1} \sum_{j=1}^{m} X_{j}^{\prime} P_{j} Y_{j} \\
& =\left(\sum_{j=1}^{m} \bar{X}_{j}^{\prime} \bar{X}_{j}\right)^{-1} \sum_{j=1}^{m} \bar{X}_{j}^{\prime} \bar{Y}_{j}=\hat{\delta}_{B E}
\end{aligned}
\]

where the second line follows since \(Z_{j}=\bar{X}_{j}\), the third and last line by \(P_{j} X_{j}=\bar{X}_{j}, P_{j} Y_{j}=\bar{Y}_{j}\) and, since \(P_{j}\) is idempotent.

Proof of Proposition 6. The optimal instrument takes the form \(Z_{j}^{*}=\mathbb{E}\left[g_{j}(\delta) g_{j}(\delta)^{\prime} \mid Z_{j}\right]^{-1} R_{j}(\delta, \tau)\), where \(R_{j}(\delta, \tau)=\mathbb{E}\left[\left.\frac{\partial}{\partial \delta} g_{j}(\delta, \tau) \right\rvert\, Z_{j}\right]\). For both moment conditions, \(R_{j}(\delta, \tau)\) is identical. Then for\\
the first moment restriction, we have:

\[
\begin{aligned}
\mathbb{E}\left[\left(\hat{Y}_{j}-X_{j} \delta\right)\left(\hat{Y}_{j}-X_{j} \delta\right)^{\prime} \mid X_{j}\right] & =\mathbb{E}\left[\left(\tilde{X}_{j}\left(\hat{\beta}_{j}-\beta\right)+\tilde{X}_{j} \beta-X_{j} \delta\right)\left(\tilde{X}_{j}\left(\hat{\beta}_{j}-\beta\right)+\tilde{X}_{j} \beta-X_{j} \delta\right)^{\prime} \mid X_{j}\right] \\
& =\mathbb{E}\left[\left(\tilde{X}_{j}\left(\hat{\beta}_{j}-\beta\right)+\alpha_{j}\right)\left(\tilde{X}_{j}\left(\hat{\beta}_{j}-\beta\right)+\alpha_{j}\right)^{\prime} \mid X_{j}\right] \\
& =\tilde{X}_{j} \frac{V_{j}}{n} \tilde{X}_{j}^{\prime}+\mathbf{l}_{n} \mathbf{l}_{n}^{\prime} \sigma_{\alpha}^{2} .
\end{aligned}
\]

The matrix \(\tilde{X}_{j} \frac{V_{j}}{n} \tilde{X}_{j}^{\prime}+\mathbf{l}_{n} \mathbf{l}_{n}^{\prime} \sigma_{\alpha}^{2}\) is singular, so that we suggest using the Moore-Penrose inverse to construct the optimal instrument.\\
For the second moment restriction, we have:

\[
\begin{aligned}
\mathbb{E}\left[\left(Y_{j}-X_{j} \delta\right)\left(Y_{j}-X_{j} \delta\right)^{\prime} \mid X_{j}\right] & =\mathbb{E}\left[\left(\alpha_{j}+\varepsilon_{i j}\right)\left(\alpha_{j}+\varepsilon_{i j}\right)^{\prime} \mid X_{j}\right] \\
& =\left(\mathbf{I}_{n} \sigma_{\varepsilon}^{2}+\mathbf{l}_{n} \mathbf{1}_{n}^{\prime} \sigma_{\alpha}^{2}\right) .
\end{aligned}
\]

Then note that \(\left(\mathbf{I}_{n} \sigma_{\varepsilon}^{2}+\mathbf{l}_{n} \mathbf{l}_{n}^{\prime} \sigma_{\alpha}^{2}\right)^{-1}=\left(\tilde{X}_{j} \tilde{X}_{j}^{+} \sigma_{\varepsilon}^{2}+\mathbf{l}_{n}^{\prime} \mathbf{l}_{n} \sigma_{\alpha}^{2}\right)^{+}=\left(\tilde{X}_{j}\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime} \sigma_{\varepsilon}^{2}+\mathbf{l}_{n}^{\prime} \mathbf{l}_{n} \sigma_{\alpha}^{2}\right)^{+}=\) \(\left(\tilde{X}_{j} \frac{V_{j}}{n} \tilde{X}_{j}^{\prime}+\mathbf{l}_{n}^{\prime} \mathbf{l}_{n} \sigma_{\alpha}^{2}\right)^{+} X_{j}\), where \(V_{j}=\left(\frac{1}{n} \tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \sigma_{\varepsilon}^{2}\) and since for a full column rank matrix \(\tilde{X}_{j}\), \(\tilde{X}_{j} \tilde{X}_{j}^{+}=I_{n}\) and \(\tilde{X}_{j}^{+}=\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime}\).

Proof of Proposition 7. Define \(Z_{g}=\left(z_{1 g}, \ldots, z_{n_{g} g}\right)^{\prime}, X_{g}=\left(x_{1 g}, \ldots, x_{n_{g} g}\right)^{\prime}, Y_{g}=\left(y_{1 g}, \ldots, y_{n_{g} g}\right)^{\prime}\) and \(\hat{Y}_{g}=\left(\hat{y}_{1 g}, \ldots, \hat{y}_{n_{g} g}\right)^{\prime}\). The first and third terms of expression (81) are identical for both estimators. Hence, we focus on the middle term. Let \(\hat{u}_{g}=Y_{g}-X_{g} \hat{\delta}\) be the vector of residuals from the regression using \(Y\) as dependent variable, and let \(\hat{u}_{g}^{M D}=\hat{Y}_{g}-X_{g} \hat{\delta}^{M D}\) be the vector of residuals of the estimator using the fitted values as regressand. We show that \(Z_{g}^{\prime} \hat{u}_{g}=Z_{g}^{\prime} \hat{u}_{g}^{M D}\) for all \(g\). By Proposition \(5, \hat{\delta}^{M D}=\hat{\delta}\). Thus, the fitted values of both estimators are identical. Next, define \(X_{g}=\operatorname{diag}\left(X_{j}: j \in g\right)\) and recall that regressing \(Y_{g}\) on \(\breve{X}_{g}\) is the same as performing separate regression for each \(j \in g\). Let \(\breve{\beta}_{g}\) be the coefficient vector of an OLS regression of \(Y_{g}\) on \(\breve{X}_{g}\). Note that \(Z_{g}\) is in the column space of \(\tilde{X}_{g}\). Define the projection matrix \(\breve{P}=\) \(\breve{X}_{g}\left(\breve{X}_{g}^{\prime} \breve{X}_{g}\right)^{-1} \breve{X}_{g}^{\prime}\). Since \(Z_{g}\) is in the column space of \(\breve{X}_{g}\),

\[
\breve{P} Z_{g}=Z_{g} .
\]

Then,

\[
\begin{aligned}
Z_{g}^{\prime} \hat{u}_{g}^{M D} & =Z_{g}^{\prime}\left(\hat{Y}_{g}-X_{g} \hat{\delta}^{M D}\right) \\
& =Z_{g}^{\prime} \tilde{X}_{g} \breve{\beta}_{g}-Z_{g} X_{g} \hat{\delta} \\
& =Z_{g}^{\prime} \tilde{X}_{g}\left(\breve{X}_{g}^{\prime} \breve{X}_{g}\right)^{-1} \breve{X}_{g}^{\prime} Y_{g}-Z_{g} X_{g} \hat{\delta} \\
& =Z_{g}^{\prime}\left(Y_{g}-X_{g} \hat{\delta}\right)=Z_{g}^{\prime} \hat{u}_{g},
\end{aligned}
\]

where the fourth line follows by (84). Since this holds for all \(g\), the desired result follows directly.

\section*{C Optimal Instruments and Minimum Distance}
In this section, we show that if \(\alpha_{j}(\tau)=0\) for all \(j\) and \(\tau\), efficient minimum distance can be implemented by optimal instruments. From equation (83) we have that if \(\alpha_{j}(\tau)=0\) for all \(j\) and all \(\tau, \mathbb{E}\left[\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-X_{j} \delta(\tau)\right)\left(\tilde{X}_{j} \hat{\beta}_{j}(\tau)-X_{j} \delta(\tau)\right)^{\prime} \mid X_{j}\right]=\tilde{X}_{j} \frac{V_{j}(\tau)}{n} \tilde{X}_{j}^{\prime}\). This implies the optimal instrument \(Z_{j}^{*}=\left(\tilde{X}_{j} \frac{V_{j}(\tau)}{n} \tilde{X}_{j}^{\prime}\right)^{+} X_{j}\). Since \(n\) is a scalar, using \(Z_{j}^{*}(\tau)=\left(\tilde{X}_{j} V_{j}(\tau) \tilde{X}_{j}^{\prime}\right)^{+} X_{j}\) leads to the same results.

Proposition 8. The IV regression with instrument \(Z_{j}^{*}(\tau)=\left(\tilde{X}_{j} V_{j}(\tau) \tilde{X}_{j}^{\prime}\right)^{+} X_{j}\) is the efficient MD estimator.

Proof.

\[
\begin{aligned}
\hat{\delta}_{E M D}(\tau) & =\left(\sum_{j=1}^{m} R_{j}^{\prime} \hat{V}_{j}^{-1}(\tau) R_{j}\right)^{-1}\left(\sum_{j=1}^{m} R_{j}^{\prime} \hat{V}_{j}^{-1}(\tau) \hat{\beta}_{j}(\tau)\right) \\
& =\left(\sum_{j=1}^{m} X_{j}^{\prime} \tilde{X}_{j}\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j} \hat{V}_{j}(\tau) \tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime} X_{j}\right)^{-1}\left(X_{j}^{\prime} \tilde{X}_{j}\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j} \hat{V}_{j}(\tau) \tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime} \hat{Y}_{j}(\tau)\right) \\
& =\left(\sum_{j=1}^{m} X_{j}^{\prime}\left(\tilde{X}_{j} \hat{V}_{j}(\tau) \tilde{X}_{j}^{\prime}\right)^{+} X_{j}\right)^{-1}\left(X_{j}^{\prime}\left(\tilde{X}_{j} \hat{V}_{j}(\tau) \tilde{X}_{j}^{\prime}\right)^{+} \hat{Y}_{j}(\tau)\right)=\hat{\delta}_{O I}(\tau)
\end{aligned}
\]

The second line follows as \(\tilde{X}_{j} R_{j}=X_{j}\) and the third line follows since for a full column rank matrix \(\tilde{X}_{j}, \tilde{X}_{j}^{+}=\left(\tilde{X}_{j}^{\prime} \tilde{X}_{j}\right)^{-1} \tilde{X}_{j}^{\prime}\).


\end{document}