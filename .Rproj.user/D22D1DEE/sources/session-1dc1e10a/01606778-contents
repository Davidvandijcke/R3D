% This LaTeX document needs to be compiled with XeLaTeX.
\documentclass[10pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{ucharclasses}
\usepackage{hyperref}
\hypersetup{colorlinks=true, linkcolor=blue, filecolor=magenta, urlcolor=cyan,}
\urlstyle{same}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[version=4]{mhchem}
\usepackage{stmaryrd}
\usepackage{bbold}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\graphicspath{ {./images/} }
\usepackage{multirow}
\usepackage{polyglossia}
\usepackage{fontspec}
\setmainlanguage{english}
\setotherlanguages{hindi}
\IfFontExistsTF{Noto Serif Devanagari}
{\newfontfamily\hindifont{Noto Serif Devanagari}}
{\IfFontExistsTF{Kohinoor Devanagari}
  {\newfontfamily\hindifont{Kohinoor Devanagari}}
  {\IfFontExistsTF{Devanagari MT}
    {\newfontfamily\hindifont{Devanagari MT}}
    {\IfFontExistsTF{Lohit Devanagari}
      {\newfontfamily\hindifont{Lohit Devanagari}}
      {\IfFontExistsTF{FreeSerif}
        {\newfontfamily\hindifont{FreeSerif}}
        {\newfontfamily\hindifont{Arial Unicode MS}}
}}}}
\IfFontExistsTF{CMU Serif}
{\newfontfamily\lgcfont{CMU Serif}}
{\IfFontExistsTF{DejaVu Sans}
  {\newfontfamily\lgcfont{DejaVu Sans}}
  {\newfontfamily\lgcfont{Georgia}}
}
\setDefaultTransitions{\lgcfont}{}
\setTransitionsForDevanagari{\hindifont}{\rmfamily}

\title{Flexible Covariate Adjustments in Regression Discontinuity Designs }

\author{Claudia Noack Tomasz Olma Christoph Rothe}
\date{}


%New command to display footnote whose markers will always be hidden
\let\svthefootnote\thefootnote
\newcommand\blfootnotetext[1]{%
  \let\thefootnote\relax\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \let\thefootnote\svthefootnote%
}

%Overriding the \footnotetext command to hide the marker if its value is `0`
\let\svfootnotetext\footnotetext
\renewcommand\footnotetext[2][?]{%
  \if\relax#1\relax%
    \ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
  \else%
    \if?#1\ifnum\value{footnote}=0\blfootnotetext{#2}\else\svfootnotetext{#2}\fi%
    \else\svfootnotetext[#1]{#2}\fi%
  \fi
}

\begin{document}
\maketitle
July 10, 2024

\begin{abstract}
Empirical regression discontinuity (RD) studies often use covariates to increase the precision of their estimates. In this paper, we propose a novel class of estimators that use such covariate information more efficiently than existing methods and can accommodate many covariates. It involves running a standard RD analysis in which a function of the covariates has been subtracted from the original outcome variable. We characterize the function that leads to the estimator with the smallest asymptotic variance, and consider feasible versions of such estimators in which this function is estimated, for example, through modern machine learning techniques.
\end{abstract}

\footnotetext{First version: July 16, 2021. This version: July 10, 2024. We thank Sebastian Calonico, Michal Kolesár, Thomas Lemieux, Jonathan Roth, Vira Semenova, Stefan Wager, Daniel Wilhelm, Andrei Zeleneev, and numerous conference and seminar participants for helpful comments and suggestions. We thank Tobias Großbölting and Merve Ögrretmek for excellent research assistance. The authors gratefully acknowledge financial support by the European Research Council (ERC) through grant SH1-77202. The second author also gratefully acknowledges support from the European Research Council through Starting Grant No. 852332. Author contact information: Claudia Noack, Department of Economics, University of Bonn. E-Mail: \href{mailto:claudia.noack@uni-bonn.de}{claudia.noack@uni-bonn.de}. Website: \href{https://claudianoack.github.io}{https://claudianoack.github.io}. Tomasz Olma, Department of Statistics, Ludwig Maximilian University of Munich. E-Mail: \href{mailto:t.olma@lmu.de}{t.olma@lmu.de}. Website: \href{https://tomaszolma.github.io}{https://tomaszolma.github.io}. Christoph Rothe, Department of Economics, University of Mannheim. E-Mail: \href{mailto:rothe@vwl.uni-mannheim.de}{rothe@vwl.uni-mannheim.de}. Website: \href{http://www.christophrothe.net}{http://www.christophrothe.net}.
}\section*{1. INTRODUCTION}
Regression discontinuity (RD) designs are widely used for estimating causal effects from observational data in economics and other social sciences. The design exploits that in many contexts a unit's treatment status is determined by whether its realization of a running variable exceeds some known cutoff value. For example, students might qualify for a scholarship if their GPA is above some threshold. Under continuity conditions on the distribution of potential outcomes, the average treatment effect at the cutoff is identified by the jump in the conditional expectation of the outcome given the running variable at the cutoff. Methods for estimation and inference based on local linear regression are widely used, and their properties are by now well understood (e.g., Hahn et al., 2001; Imbens and Kalyanaraman, 2012; Calonico et al., 2014; Armstrong and Kolesár, 2020).

While an RD analysis generally does not require data beyond the outcome and the running variable, additional covariate information can be used to reduce the variance of empirical estimates. A common strategy is to include the covariates linearly and without separate localization in a local linear RD regression (Calonico et al., 2019). This conventional linear adjustment estimator is consistent without functional form assumptions on the underlying conditional expectations if the covariates are unaffected by the treatment in some appropriate sense. However, it generally does not exploit the available covariate information efficiently and it is also not well-suited for settings with many covariates.

To address these issues, we propose a novel class of flexible covariate-adjusted RD estimators. Our approach involves running a standard local linear RD regression after subtracting an (estimated) function of the covariates from the original outcome variable. We characterize the function that leads to the RD estimator with the smallest asymptotic variance, and show how this function can be estimated with modern machine learning techniques. We also show that existing methods for bandwidth choice and inference can directly be used with our adjusted outcomes, which means that our approach is easily implemented with existing software packages.

Our proposed flexible covariate adjustments can lead to substantial efficiency gains in practice. To illustrate this, we conducted a comprehensive literature survey and reanalyzed the available data from the papers that use RD estimation with covariates and appeared between 2018 and 2023 in the AEA journals that publish applied microeconomic research (AER, AER Insights, AEJ: Applied Economics, AEJ: Economic Policy, and AEA Papers and Proceedings). In total, we reanalyzed 56 specifications from 16 papers, and studied how different types of covariate adjustments affect the length of the confidence interval for the main RD parameter. While in about half of the specifications including covariates into the RD regression does not reduce the length of the confidence intervals, the reduction due to our proposed flexible adjustments reaches more than \(35 \%\) in one setting. To put this into perspective, obtaining this reduction would require to triple the sample size if the covariates were not used. We also observe that the linear adjustments alone are unable to exhaust all the available covariate information as the largest reduction in the confidence interval length from\\
using our flexible adjustment relative to linear adjustments exceeds \(21 \%\).\\
To motivate our proposed procedure, let \(Y_{i}\) and \(Z_{i}\) denote the outcome and covariates, respectively, of observational unit \(i\), and note that the conventional linear adjustment RD estimator is asymptotically equivalent to a local linear RD estimator with the modified outcome variable \(Y_{i}-Z_{i}^{\top} \gamma_{0}\), where \(\gamma_{0}\) is a vector of projection coefficients. We consider generalizations of such estimators which replace the linearly adjusted outcome with a flexibly adjusted outcome of the form \(Y_{i}-\eta\left(Z_{i}\right)\), for some generic function \(\eta\). Such estimators are easily seen to be consistent for any fixed \(\eta\) if the distribution of the covariates varies smoothly around the cutoff in some appropriate sense (which is in line with the notion of covariates being unaffected by the treatment). We show that the asymptotic variance in this class of estimators is minimized if \(\eta=\eta_{0}\) is the average of the two conditional expectations of the outcome variable given the running variable and the covariates just above and below the cutoff. This optimal adjustment function is generally nonlinear and unknown in practice but can be estimated from the data.

Our proposed estimators hence take the form of a local linear RD regression with generated outcome \(Y_{i}-\widehat{\eta}\left(Z_{i}\right)\), where \(\widehat{\eta}\) is some estimate of \(\eta_{0}\) obtained in a preliminary stage. We implement such estimators with cross-fitting (e.g., Chernozhukov et al., 2018), which is an efficient form of sample splitting that removes some bias and allows us to accommodate a wide range of estimators of the optimal adjustment function. In particular, one can use modern machine learning methods like lasso regression, random forests, deep neural networks, or ensemble combinations thereof, to estimate the optimal adjustment function. However, in low-dimensional settings, researchers can also use classical nonparametric approaches like local polynomials or series regression, or estimators based on parametric specifications.

Our theory does not require that \(\eta_{0}\) is consistently estimated for valid inference on the RD parameter in our setup. We only require that in large samples the first-stage estimates concentrate in a mean-square sense around some deterministic function \(\bar{\eta}\), which could in principle be different from \(\eta_{0}\). The rate of this convergence can be arbitrarily slow. Our setup allows for this kind of potential misspecification because our proposed RD estimators are "very insensitive" to estimation errors in the preliminary stage. This is because they are constructed as sample analogs of a moment function that contains \(\eta_{0}\) as a nuisance function, but does not vary with it: as discussed above, our parameter of interest is equal to the jump in the conditional expectation of \(Y_{i}-\eta\left(Z_{i}\right)\) given the running variable at the cutoff for any fixed function \(\eta\). This insensitivity property is related to Neyman orthogonality, which features prominently in many modern two-stage estimation methods (e.g., Chernozhukov et al., 2018), but it is a global rather than a local property and is thus in effect substantially stronger. \({ }^{1}\)

\footnotetext{\({ }^{1}\) A moment function is Neyman orthogonal if its first functional derivative with respect to the nuisance function is zero, but the (conditional) moment function on which our estimates are based is fully invariant with respect to the nuisance function. Chernozhukov et al. (2018) give several examples of setups in which such a property occurs, which include optimal instrument problems, certain partial linear models, and treatment effect estimation under unconfoundedness with known propensity score. Such global insensitivity is also easily seen to occur more generally if
}Our theoretical analysis shows that, under the conditions outlined above, our proposed RD estimator is first-order asymptotically equivalent to a local linear "no covariates" RD estimator with \(Y_{i}-\bar{\eta}\left(Z_{i}\right)\) as the dependent variable. This result is then used to study its asymptotic bias and variance, and to derive an asymptotic normality result. The asymptotic variance of our estimator depends on the function \(\bar{\eta}\) and achieves its minimum value if \(\bar{\eta}=\eta_{0}\) (that is if \(\eta_{0}\) is consistently estimated in the first stage), but the variance can be estimated consistently irrespective of whether or not that is the case. As our result does not require a particular rate of convergence for the first step estimate of \(\eta_{0}\), our RD estimator can be seen as shielded from the "curse of dimensionality" to some degree, and can hence be expected to perform well in settings with many covariates.

Practical issues like bandwidth choice and construction of confidence intervals with good coverage properties can be addressed in a straightforward manner. Specifically, we prove that one can apply standard methods to a data set in which the outcome \(Y_{i}\) is replaced with the generated outcome \(Y_{i}-\widehat{\eta}\left(Z_{i}\right)\), ignoring that \(\widehat{\eta}\) has been estimated. Our approach can therefore easily be integrated into existing software packages.

In many empirical applications, the RD designs are fuzzy, meaning that the probability of treatment jumps at the cutoff but not necessarily from zero to one. In these settings, the parameter of interest is the ratio of two sharp RD parameters. Our methodology developed for sharp RD designs can be easily extended to these settings. Specifically, the key insight is that it is optimal to use our proposed sharp RD estimator to estimate the numerator and denominator separately.

Our theoretical results are qualitatively similar to those that have been obtained for efficient influence function (EIF) estimators of the population average treatment effect in simple randomized experiments with known and constant propensity scores (e.g., Wager et al., 2016). Such parallels arise because EIF estimators are also based on a moment function that is globally invariant with respect to a nuisance function. In fact, we argue that our RD estimator is in many ways a direct analog of the EIF estimator, and that the variance it achieves under the optimal adjustment function is similar in structure to the semiparametric efficiency bound in simple randomized experiments.

We conduct simulations based on the data set from one of the papers from our empirical literature survey. In order to cover all types of settings from our empirical literature survey, we consider simulation setups of large sample sizes and a moderate number of covariates as well as small sample sizes and a varying number of covariates. Our proposed RD estimators perform very well in all these settings, in the sense that their standard errors are close to their standard deviations and the associated confidence intervals have simulated coverage rate close to the nominal one. RD estimators based on conventional linear adjustments also perform well if the number of covariates relative to the sample size is small. However, in settings of moderate and large numbers of covariates, their standard errors can be substantially downwards biased, so that the associated confidence intervals have simulated coverage substantially below their nominal one.\\
one of the two nuisance functions in a doubly robust moment (cf. Robins and Rotnitzky, 2001) is known.

Related Literature. Our paper contributes to an extensive literature on estimation and inference in RD designs; see, e.g., Imbens and Lemieux (2008) and Lee and Lemieux (2010) for surveys, and Cattaneo et al. (2019) for a textbook treatment. Different ad-hoc methods for incorporating covariates into an RD analysis have long been used in empirical economics (see, e.g., Lee and Lemieux, 2010, Section 3.2.3). Following Calonico et al. (2019), it has become common practice to include covariates without localization into the usual local linear regression estimator. We show that our approach nests this estimator as a special case, but is generally more efficient. Other closely related papers are Kreiß and Rothe (2023) and Arai et al. (2024), who extend the approach in Calonico et al. (2019) to settings with high-dimensional covariates under sparsity conditions using the lasso. In contrast, our approach allows for flexible use of other machine learning methods in the spirit of double-debiased machine learning of Chernozhukov et al. (2018). Moreover, even if one commits to lasso-based adjustments, there are two ways in which our approach can improve upon the methods of Kreiß and Rothe (2023) and Arai et al. (2024). First, we propose a different variant of (post-) lasso adjustments that can be more stable in finite samples (see "global adjustments" in Section 3.3). Second, cross-fitting yields a more precise standard error even if the number of selected covariates is not small relative to the effective sample size. Frölich and Huber (2019) propose to incorporate covariates into an RD analysis in a fully nonparametric fashion, but their approach is generally affected by the curse of dimensionality, and is thus unlikely to perform well in practice.

Our paper is also related in a more general way to the vast literature on two-step estimation problems with infinite-dimensional nuisance parameters (e.g., Andrews, 1994; Newey, 1994), especially the recent strand that exploits Neyman orthogonal (or debiased) moment functions and cross-fitting (e.g., Belloni et al., 2017; Chernozhukov et al., 2018). The latter literature focuses mostly on regular (root- \(n\) estimable) parameters, while our RD treatment effect is a non-regular (nonparametric) quantity. Some general results on non-regular estimation based on orthogonal moments are derived in Chernozhukov et al. (2019), and specific results for estimating conditional average treatment effects in models with unconfoundedness are given, for example, in Kennedy et al. (2017), Kennedy (2020) and Fan et al. (2020). Our results are qualitatively different because, as explained above, our estimator is based on a moment function that satisfies a property that is stronger than Neyman orthogonality.

Plan of the Paper. The remainder of this paper is organized as follows. In Section 2, we introduce the setup and review existing procedures. In Section 3, we describe our proposed covariate-adjusted RD estimator, and we present the results of our empirical literature survey and reanalysis in Section 4. In Section 5, we present our main theoretical results. Further extensions are discussed in Section 6. Section 7 contains a simulation study. Section 8 concludes. The proofs of our main results are given in Appendix A. Appendix B formally studies the proposed inference procedures and Appendix C gives details on our literature survey. The Online Supplement contains additional empirical and simulation results.

\section*{2. SETUP AND PRELIMINARIES}
2.1. Model and Parameter of Interest. We begin by considering sharp RD designs. The data \(\left\{W_{i}\right\}_{i \in[n]}=\left\{\left(Y_{i}, X_{i}, Z_{i}\right)\right\}_{i \in[n]}\), where \([n]=\{1, \ldots, n\}\), are an i.i.d. sample of size \(n\) from the distribution of \(W=(Y, X, Z)\). Here, \(Y_{i} \in \mathbb{R}\) is the outcome variable, \(X_{i} \in \mathbb{R}\) is the running variable, and \(Z_{i} \in \mathbb{R}^{d}\) is a (possibly high-dimensional) vector of covariates. \({ }^{2}\) Units receive the treatment if and only if the running variable exceeds a known threshold, which we normalize to zero without loss of generality. We denote the treatment indicator by \(T_{i}\), so that \(T_{i}=\mathbf{1}\left\{X_{i} \geq 0\right\}\). The parameter of interest is the height of the jump in the conditional expectation of the observed outcome variable given the running variable at zero:

\[
\tau=\mathbb{E}\left[Y_{i} \mid X_{i}=0^{+}\right]-\mathbb{E}\left[Y_{i} \mid X_{i}=0^{-}\right],
\]

where we use the notation that \(f\left(0^{+}\right)=\lim _{x \downarrow 0} f(x)\) and \(f\left(0^{-}\right)=\lim _{x \uparrow 0} f(x)\) are the right and left limit, respectively, of a generic function \(f(x)\) at zero. In a potential outcomes framework, the parameter \(\tau\) coincides with the average treatment effect of units at the cutoff under certain continuity conditions (Hahn et al., 2001).\\
2.2. Standard RD Estimator. Without the use of covariates, the parameter \(\tau\) is typically estimated by running separate local linear regressions (Fan and Gijbels, 1996) on each side of the cutoff. That is, the baseline no covariates RD estimator takes the form

\[
\widehat{\tau}_{\text {base }}(h)=e_{1}^{\top} \underset{\beta \in \mathbb{R}^{4}}{\operatorname{argmin}} \sum_{i=1}^{n} K_{h}\left(X_{i}\right)\left(Y_{i}-S_{i}^{\top} \beta\right)^{2},
\]

where \(S_{i}=\left(T_{i}, X_{i}, T_{i} X_{i}, 1\right)^{\top}, K_{h}(v)=K(v / h) / h\) with \(K(\cdot)\) a kernel function and \(h>0\) a bandwidth, and \(e_{1}=(1,0,0,0)^{\top}\) is the first unit vector. This estimator is a linear smoother that can also be written as a weighted sum of the realizations of the outcome variable,

\[
\widehat{\tau}_{\text {base }}(h)=\sum_{i=1}^{n} w_{i}(h) Y_{i},
\]

where the \(w_{i}(h)\) are local linear regression weights that depend on the data through the realizations of the running variable only; see Appendix A. 1 for an explicit expression.

Under standard conditions (e.g. Hahn et al., 2001), which include that the running variable is continuously distributed, and that the bandwidth \(h\) tends to zero at an appropriate rate, the estimator \(\widehat{\tau}_{\text {base }}(h)\) is approximately normally distributed in large samples, with bias of order \(h^{2}\) and

\footnotetext{\({ }^{2}\) Throughout the paper, we assume that the distribution of the running variable \(X_{i}\) is fixed, but we allow the conditional distribution of \(\left(Y_{i}, Z_{i}\right)\) given \(X_{i}\) to change with the sample size in our asymptotic analysis. In particular, we allow the dimension of \(Z_{i}\) to grow with \(n\) in order to accommodate high-dimensional settings, but we generally leave such dependence on \(n\) implicit in our notation.
}
variance of order \((n h)^{-1}\) :
\[
\widehat{\tau}_{\text {base }}(h) \stackrel{a}{\sim} N\left(\tau+h^{2} B_{\text {base }},(n h)^{-1} V_{\text {base }}\right) .
\]

Here " \(\sim\) " indicates a finite-sample distributional approximation justified by an asymptotic normality result, and the bias and variance terms are given, respectively, by

\[
\begin{aligned}
B_{\text {base }} & =\frac{\bar{\nu}}{2}\left(\left.\partial_{x}^{2} \mathbb{E}\left[Y_{i} \mid X_{i}=x\right]\right|_{x=0^{+}}-\left.\partial_{x}^{2} \mathbb{E}\left[Y_{i} \mid X_{i}=x\right]\right|_{x=0^{-}}\right) \text {and } \\
V_{\text {base }} & =\frac{\bar{\kappa}}{f_{X}(0)}\left(\mathbb{V}\left[Y_{i} \mid X_{i}=0^{+}\right]+\mathbb{V}\left[Y_{i} \mid X_{i}=0^{-}\right]\right) .
\end{aligned}
\]

The terms \(\bar{\nu}\) and \(\bar{\kappa}\) are kernel constants, defined as \(\bar{\nu}=\left(\bar{\nu}_{2}^{2}-\bar{\nu}_{1} \bar{\nu}_{3}\right) /\left(\bar{\nu}_{2} \bar{\nu}_{0}-\bar{\nu}_{1}^{2}\right)\) for \(\bar{\nu}_{j}=\int_{0}^{\infty} v^{j} K(v) d v\) and \(\bar{\kappa}=\int_{0}^{\infty}\left(K(v)\left(\bar{\nu}_{1} v-\bar{\nu}_{2}\right)\right)^{2} d v /\left(\bar{\nu}_{2} \bar{\nu}_{0}-\bar{\nu}_{1}^{2}\right)^{2}\), and \(f_{X}\) denotes the density of \(X_{i}\). Practical methods for inference based on approximations like (2.3) are discussed, for instance, by Calonico et al. (2014) and Armstrong and Kolesár (2020).\\
2.3. Conventional Linear Adjustment Estimator. If covariates are available, they can be used to improve the accuracy of empirical RD estimates. The arguably most popular strategy (Calonico et al., 2019) is to include them linearly and without kernel localization in the local linear regression (2.2):

\[
\widehat{\tau}_{l i n}(h)=e_{1}^{\top} \underset{\beta, \gamma}{\operatorname{argmin}} \sum_{i=1}^{n} K_{h}\left(X_{i}\right)\left(Y_{i}-S_{i}^{\top} \beta-Z_{i}^{\top} \gamma\right)^{2} .
\]

By simple least squares algebra, this "linear adjustment" estimator can be written as a no covariates estimator with covariate-adjusted outcome \(Y_{i}-Z_{i}^{\top} \widehat{\gamma}_{h}\), where \(\widehat{\gamma}_{h}\) is the minimizer with respect to \(\gamma\) in (2.4):

\[
\widehat{\tau}_{\text {lin }}(h)=\sum_{i=1}^{n} w_{i}(h)\left(Y_{i}-Z_{i}^{\top} \widehat{\gamma}_{h}\right) .
\]

The linear adjustment estimator is consistent for the RD parameter without functional form assumptions on the underlying conditional expectations if the covariates are predetermined, in the sense that their values are not causally affected by the treatment, and thus their conditional expectation given the running variable varies smoothly around the cutoff. Moreover, if \(\mathbb{E}\left[Z_{i} \mid X_{i}=x\right]\) is twice continuously differentiable around the cutoff, then

\[
\widehat{\tau}_{\text {lin }}(h) \stackrel{a}{\sim} N\left(\tau+h^{2} B_{\text {base }},(n h)^{-1} V_{\text {lin }}\right)
\]

under regularity conditions analogous to those for the no covariates estimator. Here the bias term \(B_{\text {base }}\) is the same as that of the no covariates estimator and the new variance term is

\[
V_{\text {lin }}=\frac{\bar{\kappa}}{f_{X}(0)}\left(\mathbb{V}\left[Y_{i}-Z_{i}^{\top} \gamma_{0} \mid X_{i}=0^{+}\right]+\mathbb{V}\left[Y_{i}-Z_{i}^{\top} \gamma_{0} \mid X_{i}=0^{-}\right]\right),
\]

where \(\gamma_{0}\), a non-random vector of projection coefficients, is the probability limit of \(\widehat{\gamma}_{h}\). The first-order asymptotic properties of \(\widehat{\tau}_{l i n}(h)\) are thus the same as that of its infeasible counterpart \(\widetilde{\tau}_{\text {lin }}(h)=\sum_{i=1}^{n} w_{i}(h)\left(Y_{i}-Z_{i}^{\top} \gamma_{0}\right)\) that uses the population projection coefficients \(\gamma_{0}\) instead of their estimates \(\widehat{\gamma}_{h}\) to create the adjusted outcome variable. As \(V_{\text {lin }} \leq V_{\text {base }}\) under standard conditions (Kreiß and Rothe, 2023, Remark 3.5), including a fixed number of covariates generally increases the precision of the estimator in large samples. To construct standard errors and confidence intervals, one can then use methods developed for the no covariates case, replacing the original outcome \(Y_{i}\) with the adjusted outcome \(Y_{i}-Z_{i}^{\top} \widehat{\gamma}_{h}\) in the respective formulas (Calonico et al., 2019; Armstrong and Kolesár, 2018). For instance, one can construct a nearest-neighbor standard error \(\widehat{\operatorname{se}}_{l i n}(h)\) of \(\widehat{\tau}_{\text {lin }}(h)\) as

\[
\widehat{\operatorname{se}}_{l i n}^{2}(h)=\sum_{i=1}^{n} w_{i}(h)^{2} \widehat{\sigma}_{i, l i n}^{2}, \quad \widehat{\sigma}_{i, l i n}^{2}=\frac{R}{R+1}\left(\left(Y_{i}-Z_{i}^{\top} \widehat{\gamma}_{h}\right)-\frac{1}{R} \sum_{j \in \mathcal{R}_{i}}\left(Y_{j}-Z_{j}^{\top} \widehat{\gamma}_{h}\right)\right)^{2} .
\]

Here \(\widehat{\sigma}_{i, l i n}^{2}\) is an estimate of \(\sigma_{i, l i n}^{2}=\mathbb{V}\left(Y_{i}-Z_{i}^{\top} \gamma_{0} \mid X_{i}\right), R \geq 1\) is a (small) fixed integer, and \(\mathcal{R}_{i}\) is the set that contains the indices of the \(R\) nearest neighbors of unit \(i\) in terms of their realization of the running variable among units on the same side of the cutoff.

\section*{3. FLEXIBLE COVARIATE ADJUSTMENTS}
3.1. Motivation. While linear adjustments are easy to implement, they might not exploit the available covariate information efficiently. Inference might also not be reliable with linear adjustments if the number of covariates is large relative to the effective sample size. \({ }^{3}\) In this paper, we propose a new method to address this issue. It allows for general nonlinear covariate adjustments and can accommodate regularization methods in the estimation of the adjustment terms.

To motivate our flexible covariate adjustments, recall that the linear adjustment estimator is asymptotically equivalent to a no covariates RD estimator of the form in (2.2) that uses the covariate-adjusted outcome \(Y_{i}-Z_{i}^{\top} \gamma_{0}\) instead of the original outcome \(Y_{i}\). This can be generalized by considering a class of estimators with covariate-adjusted outcomes based on potentially nonlinear adjustment functions \(\eta\) :

\[
\widehat{\tau}(h ; \eta)=\sum_{i=1}^{n} w_{i}(h) M_{i}(\eta), \quad M_{i}(\eta)=Y_{i}-\eta\left(Z_{i}\right) .
\]

If the covariates are predetermined, one would arguably expect their entire conditional distribution given the running variable to vary smoothly around the cutoff. We formalize this notion by assuming

\footnotetext{\({ }^{3}\) If there are many covariates relative to the number of observations that receive positive kernel weights in (2.4), the standard error in (2.5) is generally downward biased. This bias occurs because due to overfitting, the local empirical variances \(\widehat{\sigma}_{i, l i n}^{2}\) are typically smaller than their population counterparts \(\sigma_{i, l i n}^{2}\) in such cases. If the number of covariates exceeds the number of observations with positive kernel weights, the estimator in equation (2.4) is of course not even well-defined in the first place.
}
that for every adjustment function \(\eta\), the function \(\mathbb{E}\left[\eta\left(Z_{i}\right) \mid X_{i}=x\right]\) is twice continuously differentiable around the cutoff. \({ }^{4}\) This assumption implies that
\[
\tau=\mathbb{E}\left[M_{i}(\eta) \mid X_{i}=0^{+}\right]-\mathbb{E}\left[M_{i}(\eta) \mid X_{i}=0^{-}\right] \text {for all } \eta .
\]

The estimator \(\widehat{\tau}(h ; \eta)\) can thus be seen as a sample analog estimator based on the moment condition (3.2), which identifies \(\tau\) and is globally invariant with respect to the adjustment function \(\eta\). Because of this global invariance, we expect that

\[
\widehat{\tau}(h ; \eta) \stackrel{a}{\sim} N\left(\tau+h^{2} B_{\text {base }},(n h)^{-1} V(\eta)\right) .
\]

for every \(\eta\) under standard regularity conditions. The bias term in (3.3) is again that of the baseline no covariates estimator. Because of the assumed smoothness of \(\mathbb{E}\left[\eta\left(Z_{i}\right) \mid X_{i}=x\right]\), it does not depend on the adjustment function. On the other hand, the variance term in (3.3) does depend on \(\eta\), and is given by

\[
V(\eta)=\frac{\bar{\kappa}}{f_{X}(0)}\left(\mathbb{V}\left[M_{i}(\eta) \mid X_{i}=0^{+}\right]+\mathbb{V}\left[M_{i}(\eta) \mid X_{i}=0^{-}\right]\right) .
\]

To maximize the precision of the estimator \(\widehat{\tau}(h ; \eta)\) for any particular bandwidth \(h\), we want to choose \(\eta\) such that \(V(\eta)\) is as small as possible. Our analysis below shows that using the equally-weighted average of the left and right limits of the "long" conditional expectation function \(\mathbb{E}\left[Y_{i} \mid X_{i}=x, Z_{i}=z\right]\) at the cutoff achieves this goal. That is, we show that

\[
V(\eta) \geq V\left(\eta_{0}\right) \text { for all } \eta
\]

where

\[
\eta_{0}(z)=\frac{1}{2}\left(\mu_{0}^{+}(z)+\mu_{0}^{-}(z)\right), \quad \mu_{0}^{\star}(z)=\mathbb{E}\left[Y_{i} \mid X_{i}=0^{\star}, Z_{i}=z\right] \text { for } \star \in\{+,-\} .
\]

As the optimal adjustment function \(\eta_{0}\) is generally unknown in practice, we propose to estimate the RD parameter \(\tau\) by a feasible version of \(\widehat{\tau}\left(h ; \eta_{0}\right)\).\\
3.2. Proposed Estimator and its General Properties. Our proposed estimator requires a first-stage estimate of the optimal adjustment function. This does not have to be of a particular type: practitioners can use classical nonparametric or modern machine learning methods to reduce the risk of model misspecification, or choose suitable parametric methods based on their domain knowledge (conventional linear adjustments can be seen as a special case of the latter type). Our

\footnotetext{\({ }^{4}\) Our analysis only rules out adjustment functions that do not satisfy certain technical regularity conditions, such as functions for which the respective conditional expectation does not exist in the first place. Assuming smoothness of \(\mathbb{E}\left[\eta\left(Z_{i}\right) \mid X_{i}=x\right]\) for (essentially) all \(\eta\) is of course stronger than only assuming smoothness of \(\mathbb{E}\left[Z_{i} \mid X_{i}=x\right]\), as in Calonico et al. (2019). Our stronger assumption, however, is still very much in line with the notion of covariates being predetermined.
}
proposed estimator also uses cross-fitting, which is an efficient form of sample splitting that prevents overfitting of the estimated adjustment function and avoids unrealistic empirical process conditions in the theoretical analysis (Chernozhukov et al., 2018). Specifically, our estimator is computed in two steps:

\begin{enumerate}
  \item Randomly split the data \(\left\{W_{i}\right\}_{i \in[n]}\) into \(S\) folds of equal size, collecting the corresponding indices in the sets \(I_{s}\), for \(s \in[S]\). In practice, \(S=5\) or \(S=10\) are common choices for the number of cross-fitting folds. Let \(\widehat{\eta}(z)=\widehat{\eta}\left(z ;\left\{W_{i}\right\}_{i \in[n]}\right)\) be the researcher's preferred estimator of \(\eta_{0}\), calculated on the full sample; and let \(\widehat{\eta}_{s}(z)=\widehat{\eta}\left(z ;\left\{W_{i}\right\}_{i \in I_{s}^{c}}\right)\), for \(s \in[S]\), be a version of this estimator that only uses data outside the \(s\) th fold.
  \item Estimate \(\tau\) by computing a local linear no covariates RD estimator that uses the adjusted outcome \(M_{i}\left(\widehat{\eta}_{s(i)}\right)=Y_{i}-\widehat{\eta}_{s(i)}\left(Z_{i}\right)\) as the dependent variable, where \(s(i)\) denotes the fold that contains observation \(i\) :
\end{enumerate}

\[
\widehat{\tau}(h ; \widehat{\eta})=\sum_{i=1}^{n} w_{i}(h) M_{i}\left(\widehat{\eta}_{s(i)}\right) .
\]

Our theoretical analysis below shows that under weak conditions the estimator \(\widehat{\tau}(h ; \widehat{\eta})\) is asymptotically equivalent to the infeasible estimator \(\widehat{\tau}(h ; \bar{\eta})=\sum_{i=1}^{n} w_{i}(h) M_{i}(\bar{\eta})\), where \(\bar{\eta}\) is a deterministic approximation of \(\widehat{\eta}\) whose error vanishes in large samples in some appropriate sense. Importantly, our approach does not require the first-stage estimator of \(\eta_{0}\) to be consistent, in the sense that we allow for the possibility that \(\bar{\eta} \neq \eta_{0}\). The first-stage estimator also does not have to converge with a particularly fast rate. In view of (3.3), it then holds that

\[
\widehat{\tau}(h ; \widehat{\eta}) \stackrel{a}{\sim} N\left(\tau+h^{2} B_{\text {base }},(n h)^{-1} V(\bar{\eta})\right) .
\]

As mentioned above, the variance term \(V(\bar{\eta})\) is minimized if \(\bar{\eta}=\eta_{0}\). However, the distributional approximation is also valid if \(\bar{\eta} \neq \eta_{0}\) because the moment condition (3.2) holds for all adjustment functions, and not just the optimal one. In that sense, our procedure is robust to misspecification or over-regularized estimation of the optimal adjustment function. Moreover, we argue that \(V(\bar{\eta})\) is typically smaller than \(V_{\text {base }}\) or \(V_{\text {lin }}\) even if \(\bar{\eta} \neq \eta_{0}\).

We also show that other common steps in an empirical RD analysis can easily be implemented by applying existing methods that are devised for settings without covariates to the generated data set \(\left\{\left(X_{i}, M_{i}\left(\widehat{\eta}_{s(i)}\right)\right)\right\}_{i \in[n]}\). For example, we can construct an estimator of the bandwidth that minimizes the asymptotic mean squared error of \(\widehat{\tau}(h ; \widehat{\eta})\) by using the procedures proposed by Calonico et al. (2014) or Imbens and Kalyanaraman (2012). Similarly, we can generalize the standard error (2.5) and construct a valid nearest-neighbor standard error \(\widehat{\operatorname{se}}(h ; \widehat{\eta})\) as

\[
\widehat{\operatorname{se}}^{2}(h ; \widehat{\eta})=\sum_{i=1}^{n} w_{i}(h)^{2} \widehat{\sigma}_{i}^{2}(\widehat{\eta}), \quad \widehat{\sigma}_{i}^{2}(\widehat{\eta})=\frac{R}{R+1}\left(M_{i}\left(\widehat{\eta}_{s(i)}\right)-\frac{1}{R} \sum_{j \in \mathcal{R}_{i}} M_{j}\left(\widehat{\eta}_{s(j)}\right)\right)^{2},
\]

and construct "robust bias correction" and "bias-aware" confidence intervals as in Calonico et al. (2014) and Armstrong and Kolesár (2020), respectively. To reduce the sensitivity of empirical findings to the particular data split in the cross-fitting step, we can proceed as in Chernozhukov et al. (2018, Section 3.4) by repeating the respective procedure several times and reporting a summary measure of the results, such as the median. We recommend proceeding like this especially when working with smaller sample sizes.\\
3.3. Estimating the Adjustment Function. We now discuss some implementation details for the first-stage estimator of the optimal adjustment function \(\eta_{0}\). Our asymptotic analysis allows for a variety of different methods to be used in this context.

If one wishes to maintain the simplicity of the conventional linear adjustment, one can obtain a "cross-fitted" version of \(\widehat{\tau}_{\text {lin }}(h)\) by setting \(\widehat{\eta}_{s}(z)=z^{\top} \widehat{\gamma}_{s, h}\), for \(s \in\{1, \ldots, S\}\), where \(\widehat{\gamma}_{s, h}\) is the minimizer w.r.t. \(\gamma\) in the minimization problem

\[
\min _{\beta, \gamma} \sum_{i \in I_{s}^{c}} K_{h}\left(X_{i}\right)\left(Y_{i}-S_{i}^{\top} \beta-Z_{i}^{\top} \gamma\right)^{2} .
\]

We refer to this procedure as the cross-fitted "localized" linear adjustment. The adjustment function, however, does not need to be estimated using the kernel weights from the second-stage regression. As an important variant of cross-fitted linear adjustments, we consider \(\widehat{\eta}_{s}(z)=z^{\top} \widehat{\gamma}_{s, \infty}\), where \(\widehat{\gamma}_{s, \infty}\) is obtained via (3.6) with " \(h=\infty\) ", i.e., with equal kernel weights for all observations. Since all the observations outside of fold \(s\) are used to obtain \(\widehat{\gamma}_{s, \infty}\), we refer to this procedure as the cross-fitted "global" linear adjustment. In finite samples, the global version can outperform the localized one in terms of the resulting standard deviation of the RD estimator due to the increased stability of the first-stage estimates. This approach can be naturally extended to other parametric models where the components involving \(S_{i}\) and \(Z_{i}\) are additively separable, and it can be combined with lasso regularization. The cross-fitted post-lasso adjustments are then obtained via (3.6) with the set of covariates restricted to those "selected" by the lasso.

More generally, our approach allows for any parametric, classical nonparametric as well as generic modern machine learning methods. To allow for such generality, we consider adjustment functions of the form

\[
\widehat{\eta}_{s}(z)=\frac{1}{2}\left(\hat{\mu}_{s}^{+}(z)+\widehat{\mu}_{s}^{-}(z)\right), \quad s \in\{1, \ldots, S\},
\]

where \(\widehat{\mu}_{s}^{+}(z)\) and \(\widehat{\mu}_{s}^{-}(z)\) are separate estimators of \(\mu_{0}^{+}(z)=\mathbb{E}\left[Y_{i} \mid X_{i}=0^{+}, Z_{i}=z\right]\) and \(\mu_{0}^{-}(z)=\) \(\mathbb{E}\left[Y_{i} \mid X_{i}=0^{-}, Z_{i}=z\right]\), respectively, using the data outside of fold \(s\). With appropriate subject knowledge, one can then, for example, specify parametric models for \(\mathbb{E}\left[Y_{i} \mid X_{i}=x, Z_{i}=z\right]\). If the number of covariates is small, the functions \(\mu_{0}^{+}\)and \(\mu_{0}^{-}\)can be also estimated using classical nonparametric methods under smoothness conditions, with local polynomial regression being particularly suitable due to its good boundary properties. If the number of covariates is large, however, we recommend using modern machine learning methods, such as lasso or post-lasso regression, random forests, deep\\
neural networks, boosting, or ensemble combinations thereof.\\
One issue to consider is that the default implementations of generic machine learning estimators of \(\mathbb{E}\left[Y_{i} \mid X_{i}=x, Z_{i}=z\right]\) will not automatically produce an estimate with a jump at the cutoff. As having this feature is potentially important in our context, we consider two simple variations of generic machine learning estimators. To define the first type, let

\[
\widehat{\mathbb{E}}_{s}\left[Y_{i} \mid T_{i}=t, X_{i}=x, Z_{i}=z\right]=\underset{f \in \mathcal{F}}{\operatorname{argmin}} \sum_{i \in I_{s}^{c}} l\left(Y_{i}, f\left(T_{i}, X_{i}, Z_{i}\right)\right)
\]

be a generic machine learning estimator of \(\mathbb{E}\left[Y_{i} \mid T_{i}=t, X_{i}=x, Z_{i}=z\right]\), computed by minimizing some empirical loss function \(L(f)=\sum_{i \in I_{s}^{c}} l\left(Y_{i}, f\left(T_{i}, X_{i}, Z_{i}\right)\right)\) over a set of candidate functions \(\mathcal{F}\). We can then estimate \(\mu^{+}(z)\) by \(\widehat{\mathbb{E}}_{s}\left[Y_{i} \mid T_{i}=1, X_{i}=0, Z_{i}=z\right]\) and \(\mu^{-}(z)\) by \(\widehat{\mathbb{E}}_{s}\left[Y_{i} \mid T_{i}=0, X_{i}=0, Z_{i}=z\right]\). Here including the seemingly superfluous treatment indicator \(T_{i}=\mathbf{1}\left\{X_{i} \geq 0\right\}\) as a predictor allows the machine learner to create the "jump" in the estimated function at the cutoff value. We refer to this type of implementation as "global", as it uses all available observations.

To define the second type of implementation of machine learning we consider in this paper, let

\[
\widehat{\mathbb{E}}_{s}\left[Y_{i} \mid T_{i}=t, Z_{i}=z\right]=\underset{f \in \mathcal{F}}{\operatorname{argmin}} \sum_{i \in I_{s}^{c}} K\left(X_{i} / b\right) l\left(Y_{i}, f\left(T_{i}, Z_{i}\right)\right)
\]

be a generic machine learning estimator of \(\mathbb{E}\left[Y_{i} \mid T_{i}=t, Z_{i}=z\right]\), where \(b>0\) is some positive bandwidth and \(K\) is again a kernel function. We can then estimate \(\mu^{+}(z)\) as \(\widehat{\mathbb{E}}_{s}\left[Y_{i} \mid T_{i}=1, Z_{i}=z\right]\) and \(\mu^{-}(z)\) as \(\widehat{\mathbb{E}}_{s}\left[Y_{i} \mid T_{i}=0, Z_{i}=z\right]\). We refer to this type of implementation as "localized", as it effectively only uses data points whose realization of the running variable is close to the cutoff. The idea is to produce an estimate with small empirical loss in the relevant area around the cutoff rather than one with small "overall" loss. The downside of this approach is the reduced effective sample size and the need to choose the tuning parameter \(b .{ }^{5}\)\\
3.4. Our Proposed Flexible Adjustment. The specific flexible covariate adjustment that we propose and implement in our empirical analysis and simulations is an ensemble of the following methods: (i) linear regression; (ii) post-lasso; (iii) boosted trees; and (iv) random forest. All four methods are implemented in localized and global versions discussed above. We use the cross-fitted linear and post-lasso adjustments specified in the discussion following (3.6), and the boosted trees and random forest adjustments are based on the formulations in (3.7) and (3.8). Our proposed

\footnotetext{\({ }^{5}\) The choice of \(b\) involves a bias-variance trade-off similar to the one encountered in classical nonparametric kernel regression problems. We are not aware of generic theoretical results for such estimators in settings with \(b \rightarrow 0\) as \(n \rightarrow \infty\). Specific results are given by Su et al. (2019) for the lasso, and by Colangelo and Lee (2022) for series estimators and deep neural networks. In our applications below, we simply use \(b=h\). To make this simultaneous choice feasible, we use an iterative procedure. We first choose a reasonable preliminary first-stage bandwidth, like the one that would be optimal for RD estimation without covariates, and generate preliminary versions of the adjustment terms as described above. Next, we use the preliminary covariate-adjusted outcomes to pick an optimized second-stage bandwidth. Finally, we rerun both stages with this last bandwidth to obtain our empirical results.
}
flexible covariate adjustment is a convex combination of these eight adjustment functions and the trivial no-adjustment function that minimizes the mean squared error for predicting the outcome close to the cutoff. Specifically, we employ the super learning approach of Van der Laan et al. (2007), where the optimal weights are chosen via cross-validation. \({ }^{6}\)

\section*{4. PRACTICAL PERFORMANCE}
Between 2018 and 2023, the main AEA journals for applied microeconomic research published 16 empirical RD studies that use covariates, fit into our general framework, and have directly available public replication data. To illustrate the scope for efficiency improvements that flexible covariate adjustments can achieve in practically relevant settings, we compare the performance of our proposed method to that of existing ones on the 56 main empirical specifications considered in these papers. Specifically, we compute the length of bias-aware \(95 \%\) confidence intervals (as described in Section 6.1) for the respective RD parameter in each specification based on local linear estimators that use our flexible covariate adjustments, linear adjustments, and no covariate adjustments, respectively. See Appendix C for further details on the implementation and the data collection process, and Table S2 in the Online Supplement for the complete list of papers and specifications used.

Before turning to the results of this exercise, we want to comment briefly on the empirical relevance of concerns about the bias of the usual standard error of the conventional linear adjustment estimator in settings with rather many covariates relative to the effective sample size mentioned in Section 3. From Table S2, we can see that the ratio of the effective sample size to the number of covariates ranges from 2.7 to 23,254 , with a median value of 112 , across the 56 specifications under consideration in this section. In about one-fifth of the specifications, the ratio falls below \(20 .{ }^{7}\) Empirical RD specifications with rather high-dimensional covariate adjustments are thus not uncommon in the recent literature. To take this into account, we use cross-fitting with both flexible and linear adjustments in this section.

Turning to the results of our empirical exercise, Figure 1 shows the distribution of the ratio of confidence interval lengths for flexible adjustments relative to no adjustments in its left panel, and for flexible adjustments relative to linear adjustments in its right panel. From the left panel, we can see that in about half of our specifications the flexible covariate adjustments yield confidence

\footnotetext{\({ }^{6}\) We implemented our procedure in the R programming language. The boosted trees adjustments are obtained using the package xgboost with trees of depth 2 and shrinkage rate 0.1 . The number of boosting iterations is chosen via cross-validation with a maximum of 1000 iterations, separately for the localized and global versions. The random forest with 1000 trees is implemented using the package ranger with the minimal node size set to the maximum of 10 and \(0.1 \%\) of the sample size. All other parameters are set to the default values in the respective packages. For post-lasso estimation, we use the function rlasso from the package hdm with a data-driven penalty parameter. We use the package SuperLearner to choose the optimal weights via cross-validation.\\
\({ }^{7}\) In separate simulations in Section 7, we show that the conventional standard error of the linear adjustment estimator exhibits a moderate \(10 \%\) downward bias if the ratio of effective sample size and the number of covariates is around 20 , and a substantial bias of more than \(20 \%\) if that ratio falls below a value around 8 , and that cross-fitting can remove this bias almost completely.
}
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-14}

Figure 1: CI lengths with flexible covariate adjustment relative to CI length with no covariates and with cross-fitted localized linear adjustment for all specifications of the empirical literature survey. We choose the bandwidth and conduct inference using the bias-aware approach with smoothness constants calibrated via the rule of thumb of Armstrong and Kolesár (2020) for each of the specification separately. The effective sample size refers to the no covariates estimator of the respective specification.\\
intervals that are not noticeably shorter than the ones obtained without covariate adjustments. Given the flexibility of our methods, this suggests that the covariates are not very informative about the outcome in these specifications, and that there is hence no scope for efficiency gains. In many specifications, however, flexible covariate adjustments lead to substantially shorter confidence intervals, with the biggest reduction being greater than \(35 \%\). To put this into perspective, note that one would have to increase the sample size used by an unadjusted "baseline" RD estimator by a factor of about 3 to receive a similar reduction in the length of the confidence interval. From the right panel of Figure 1, we can see that linear adjustments are typically unable to exhaust the available covariate information. Indeed, the confidence intervals based on flexible adjustments can be substantially shorter, with the biggest reduction amounting to \(21 \%\) in our empirical exercise.

\section*{5. MAIN THEORETICAL RESULTS}
5.1. Assumptions. We study the theoretical properties of our proposed estimator under a number of conditions that are either standard in the RD literature, or concern the general properties of the first-stage estimator \(\widehat{\eta}\). To describe them, we denote the support of \(Z_{i}\) by \(\mathcal{Z}\), and the support of \(X_{i}\) by \(\mathcal{X}\). We write \(\mathcal{X}_{h}=\mathcal{X} \cap[-h, h]\), and \(\mathcal{Z}_{h}\) denotes the support of \(Z_{i}\) given \(X_{i} \in \mathcal{X}_{h}\). We also define the following class of admissible adjustment functions:

\[
\mathcal{E}=\left\{\eta: \mathbb{E}\left[\eta\left(Z_{i}\right) \mid X_{i}=x\right] \text { exists and is twice continuously differentiable around the cutoff }\right\} .
\]

The class \(\mathcal{E}\) implicitly depends on the underlying conditional distribution of the covariates given the running variable. If this conditional distribution changes smoothly around the cutoff, the class \(\mathcal{E}\) contains essentially all functions of the covariates, subject only to technical integrability conditions. \({ }^{8}\)

Assumption 1. For all \(n \in \mathbb{N}\), there exist a set \(\mathcal{T}_{n} \subset \mathcal{E}\) and a function \(\bar{\eta} \in \mathcal{T}_{n}\) such that: (i) \(\widehat{\eta}_{s}\) belongs to \(\mathcal{T}_{n}\) with probability approaching 1 for all \(s \in[S]\); (ii) it holds that:

\[
\sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \mathcal{X}_{h}} \mathbb{E}\left[\left(\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2} \mid X_{i}=x\right]=O\left(r_{n}^{2}\right)
\]

for some deterministic sequence \(r_{n}=o(1)\).\\
Assumption 1 states that with high probability the first-stage estimator belongs to some realization set \(\mathcal{T}_{n} \subset \mathcal{E}\). As discussed above, this requirement seems weak as we generally expect the class \(\mathcal{E}\) to be very large. The assumption also states that the sets \(\mathcal{T}_{n}\) contract around a deterministic sequence of functions in a particular \(L_{2}\)-type sense. Note that taking the supremum in Assumption 1 over \(\mathcal{X}_{h}\) instead of \(\mathcal{X}\) suffices as the properties of the first stage estimator are only relevant for observations with non-zero kernel weights in the second-stage local linear regression. The assumption does not impose any restrictions on the speed at which \(\widehat{\eta}\) concentrates around \(\bar{\eta}\). It also allows the function \(\bar{\eta}\) to be different from the target function \(\eta_{0}\), which means that \(\widehat{\eta}\) can be inconsistent for \(\eta_{0}\).

Mean-square error consistency as prescribed in Assumption 1 follows under classical conditions for the parametric and nonparametric procedures for settings in which the number of covariates is fixed. For the "localized" versions of the machine learning methods described in Section 3.3, existing results imply that for fixed \(b>0\) and \(K\) the uniform kernel,

\[
\sup _{\eta \in \mathcal{T}_{n}} \mathbb{E}\left[\left(\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2} \mid X_{i} \in(-b, b)\right]=O\left(r_{n}^{2}\right),
\]

with \(\bar{\eta}(z)=\left(\mathbb{E}\left[Y_{i} \mid X_{i} \in(-b, 0), Z_{i}=z\right]+\mathbb{E}\left[Y_{i} \mid X_{i} \in(0, b), Z_{i}=z\right]\right) / 2\) and some \(r_{n}=o(1)\), under general conditions. For example, if \(\bar{\eta}(z)\) is contained in a Hölder class of order \(s\), then (5.1) can hold with \(r_{n}^{2}=n^{-2 s /(2 s+d)}\) for estimators that exploit smoothness. If \(\bar{\eta}(z)\) is \(s\)-sparse, then (5.1) can hold with \(r_{n}^{2}=s \log (d) / n\) for estimators that exploit sparsity. Assumption 1 then follows from (5.1) if the conditional distribution of the covariates does not change "too quickly" when moving away from the cutoff. For example, if the covariates are continuously distributed conditional on the running variable, having that

\[
\sup _{x \in \mathcal{X}_{h}} \sup _{z \in \mathcal{Z}_{h}} \frac{f_{Z \mid X}(z \mid x)}{f_{Z \mid X \in(-b, b)}(z)}<C,
\]

for some constant \(C\) and all \(n\) sufficiently large, suffices. Similar conditions can be given for discrete conditional covariate distributions, or intermediate cases. If \(\mathbb{E}\left[Y_{i} \mid X_{i}=x, Z_{i}=z\right]\) is sufficiently

\footnotetext{\({ }^{8}\) For example, if the conditional distribution of \(Z_{i}\) given \(X_{i}\) admits a density \(f_{Z \mid X}(z \mid x)\) that is twice continuously differentiable in \(x\) and \(\left|\partial_{x}^{j} f_{Z \mid X}(z \mid x)\right| \leq g_{j}(z)\) for all \(x\) in a neighborhood of the cutoff, some integrable functions \(g_{j}\), and \(j \in\{0,1,2\}\), then \(\mathcal{E}\) contains all bounded Borel functions. The class \(\mathcal{E}\) also contains all polynomials if the corresponding conditional moments of \(Z_{i}\) exist and are twice continuously differentiable.
}
smooth in \(x\) on both sides of the cutoff, we can also expect that \(\bar{\eta}\) is "close" to \(\eta_{0}\) for "small" values of \(b\). Formal rate results with \(b \rightarrow 0\) are given by Su et al. (2019) for the Lasso, and by Colangelo and Lee (2022) for series estimators and deep neural networks.

Assumption 2. For \(j \in\{1,2\}\), it holds that:

\[
\sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \mathcal{X}_{h} \backslash\{0\}}\left|\partial_{x}^{j} \mathbb{E}\left[\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i}=x\right]\right|=O\left(v_{j, n}\right) .
\]

for some deterministic sequences \(v_{j, n}=o(1)\).\\
Assumption 2 also concerns the first-stage estimator, and requires the first and second derivatives of \(\mathbb{E}\left[\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i}=x\right]\) to be close to zero in large samples for all \(\eta \in \mathcal{T}_{n}\). We generally expect this condition to hold with \(v_{1, n}=v_{2, n}=r_{n}\), where \(r_{n}\) is as in Assumption 1. \({ }^{9}\)

Assumption 3. \(X_{i}\) is continuously distributed with density \(f_{X}\), which is continuous and bounded away from zero over an open neighborhood of the cutoff.

Assumption 3 is a standard condition from the RD literature. Continuity of the running variable's density \(f_{X}\) around the cutoff is strictly speaking not required for an RD analysis. However, a discontinuity in \(f_{X}\) is typically considered to be an indication of a design failure that prevents \(\tau\) from being interpreted as a causal parameter (McCrary, 2008; Gerard et al., 2020). For this reason, we focus on the case of a continuous running variable density in this paper.

Assumption 4. (i) The kernel function \(K\) is a bounded and symmetric density function that is continuous on its support, and equal to zero outside some compact set, say \([-1,1]\); (ii) The bandwidth satisfies \(h \rightarrow 0\) and \(n h \rightarrow \infty\) as \(n \rightarrow \infty\).

The conditions on the kernel and the bandwidth that are imposed in Assumption 4 are standard in the RD literature.

Assumption 5. There exist constants \(C\) and \(L\) such that the following conditions hold for all \(n \in \mathbb{N}\). (i) \(\mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\) is twice continuously differentiable on \(\mathcal{X} \backslash\{0\}\) with L-Lipschitz continuous second derivative bounded by \(C\); (ii) For all \(x \in \mathcal{X}\) and some \(q>2 \mathbb{E}\left[\left(M_{i}(\bar{\eta})-\mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}\right]\right)^{q} \mid X_{i}=x\right]\) exists and is bounded by \(C\); (iii) \(\mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\) is L-Lipschitz continuous and bounded from below by \(1 / C\) for all \(x \in \mathcal{X} \backslash\{0\}\).

\footnotetext{\({ }^{9}\) For example, this can easily be seen to be the case if \(\widehat{\eta}\) converges to \(\bar{\eta}\) uniformly over \(\mathcal{Z}\) with rate \(r_{n}\) and the smoothness conditions for \(f_{Z \mid X}(z \mid x)\) given in footnote 8 hold. Similarly, under regularity conditions on \(\mathbb{E}\left[Z_{i} \mid X_{i}=x\right]\), these three rates coincide if \(\mathcal{T}_{n}\) contains only linear functions. Without any additional restrictions on first stage estimators or \(\mathcal{T}_{n}\), except that it contains only bounded functions, Assumption 2 also follows from Assumption 1, again with \(v_{1, n}=v_{2, n}=r_{n}\), under restrictions concerning solely the conditional density \(f_{Z \mid X}(z \mid x)\). Specifically, it suffices that \(\mathbb{E}\left[\left(\partial_{x}^{j} f_{Z \mid X}\left(Z_{i} \mid x\right) / f_{Z \mid X}\left(Z_{i} \mid x\right)\right)^{2} \mid X_{i}=x\right]\) is bounded for \(j \in\{1,2\}\) uniformly in \(x\) and the conditions from footnote 8 hold.
}Assumption 5 collects standard conditions for an RD analysis with \(M_{i}(\bar{\eta})\) as the outcome variable. Part (i) imposes smoothness conditions on \(\mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\), and parts (ii) and (iii) impose restrictions on conditional moments of the outcome variable. Throughout, we use constants \(C\) and \(L\) independent of the sample size to ensure asymptotic normality of the infeasible estimator \(\widehat{\tau}(h ; \bar{\eta})\) even in settings where the distribution of the data, and thus \(\bar{\eta}\), might change with \(n\).\\
5.2. Asymptotic Properties. We give four main results in this subsection. The first shows that our proposed estimator \(\widehat{\tau}(h ; \widehat{\eta})\) is asymptotically equivalent to an infeasible analog \(\widehat{\tau}(h ; \bar{\eta})\) that replaces the estimator \(\widehat{\eta}\) with the deterministic sequence \(\bar{\eta}\); the second shows the asymptotic normality of the estimator; the third characterizes how the asymptotic variance changes with the adjustment function and shows that \(\eta_{0}\) is indeed the optimal adjustment; and the fourth shows the impact of flexible covariate adjustments on the optimal bandwidth and the corresponding mean squared error.

Theorem 1. Suppose that Assumptions 1-4 hold. Then

\[
\widehat{\tau}(h ; \widehat{\eta})=\widehat{\tau}(h ; \bar{\eta})+O_{P}\left(r_{n}(n h)^{-1 / 2}+v_{1, n} h(n h)^{-1 / 2}+v_{2, n} h^{2}\right) .
\]

Theorem 1 is easiest to interpret in what is arguably the standard case that \(v_{1, n}=v_{2, n}=r_{n}\), in which it holds that

\[
\widehat{\tau}(h ; \widehat{\eta})=\widehat{\tau}(h ; \bar{\eta})+O_{P}\left(r_{n}\left(h^{2}+(n h)^{-1 / 2}\right)\right)=\widehat{\tau}(h ; \bar{\eta})+O_{P}\left(r_{n}|\widehat{\tau}(h ; \bar{\eta})-\tau|\right) .
\]

The accuracy of the approximation that \(\widehat{\tau}(h ; \widehat{\eta}) \approx \widehat{\tau}(h ; \bar{\eta})\) thus increases with the rate at which \(\widehat{\eta}\) concentrates around \(\bar{\eta}\), but first-order asymptotic equivalence holds even if the first-stage estimator converges arbitrarily slowly. This insensitivity of \(\widehat{\tau}(h ; \widehat{\eta})\) to sampling variation in \(\widehat{\eta}\) occurs because \(\widehat{\tau}(h ; \widehat{\eta})\) is based on the moment condition

\[
\tau=\mathbb{E}\left[M_{i}(\eta) \mid X_{i}=0^{+}\right]-\mathbb{E}\left[M_{i}(\eta) \mid X_{i}=0^{-}\right],
\]

which is insensitive to variation in \(\eta\). Moment conditions with a local form of insensitivity with respect to a nuisance function, often called Neyman orthogonality, are used extensively in the recent literature on two-stage estimators that use machine learning in the first stage (e.g. Belloni et al., 2017; Chernozhukov et al., 2018). The global insensitivity that arises in our RD setup is stronger, and allows us to work with weaker conditions on the first-stage estimates than those used in papers that work with Neyman orthogonality. Similarly, globally insensitive moment function exists, for example, in certain types of randomized experiments, and our proposed estimator is in many ways analogous to efficient estimators in such setups; see Section 6.2 for further discussion.

Theorem 2. Suppose that Assumptions 1-5 hold. Then

\[
\sqrt{n h} V(\bar{\eta})^{-1 / 2}\left(\widehat{\tau}(h ; \widehat{\eta})-\tau-h^{2} B_{n}\right) \xrightarrow{d} \mathcal{N}(0,1),
\]

for some \(B_{n}=B_{\text {base }}+o_{P}(1)\), where \(B_{\text {base }}\) and \(V(\cdot)\) are as defined in Sections 2.2 and 3.1, respectively.\\
Theorem 2 follows from Theorem 1 under the additional regularity conditions of Assumption 5. It shows that our estimator is asymptotically normal, gives explicit expressions for its asymptotic bias and variance, and justifies the distributional approximation given in Section 3.2.

Theorem 3. Suppose \(\mathbb{E}\left[Y_{i}^{2} \mid X_{i}=x\right]\) is uniformly bounded in \(x\), the limit \(\mathbb{V}\left[Y_{i}-\mu_{0}^{\star}\left(Z_{i}\right) \mid X_{i}=0^{\star}\right]\) exists for \(\star \in\{+,-\}\), and \(\eta_{0} \in \mathcal{V}\), where the function class \(\mathcal{V}\) is defined as

\[
\mathcal{V} \equiv\left\{\eta: \mathbb{V}\left[\eta\left(Z_{i}\right) \mid X=x\right] \text { and } \operatorname{Cov}\left[\eta\left(Z_{i}\right), \mu_{0}^{\star}\left(Z_{i}\right) \mid X_{i}=x\right] \text { are continuous for } \star \in\{+,-\}\right\} .
\]

Then, for any \(\eta^{(a)}, \eta^{(b)} \in \mathcal{V}\),

\[
V\left(\eta^{(a)}\right)-V\left(\eta^{(b)}\right)=2 \frac{\bar{\kappa}}{f_{X}(0)}\left(\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-\eta^{(a)}\left(Z_{i}\right) \mid X_{i}=0\right]-\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-\eta^{(b)}\left(Z_{i}\right) \mid X_{i}=0\right]\right) .
\]

Theorem 3 introduces a function class \(\mathcal{V}\) that, similarly to the class \(\mathcal{E}\) above, enforces some technical integrability conditions. The theorem shows that \(V\left(\eta^{(a)}\right)<V\left(\eta^{(b)}\right)\) for generic adjustment functions \(\eta^{(a)}\) and \(\eta^{(b)}\) if and only if \(\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-\eta^{(a)}\left(Z_{i}\right) \mid X_{i}=0\right]<\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-\eta^{(b)}\left(Z_{i}\right) \mid X_{i}=0\right]\). That is, the "closer" (in a particular \(L_{2}\)-sense) the adjustment function is to the optimal one, the smaller the asymptotic variance. In consequence, the lowest possible value of \(V(\bar{\eta})\) is achieved for \(\bar{\eta}=\eta_{0}\). Even if \(\bar{\eta} \neq \eta_{0}\), our flexible covariate adjustments typically still have smaller asymptotic variances than the no covariates and linear adjustment RD estimators. Specifically, \(V(\bar{\eta})<V_{\text {base }}\) if and only if \(\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i}=0\right]<\mathbb{V}\left[\eta_{0}\left(Z_{i}\right) \mid X_{i}=0\right]\), i.e. whenever \(\bar{\eta}\left(Z_{i}\right)\) captures some of the variance of \(\eta_{0}\left(Z_{i}\right)\) among units near the cutoff; and \(V(\bar{\eta})<V_{\text {lin }}\) if and only if \(\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i}=\right.\) \(0]<\mathbb{V}\left[\eta_{0}\left(Z_{i}\right)-Z_{i}^{\top} \gamma_{0} \mid X_{i}=0\right]\), i.e. whenever \(\bar{\eta}\) is "closer" to \(\eta_{0}\) in our particular \(L_{2}\)-sense than the population linear adjustment function.

Theorem 4. Let \(\operatorname{AMSE}(h, \eta)=h^{4} B_{\text {base }}^{2}+(n h)^{-1} V(\eta)\) be the approximate (first-order) mean squared error of \(\widehat{\tau}(h ; \eta)\), and \(h_{A M S E}(\eta)=\operatorname{argmin}_{h} \operatorname{AMSE}(h, \eta)=n^{-1 / 5}\left(V(\eta) / 4 B_{\text {base }}^{2}\right)^{1 / 5}\) the corresponding optimal bandwidth. Then for any pair of adjustment functions \(\eta^{(a)}, \eta^{(b)} \in \mathcal{V}\) we have

\[
\frac{h_{A M S E}\left(\eta^{(a)}\right)}{h_{A M S E}\left(\eta^{(b)}\right)}=\left(\frac{v\left(\eta^{(a)}\right)}{v\left(\eta^{(b)}\right)}\right)^{1 / 5} \quad \text { and } \quad \frac{\operatorname{AMSE}\left(h_{A M S E}\left(\eta^{(a)}\right), \eta^{(a)}\right)}{\operatorname{AMSE}\left(h_{A M S E}\left(\eta^{(b)}\right), \eta^{(b)}\right)}=\left(\frac{v\left(\eta^{(a)}\right)}{v\left(\eta^{(b)}\right)}\right)^{4 / 5},
\]

where \(v(\eta)=\mathbb{V}\left[M_{i}(\eta) \mid X_{i}=0^{+}\right]+\mathbb{V}\left[M_{i}(\eta) \mid X_{i}=0^{-}\right]\).\\
Theorem 4 implies that flexible covariate adjustments can reduce the (approximate) mean squared error of our estimator not only directly through a smaller asymptotic variance term but also indirectly through a change in the optimal bandwidth and a corresponding reduction in bias. That is, if \(V\left(\eta^{(a)}\right)<V\left(\eta^{(b)}\right)\) for generic adjustment functions \(\eta^{(a)}\) and \(\eta^{(b)}\), then the optimal bandwidth \(h_{A M S E}\left(\eta^{(a)}\right)\) is smaller than \(h_{A M S E}\left(\eta^{(b)}\right)\), and the corresponding estimator \(\widehat{\tau}\left(h_{A M S E}\left(\eta^{(a)}\right) ; \eta^{(a)}\right)\) has both smaller asymptotic bias and smaller asymptotic variance than \(\widehat{\tau}\left(h_{\text {AMSE }}\left(\eta^{(b)}\right) ; \eta^{(b)}\right)\).

\section*{6. ADDITIONAL THEORETICAL RESULTS AND DISCUSSIONS}
6.1. Bandwidth Choice and Inference. We formally show in Appendix B that standard methods for bandwidth choice and confidence interval construction based on the no covariates RD estimator maintain their general asymptotic properties when they are applied to the generated data set \(\left\{\left(X_{i}, M_{i}\left(\widehat{\eta}_{s(i)}\right)\right)\right\}_{i \in[n]}\) without any adjustment for the sampling uncertainty about the estimated adjustment function. Specifically, we derive three groups of results, all under conditions that are rather weak and analogous to those commonly imposed in setups without covariates.

First, we show that the nearest neighbor standard error (3.5) is consistent, in the sense that

\[
n h \widehat{\mathrm{se}}^{2}(h ; \widehat{\eta}) / V(\bar{\eta}) \xrightarrow{p} 1
\]

Second, we show that commonly used methods for confidence interval construction achieve correct asymptotic coverage. For example, assuming a bound on \(\left|\partial_{x}^{2} \mathbb{E}\left[Y_{i} \mid X_{i}=x\right]\right|\), the absolute value of the second derivative of the conditional expectation of the outcome given the running variable, one can construct a "bias-aware" confidence interval as in Armstrong and Kolesár (2020) as

\[
C I_{1-\alpha}^{b a}=\left[\widehat{\tau}(h ; \widehat{\eta}) \pm z_{\alpha}(\bar{b}(h) / \widehat{\operatorname{se}}(h ; \widehat{\eta})) \widehat{\operatorname{se}}(h ; \widehat{\eta})\right]
\]

Here \(z_{\alpha}(r)\) is the \(1-\alpha / 2\) quantile of \(|N(r, 1)|\), the absolute value of the normal distribution with mean \(r\) and variance one and \(\bar{b}(h)\) is an explicit bound on the finite sample bias of the no covariates RD estimator given in the Appendix. Alternatively, one can also construct a "robust bias correction" confidence interval as in Calonico et al. (2014) by subtracting a local quadratic estimate of the first-order bias of \(\widehat{\tau}(h ; \widehat{\eta})\) from the estimator, and adjusting the standard error appropriately. This yields a confidence interval of the form

\[
C I_{1-\alpha}^{r b c}=\left[\widehat{\tau}^{r b c}(h ; \widehat{\eta}) \pm z_{\alpha} \widehat{\mathrm{se}}^{r b c}(h ; \widehat{\eta})\right]
\]

where \(z_{\alpha}=z_{\alpha}(0)\) and the other terms are formally defined in the appendix. Third, we show that the "MSE-optimal" bandwidth selector \(\widehat{h}_{n}\) of Calonico et al. (2014), which is similar to that of Imbens and Kalyanaraman (2012), consistently estimates the AMSE optimal bandwidth \(h_{A M S E}(\bar{\eta})\) defined in Theorem 4, in the sense that

\[
\widehat{h}_{n} / h_{A M S E}(\bar{\eta}) \xrightarrow{p} 1
\]

RD estimation and inference with flexible covariate adjustments are thus easy to implement with existing software packages.\\
6.2. Analogies with Randomized Experiments. The results in Section 5 are qualitatively similar to ones obtained for efficient influence function (EIF) estimators of the population average treatment effect (PATE) in randomized experiments with known and constant propensity scores (e.g., Wager et al., 2016; Chernozhukov et al., 2018). To see this, consider a randomized experiment\\
with unconfounded treatment assignment and a known and constant propensity score \(p\). Using our notation in an analogous fashion, the EIF of the PATE in such a setup is typically given in the literature (e.g., Hahn, 1998) in the form

\[
\psi_{i}\left(m_{0}^{0}, m_{0}^{1}\right)=m_{0}^{1}\left(Z_{i}\right)-m_{0}^{0}\left(Z_{i}\right)+\frac{T_{i}\left(Y_{i}-m_{0}^{1}\left(Z_{i}\right)\right)}{p}-\frac{\left(1-T_{i}\right)\left(Y_{i}-m_{0}^{0}\left(Z_{i}\right)\right)}{1-p},
\]

where \(m_{0}^{t}(z)=\mathbb{E}\left[Y_{i} \mid Z_{i}=z, T_{i}=t\right]\) for \(t \in\{0,1\}\). The minimum variance any regular estimator of the PATE can achieve is thus \(V_{\text {PATE }}=\mathbb{V}\left(\psi_{i}\left(m_{0}^{0}, m_{0}^{1}\right)\right)\). By randomization, it also holds that \(\tau_{\text {PATE }}=\mathbb{E}\left[\psi_{i}\left(m^{0}, m^{1}\right)\right]\) for all (suitably integrable) functions \(m^{0}\) and \(m^{1}\), and thus the PATE is identified by a moment function that satisfies a global invariance property. A sample analog estimator of \(\tau_{\text {PATE }}\) based on this moment function reaches has asymptotic variance \(V_{\text {PATE }}\) if \(\widehat{m}^{t}\) is a consistent estimator of \(m_{0}^{t}\) for \(t \in\{0,1\}\), but remains consistent and asymptotically normal with asymptotic variance \(\mathbb{V}\left(\psi_{i}\left(\bar{m}^{0}, \bar{m}^{1}\right)\right)\) if \(\widehat{m}^{t}\) is consistent for some other function \(\bar{m}^{t}, t \in\{0,1\}\). The convergence of \(\widehat{m}^{t}\) to \(\bar{m}^{t}\) can be arbitrarily slow for these results (e.g. Wager et al., 2016; Chernozhukov et al., 2018).

The qualitative parallels between these findings and ours in Section 5 arise because our covariateadjusted RD estimator is in many ways a direct analog of such EIF estimators. To show this, write \(m(z)=(1-p) m^{1}(z)+p m^{0}(z)\) for any two functions \(m^{0}\) and \(m^{1}\), so that \(m_{0}(z)=(1-p) m_{0}^{1}(z)+\) \(p m_{0}^{0}(z)\). The PATE's influence function can then be expressed as

\[
\psi_{i}\left(m_{0}^{0}, m_{0}^{1}\right)=\frac{T_{i}\left(Y_{i}-m_{0}\left(Z_{i}\right)\right)}{p}-\frac{\left(1-T_{i}\right)\left(Y_{i}-m_{0}\left(Z_{i}\right)\right)}{1-p},
\]

and it holds that

\[
\mathbb{E}\left[\psi_{i}\left(m^{0}, m^{1}\right)\right]=\mathbb{E}\left[Y_{i}-m\left(Z_{i}\right) \mid T_{i}=1\right]-\mathbb{E}\left[Y_{i}-m\left(Z_{i}\right) \mid T_{i}=0\right],
\]

which is the difference in average covariate-adjusted outcomes between treated and untreated units. This last equation is fully analogous to our equation (3.2), with \(p=1 / 2\), and conditioning on \(T_{i}=1\) and \(T_{i}=0\) replaced by conditioning on \(X_{i}\) in infinitesimal right and left neighborhoods of the cutoff (the value \(p=1 / 2\) is appropriate here because continuity of the running variable's density implies that an equal share of units close to the cutoff can be found on either side). An EIF estimator of \(\tau_{\text {PAte }}\) is thus analogous to our estimator \(\widehat{\tau}(h ; \widehat{\eta})\), as they are both sample analogs of a moment function with the same basic properties.\\
6.3. Fuzzy RD Designs. In fuzzy RD designs, units are assigned to treatment if their realization of the running variable falls above the threshold value, but might not comply with their assignment. The conditional treatment probability given the running variable hence changes discontinuously at the cutoff, but in contrast to sharp RD designs it does not jump from zero to one. The parameter of\\
interest in fuzzy RD designs is

\[
\theta=\frac{\tau_{Y}}{\tau_{T}} \equiv \frac{\mathbb{E}\left[Y_{i} \mid X_{i}=0^{+}\right]-\mathbb{E}\left[Y_{i} \mid X_{i}=0^{-}\right]}{\mathbb{E}\left[T_{i} \mid X_{i}=0^{+}\right]-\mathbb{E}\left[T_{i} \mid X_{i}=0^{-}\right]},
\]

which is the ratio of two sharp RD estimands (throughout this subsection, the notation is analogous to that used before, with the subscripts \(Y\) and \(T\) referencing the respective outcome variable). Under standard conditions (Hahn et al., 2001; Dong, 2018), one can interpret \(\theta\) as the average causal effect of the treatment among units at the cutoff whose treatment decision is affected by whether their value of the running variable is above or below the cutoff.

Similarly to sharp RD designs, predetermined covariates can be used in fuzzy RD designs to improve efficiency. Building on our proposed method, we consider estimating \(\theta\) by the ratio of two generic flexible covariate-adjusted sharp RD estimators:

\[
\widehat{\theta}\left(h ; \widehat{\eta}_{Y}, \widehat{\eta}_{T}\right)=\frac{\widehat{\tau}_{Y}\left(h ; \widehat{\eta}_{Y}\right)}{\widehat{\tau}_{T}\left(h ; \widehat{\eta}_{T}\right)}=\frac{\sum_{i=1}^{n} w_{i}(h)\left(Y_{i}-\widehat{\eta}_{Y, s(i)}\left(Z_{i}\right)\right)}{\sum_{i=1}^{n} w_{i}(h)\left(T_{i}-\widehat{\eta}_{T, s(i)}\left(Z_{i}\right)\right)} .
\]

Proposition 1. Suppose that Assumptions 1-5 hold also with \(T_{i}\) replacing \(Y_{i}\), mutatis mutandis.\\
(i) It holds that

\[
\sqrt{n h} V_{\theta}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right)^{-1 / 2}\left(\widehat{\theta}\left(h ; \widehat{\eta}_{Y}, \widehat{\eta}_{T}\right)-\theta-B_{\theta}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right) h^{2}\right) \xrightarrow{d} \mathcal{N}(0,1),
\]

where

\[
\begin{aligned}
B_{\theta}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right) & =\frac{\bar{\nu}}{2 \tau_{T}}\left(\left.\partial_{x}^{2} \mathbb{E}\left[Y_{i}-\theta T_{i} \mid X_{i}=x\right]\right|_{x=0^{+}}-\left.\partial_{x}^{2} \mathbb{E}\left[Y_{i}-\theta T_{i} \mid X_{i}=x\right]\right|_{x=0^{-}}\right)+o_{P}(1), \\
V_{\theta}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right) & =\frac{\bar{\kappa}}{f_{X}(0)}\left(\mathbb{V}\left[U_{i}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right) \mid X_{i}=0^{+}\right]+\mathbb{V}\left[U_{i}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right) \mid X_{i}=0^{-}\right]\right), \\
\text {and } U_{i}\left(\bar{\eta}_{Y}, \bar{\eta}_{T}\right) & =\left(Y_{i}-\theta T_{i}-\left(\bar{\eta}_{Y}\left(Z_{i}\right)-\theta \bar{\eta}_{T}\left(Z_{i}\right)\right)\right) / \tau_{T} .
\end{aligned}
\]

(ii) Suppose additionally that the assumptions of Theorem 3 hold, mutatis mutandis, also with \(T_{i}\) replacing \(Y_{i}\) and the definition of \(\mathcal{V}\) adjusted accordingly. Then, for any \(\eta_{Y}^{(a)}, \eta_{Y}^{(b)}, \eta_{T}^{(a)}, \eta_{T}^{(b)} \in \mathcal{V}\), it holds that

\[
\begin{aligned}
& V_{\theta}\left(\eta_{Y}^{(a)}, \eta_{T}^{(a)}\right)-V_{\theta}\left(\eta_{Y}^{(b)}, \eta_{T}^{(b)}\right) \\
& \qquad=\frac{2 \bar{\kappa}}{\tau_{T}^{2} f_{X}(0)}\left(\mathbb{V}\left[\eta_{Y, 0}\left(Z_{i}\right)-\theta \eta_{T, 0}\left(Z_{i}\right)-\left(\eta_{Y}^{(a)}\left(Z_{i}\right)-\theta \eta_{T}^{(a)}\left(Z_{i}\right)\right) \mid X_{i}=0\right]\right. \\
& \left.\quad \quad-\mathbb{V}\left[\eta_{Y, 0}\left(Z_{i}\right)-\theta \eta_{T, 0}\left(Z_{i}\right)-\left(\eta_{Y}^{(b)}\left(Z_{i}\right)-\theta \eta_{T}^{(b)}\left(Z_{i}\right)\right) \mid X_{i}=0\right]\right)
\end{aligned}
\]

The first part of the proposition shows that our flexible covariate-adjusted fuzzy RD estimator is asymptotically normal, with asymptotic variance that depends on the population counterparts \(\bar{\eta}_{Y}\) and \(\bar{\eta}_{T}\) of the two estimated adjustment functions. This result can then be used to construct\\
a confidence interval for \(\theta\) based on the t-statistic. Alternatively, confidence sets for \(\theta\) can be constructed via an Anderson-Rubin-type approach, which circumvents certain problems of ratio estimators (Noack and Rothe, 2024).

The second part of the proposition shows that the asymptotic variance of our estimator is minimized if the estimated adjustment functions concentrate around \(\bar{\eta}_{Y}=\eta_{Y, 0}\) and \(\bar{\eta}_{T}=\eta_{T, 0}\), respectively. That is, the optimal adjustment functions for fuzzy RD designs can be obtained by separately considering two covariate-adjusted sharp RD problems with outcomes \(Y_{i}\) and \(T_{i}\), respectively. This holds because for fixed adjustment functions \(\eta_{Y}\) and \(\eta_{T}\) we have that \(\widehat{\theta}\left(h ; \eta_{Y}, \eta_{T}\right)-\) \(\theta\) is first-order asymptotically equivalent to a sharp RD estimator with the infeasible outcome \(U_{i}\left(\eta_{Y}, \eta_{T}\right)=\left(Y_{i}-\theta T_{i}-\left(\eta_{Y}\left(Z_{i}\right)-\theta \eta_{T}\left(Z_{i}\right)\right)\right) / \tau_{T}\). By our Theorem 3, the asymptotic variance of \(\widehat{\theta}\left(h ; \eta_{Y}, \eta_{T}\right)\) is minimized if \(\left(\eta_{Y}\left(Z_{i}\right)-\theta \eta_{T}\left(Z_{i}\right)\right) / \tau_{T}\) equals the optimal adjustment function for the outcome \(\left(Y_{i}-\theta T_{i}\right) / \tau_{T}\). By linearity of conditional expectations, this holds if \(\eta_{Y}=\eta_{Y, 0}\) and \(\eta_{T}=\eta_{T, 0}\).\\
6.4. Variants of Cross-Fitting. We note that instead of the type of cross-fitting described in Section 3.2, which is analogous to the "DML2" method in Chernozhukov et al. (2018), one could also consider an analog of their "DML1" method, which creates an overall estimate by averaging separate estimates from each data fold. In our context, this would yield an estimator of the form

\[
\widehat{\tau}_{a l t}(h ; \widehat{\eta})=\frac{1}{S} \sum_{s \in[S]} \sum_{i \in I_{s}} w_{i, s}(h) M_{i}\left(\widehat{\eta}_{s}\right),
\]

where \(w_{i, s}(h)\) is the local linear regression weight of unit \(i\) using only data from the \(s\)-th fold; see Appendix A.1. Under the conditions of Theorem 1, one can see from its proof that

\[
\widehat{\tau}_{a l t}(h ; \widehat{\eta})-\widehat{\tau}(h ; \bar{\eta})=O_{P}\left(r_{n}(n h)^{-1 / 2}+v_{2, n} h^{2}\right) .
\]

The estimators \(\widehat{\tau}(h ; \widehat{\eta})\) and \(\widehat{\tau}_{\text {alt }}(h ; \widehat{\eta})\) thus have the same first-order asymptotic distribution. However, comparing the rate in (6.1) to that in Theorem 1 shows that the alternative implementation removes a term of order \(O_{P}\left(v_{1, n} h(n h)^{-1 / 2}\right)\). We still prefer our proposed implementation of cross-fitting despite this improvement in second-order asymptotic properties because it allows existing routines for bandwidth selection and confidence interval construction to be applied directly to the generated data set \(\left\{\left(X_{i}, M_{i}\left(\widehat{\eta}_{s(i)}\right)\right)\right\}_{i \in[n]}\), as discussed in Section 6.1.

\section*{7. SIMULATIONS}
In this section, we investigate the finite sample properties of our proposed flexible covariate adjustment RD estimators under realistic conditions in two simulation studies. The first study's purpose is to show that our theoretical results provide accurate approximations to our estimator's actual finite properties, whereas the second study's purpose is to document how the properties of our estimator\\
and that of existing methods are affected if the number of covariates becomes large relative to the effective sample size.\\
7.1. General Setup. Our simulations are based on real data from Londoño-Vélez et al. (2020), who study the impact of merit-based college financial aid for low-income students in a sharp RD design. Their data contain the outcome variable, a dummy for immediate enrollment in any post secondary education, the running variable, a test score, \({ }^{10}\) and 21 covariates, namely age, family size, indicators for gender, ethnicity, employment status, parent's education, household residential stratum, high school schedule, and high school type. Our simulations involve repeatedly drawing random samples from a version of the data that is restricted to the \(n=259,419\) observations with test scores below the original treatment threshold (so that none of the students remaining in the data set are actually assigned to treatment), and then estimating the effect of a placebo treatment "received" by students with test scores above the median test score value. We use either the original outcome (enrollment in any post secondary education) or age (one of the original covariates) as the dependent variable. These two dependent variables correspond to settings in which covariate adjustments achieve almost no and quite substantial efficiency gains, respectively. See the RD estimates from the entire restricted data in Table S 1 for details. In the main text, we conduct inference using the bias-aware approach with smoothness constants calibrated via the rule of thumb of Armstrong and Kolesár (2020) on the full restricted dataset. \({ }^{11}\) All simulations are based on 10,000 Monte Carlo draws.\\
7.2. Simulation I: Moderate number of covariates. In this simulation study, we evaluate the finite-sample performance of our methods in a typical RD setting with a moderate number of covariates and a relatively large number of observations. Specifically, we consider estimation with the original baseline covariates and samples of size 5000 , which is around the median of the sample sizes of the empirical applications of our literature survey. We apply our flexible covariate adjustment discussed in Section 3.4. Additionally, we consider the deterministic approximations of all the feasible adjustment methods, which were obtained by running the respective method on the restricted dataset. By comparing the feasible adjustments and their respective deterministic approximation, we can assess the quality of the approximation in our equivalence result of Theorem 1. For comparison, we also report the results without covariate adjustments and with conventional linear adjustments. For each adjustment method, we select the bandwidth and construct a confidence interval using the bias-aware approach. \({ }^{12}\) The results are based on one random data split for each Monte Carlo draw.

Table 1 reports the main results of this simulation study. Our methods work very well for both

\footnotetext{\({ }^{10}\) Londoño-Vélez et al. (2020) consider two different test scores as running variables. We focus on the SABER 11 test score in this section as it is available for a larger number of data points.\\
\({ }^{11}\) The smoothness constants selected via the rule of thumb are 0.00024 and 0.01034 for the original outcome and the age as the dependent variable, respectively.\\
\({ }^{12}\) The number of effective observations used in the second stage is on average around 2500 for the original outcome and around 1600 for age as the dependent variable.
}Table 1: Main results for Simulation I with bias-aware inference.

\begin{center}
\begin{tabular}{llllllllll}
\hline
Adjustment Method &  & \begin{tabular}{l}
SE \\
x100 \\
\end{tabular} & \begin{tabular}{l}
SD \\
x100 \\
\end{tabular} & \begin{tabular}{l}
Bias \\
x100 \\
\end{tabular} & \begin{tabular}{l}
RMSE \\
x100 \\
\end{tabular} & \begin{tabular}{l}
Band- \\
width \\
\end{tabular} & \begin{tabular}{l}
CI Cov \\
in \(\%\) \\
\end{tabular} & \begin{tabular}{l}
CI Length \\
x100 \\
\end{tabular} & \begin{tabular}{l}
CI Length \\
Reduction \\
in \% \\
\end{tabular} \\
\hline
Panel A - Original Outcome &  &  &  &  &  &  &  &  &  \\
No Covariates &  & 2.57 & 2.56 & 0.38 & 2.59 & 23.31 & 97.10 & 11.25 & 0.00 \\
Conventional Linear &  & 2.50 & 2.52 & 0.44 & 2.56 & 23.16 & 96.95 & 10.97 & 2.49 \\
Localized Linear & Feasible & 2.55 & 2.53 & 0.47 & 2.57 & 23.21 & 96.98 & 11.13 & 1.04 \\
 & Oracle & 2.53 & 2.51 & 0.47 & 2.56 & 23.15 & 96.90 & 11.06 & 1.68 \\
Flexible & Feasible & 2.53 & 2.52 & 0.44 & 2.56 & 23.15 & 96.98 & 11.05 & 1.71 \\
 & Oracle & 2.52 & 2.51 & 0.42 & 2.54 & 23.12 & 97.07 & 11.03 & 1.93 \\
Panel B - Age &  &  &  &  &  &  &  &  &  \\
No Covariates &  &  &  &  &  &  &  &  &  \\
Conventional Linear &  & 38.39 & 38.79 & -7.53 & 39.51 & 14.66 & 97.85 & 173.82 & 0.00 \\
Localized Linear & Feasible & 34.41 & 34.19 & -6.66 & 34.83 & 13.92 & 97.54 & 152.55 & 12.24 \\
Flexible & Oracle & 34.04 & 34.52 & -6.64 & 35.15 & 13.96 & 97.86 & 156.46 & 9.99 \\
 & Feasible & 33.52 & 33.69 & -6.64 & 34.74 & 13.91 & 97.85 & 154.66 & 11.02 \\
\hline
\end{tabular}
\end{center}

Notes: Results are based on 10,000 Monte Carlo draws and a sample size of \(n=5000\) (see Section 7 for details). Data generating process is based on Londoño-Vélez et al. (2020). The bandwidth is chosen and the confidence sets are constructed based on bias-aware inference. The columns show results for simulated mean standard error (SE); standard deviation (SD); bias (Bias); root mean squared error (RMSE); the average bandwidth (Bandwidth); coverage of confidence intervals with \(95 \%\) nominal level (CI Cov); the average confidence interval length (CI Length); and the mean CI length relative to the no covariates CI length (CI Length Reduction). Estimators are described in Section 3.\\
dependent variables and all types of adjustments in that the mean simulated standard errors are close to the simulated standard deviations and the confidence intervals have simulated coverage rates close to the nominal one. The confidence intervals are slightly conservative, which is typical in bias-aware inference. The changes in the mean bias for different types of adjustments are negligible relative to the standard deviation, which is consistent with our key insight that covariate adjustments have no first-order effect on the leading bias constant.

In Panel A, the covariates have essentially no impact on the dependent variable, and so none of the methods leads to noticeable reductions in the standard deviation. In Panel B, where the covariates have some explanatory power for the dependent variable, the cross-fitted RD estimator with localized linear adjustment yields a confidence interval that is on average \(10 \%\) shorter than the no covariates confidence interval. The flexible adjustment improves this performance even further. As can be expected in a setting with a small number of covariates relative to the sample size, the conventional and cross-fitted localized linear covariate adjustments yield similar results.

In Appendix B of the Online Supplement, we present additional simulation results for all individual adjustment methods described in Section 3.4. We further investigate the asymptotic equivalence result presented in Theorem 1 within this simulation design and we illustrate that\\
the estimation uncertainty of the adjustment functions is indeed almost negligible relative to the overall estimation uncertainty of the RD estimators. Additionally, we present simulation results for covariate-adjusted RD estimators that conduct estimation and inference based on robust bias corrections. The qualitative conclusions remain very similar to those presented above.\\
7.3. Simulation II: Many covariates. Our literature survey documents that the effective sample size relative to the number of covariates is small in many empirical applications; see Table S 2 in the Online Supplement. We investigate the performance of our proposed approach in such settings within a simulation study with a fixed effective sample size and varying number of covariates, using the original outcome as the dependent variable. We create additional covariates by generating all second-order interaction terms of the original covariates, and we consider different settings by including the first \(2,10,50,100\), and 150 of these covariates. \({ }^{13}\) To mimic settings where there are many covariates relative to the effective sample size, we sample 500 observations without replacement within a distance of 25 from the placebo cutoff and use all of them in the RD regressions, i.e. we use a fixed bandwidth \(h=25 .{ }^{14}\) In this setting, the ratio of the effective sample size to the number of covariates lies in the range between 250 and 3.33 , which corresponds to the settings in our literature analysis with small values of this ratio. For each subsample, we estimate the RD parameter using the no covariates, the conventional linear adjustment, and our cross-fitted RD estimator with localized linear adjustments and localized random forest adjustments. \({ }^{15}\) The results are based on the bias-aware inference approach and \(B=11\) data splits for each Monte Carlo draw.

First, we illustrate that the conventional linear adjustment RD estimator can be less efficient than the no covariates RD estimator and the conventional standard errors can be severely distorted even in settings with a moderate number of covariates. Second, the cross-fitted linear adjustment RD estimator can also be less efficient than the no covariates RD estimators, but its standard error is not downward biased in this simulation, which results in valid inference. Third, cross-fitted RD estimators employing machine learning methods and regularization mitigate both of these issues in our simulation study, i.e. they are at least as efficient as the no covariates RD estimator and the standard errors are unbiased.\\
7.3.1. Results on efficiency. Figure 2 shows the bias and the standard deviation of the four estimation methods, normalized by the standard deviation of the no covariates RD estimator, for a varying number of covariates. We note that the simulated bias is insensitive to including many covariates, and we therefore focus on the standard deviation. In this setting, the covariates seem to have essentially no explanatory power for the dependent variable, and so adjustments based on them

\footnotetext{\({ }^{13}\) The first 21 of the technical covariates correspond to the original covariates, followed by the interaction terms. Since the covariates have essentially no explanatory power for the original outcome, the exact order of inclusion does not affect the results in this section.\\
\({ }^{14}\) This fixed bandwidth is close to the average optimal bandwidth selected in Simulation I in a data-driven way.\\
\({ }^{15}\) We chose the random forest to represent the machine learning adjustments here, but the qualitative results are similar when employing other methods. In this section, we focus on the individual adjustment methods, rather than on the flexible ensemble, to offer more direct insights into the mechanics of the linear and regularized adjustments.
}
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-26}

No covariates

\begin{itemize}
  \item Conventional Linear
\end{itemize}

Localized Linear \(\leqslant\) Localized Random Forest

Figure 2: Results for Simulation II - Bias and standard deviation of the respective estimator relative to the standard deviation of the no covariates RD estimator in the left and right panel, respectively. The results are based on samples of size \(n=500\) within the estimation window with \(h=25\) and 10,000 Monte Carlo draws.\\
cannot lead to a reduction in the asymptotic variance of the RD estimator; see estimation results in Table S1. As predicted by our theory, when the number of covariates remains moderate, all estimators perform very similarly, meaning that all the adjustments concentrate around the optimal function of no adjustment. However, as the number of covariates increases, the standard deviations of both the conventional and cross-fitted localized linear adjustment estimators become substantially larger than that of the no covariates RD estimator. The reason for that is that the linear regression with a large number of covariates is very variable, such that the estimated adjustments are no longer close to a constant \({ }^{16}\) and the high-dimensional linear adjustments effectively add non-negligible noise to the outcome variable in this setting.

In contrast, the RD estimator with random forest adjustments does not become more variable as the number of covariates increases, meaning that the estimated adjustment function remains close to the optimal function of no adjustment. This simulation illustrates the general point that by means of regularization, the machine learning methods produce stable adjustments in a much wider range of settings than the linear regression does, and they typically do not perform worse than the no covariates RD estimator. It is therefore advisable to always rely on regularized adjustments in high-dimensional settings.\\
7.3.2. Results on inference. We now turn to the standard error and coverage of the confidence intervals for the respective methods. The left panel of Figure 3 shows that the standard error of the

\footnotetext{\({ }^{16}\) Such finite-sample behavior renders our asymptotic theory as well as the results of Calonico et al. (2019) inapplicable in this setting.
}
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-27}

Figure 3: Results for Simulation II - Mean standard error relative to the standard deviation of the respective estimator and simulated confidence interval coverage for nominal confidence level \(95 \%\). We consider bias-aware confidence intervals and nearest neighbor standard errors. The results are based on samples of size \(n=500\) within the estimation window with \(h=25\) and 10,000 Monte Carlo draws.\\
conventional linear adjustment estimator exhibits a downward bias that increases substantially with the number of covariates, reaching \(35 \%\) for 150 covariates. This effect is due to overfitting: with many covariates, the regression residuals that enter the standard error formula become "too close to zero", and standard errors therefore become "too small". With cross-fitting, this issue occurs neither for the linear adjustment nor for the random forest adjustment.

The right panel of Figure 3 shows that, due to increasingly biased standard errors, the coverage of linear adjustment bias-aware confidence intervals with nominal level \(95 \%\) falls below \(85 \%\) for 150 covariates. With cross-fitting, bias-aware confidence intervals have close to nominal coverage for both adjustment methods and all numbers of covariates under consideration. These simulation results demonstrate the benefits of cross-fitting and suggest that practitioners should exercise caution when doing inference based on the conventional linear adjustment estimators even if the number of covariates is only moderate to low (relative to the effective sample size), as the corresponding standard errors can be severely downward biased.

\section*{8. CONCLUSIONS}
We have proposed a novel class of estimators that can make use of covariate information more efficiently than the conventional linear adjustment estimators that are currently used widely in practice. In particular, our approach allows the use of modern machine learning tools to adjust for covariates, and is at the same time largely unaffected by the "curse of dimensionality". Our estimator\\
is also easy to implement in practice, and can be combined in a straightforward manner with existing methods for bandwidth choice and the construction of confidence intervals. In our reanalysis of the literature, we show that our proposed estimator yields shorter confidence intervals in almost all empirical applications, and in some cases, these reductions can be substantial. We therefore expect our proposed estimator to be very attractive for a wide range of future economic applications.

\section*{A. PROOFS OF THE MAIN RESULTS}
In this section, we prove Theorems 1-4 and Proposition 1. To this end, we show a more general result that allows for a local polynomial regression of an arbitrary order \(p\). We use this result also in Appendix B to establish the validity of the inference methods discussed in Section 6.1.\\
A.1. Additional Notation. Let \(\mathbb{X}_{n}=\left(X_{i}\right)_{i \in[n]}\) denote the realizations of the running variable. For \(0 \leq v \leq p\), we define feasible and infeasible estimators of the jump in the \(v\)-th derivative of the conditional expectation of the modified outcome \(M(\bar{\eta})\) at the cutoff using the \(p\)-th order local polynomial regression as

\[
\begin{aligned}
\widehat{\tau}_{v, p}(h ; \widehat{\eta}) & =\sum_{i=1}^{n} w_{i, v, p}(h) M_{i}\left(\widehat{\eta}_{s}(i)\right) \quad \text { and } \quad \widehat{\tau}_{v, p}(h ; \bar{\eta})=\sum_{i=1}^{n} w_{i, v, p}(h) M_{i}(\bar{\eta}), \\
w_{i, v, p}(h) & =w_{i, v, p}^{+}(h)-w_{i, v, p}^{-}(h), \\
w_{i, v, p}^{\star}(h) & =e_{v}^{\top}\left(\sum_{i=1}^{n} K_{h}^{\star}\left(X_{i}\right) \widetilde{X}_{p, i} \widetilde{X}_{p, i}^{\top}\right)^{-1} K_{h}^{\star}\left(X_{i}\right) \widetilde{X}_{p, i} \quad \text { for } \star \in\{+,-\},
\end{aligned}
\]

where \(\widetilde{X}_{p, i}=\left(1, X_{i}, \ldots, X_{i}^{p}\right)^{\top}, K_{h}(v)=K(v / h) / h, K_{h}^{+}(v)=K_{h}(v) \mathbf{1}\{v \geq 0\}, K_{h}^{-}(v)=K_{h}(v) \mathbf{1}\{v<\) \(0\}\). The corresponding estimates of \(\beta_{v}^{\star}(\bar{\eta})=\left.\partial_{x}^{v} \mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\right|_{x=0^{\star}}\) are given by

\[
\widehat{\beta}_{v, p}^{\star}(h ; \widehat{\eta})=\sum_{i=1}^{n} w_{i, v, p}^{\star}(h) M_{i}\left(\widehat{\eta}_{s(i)}\right) \quad \text { and } \quad \widehat{\beta}_{v, p}^{\star}(h ; \bar{\eta})=\sum_{i=1}^{n} w_{i, v, p}^{\star}(h) M_{i}(\bar{\eta}) \quad \text { for } \star \in\{+,-\} .
\]

A.2. General Result. We state and prove two theorems that generalize our Theorems 1 and 2.

Theorem A.1. Suppose that Assumptions \(1-4\) hold with \(j \in\{1, \ldots, p+1\}\) in Assumption 2. Then:

\[
\widehat{\tau}_{0, p}(h ; \widehat{\eta})=\widehat{\tau}_{0, p}(h ; \bar{\eta})+O_{P}\left(t_{p}\right),
\]

where \(t_{p}=r_{n}(n h)^{-1 / 2}+\sum_{j=1}^{p} v_{j, n} h^{j}(n h)^{-1 / 2}+v_{p+1, n} h^{p+1}\).\\
In the proof of Theorem A.1, we will use the following lemma that collects some standard intermediate steps in the analysis of local polynomial estimators, taking into account cross-fitting.

Lemma A.1. Suppose that Assumptions 3 and 4 hold. For \(s \in[S]\) and \(\star \in\{-,+\}\), it holds that:\\
(i) \(\frac{S}{n} \sum_{i \in I_{s}} K_{h}^{\star}\left(X_{i}\right)\left(X_{i} / h\right)^{j}=\mathbb{E}\left[K_{h}^{\star}\left(X_{i}\right)\left(X_{i} / h\right)^{j}\right]+O_{p}\left((n h)^{-1 / 2}\right)\) for \(j \in \mathbb{N}\),\\
(ii) \(\sum_{i \in I_{s}} w_{i, 0, p}^{\star}(h)=1 / S+O_{p}\left((n h)^{-1 / 2}\right)\),\\
(iii) \(\sum_{i \in I_{s}} w_{i, 0, p}^{\star}(h) X_{i}^{j}=O_{p}\left(h^{j}(n h)^{-1 / 2}\right)\) for \(1 \leq j \leq p\),\\
(iv) \(\sum_{i \in I_{s}}\left|w_{i, 0, p}^{\star}(h) X_{i}^{j}\right|=O_{P}\left(h^{j}\right)\) for \(j \in \mathbb{N}\),\\
(v) \(\sum_{i \in I_{s}} w_{i, 0, p}^{\star}(h)^{2}=O_{P}\left((n h)^{-1}\right)\).

Proof. The results follow from standard kernel calculations.\\
Proof of Theorem A.1. To begin with, note that

\[
\widehat{\tau}_{0, p}(h ; \bar{\eta})-\widehat{\tau}_{0, p}(h ; \widehat{\eta})=\sum_{s=1}^{S} G_{s}(p), \quad G_{s}(p) \equiv \sum_{i \in I_{s}} w_{i, 0, p}(h)\left(\widehat{\eta}_{s}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right) .
\]

Since \(S\) is a fixed number, it suffices to show that \(G_{s}(p)=O_{p}\left(t_{p}\right)\) for \(s \in\{1, \ldots, S\}\). We analyze the expectation and variance of \(G_{s}(p)\) conditional on \(\mathbb{X}_{n}\) and \(\left(W_{j}\right)_{j \in I_{s}^{c}}\). We begin with the expectation. It holds with probability approaching one that

\[
\begin{aligned}
\left|\mathbb{E}\left[G_{s}(p) \mid \mathbb{X}_{n},\left(W_{j}\right)_{j \in I_{s}^{c}}\right]\right| & =\left|\sum_{i \in I_{s}} w_{i, 0, p}(h) \mathbb{E}\left[\hat{\eta}_{s}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i},\left(W_{j}\right)_{j \in I_{s}^{c}}\right]\right| \\
& \leq \sup _{\eta \in \mathcal{T}_{n}}\left|\sum_{i \in I_{s}} w_{i, 0, p}(h) \mathbb{E}\left[\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i}\right]\right| .
\end{aligned}
\]

Let \(m(x ; \eta)=\mathbb{E}\left[\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right) \mid X_{i}=x\right]\). Taylor's theorem yields

\[
m\left(X_{i} ; \eta\right)=m(0 ; \eta)+\sum_{j=1}^{p} \frac{1}{j!} \partial_{x}^{j} m(0 ; \eta) X_{i}^{j}+\frac{1}{(p+1)!} \partial_{x}^{p+1} m\left(\widetilde{x}_{i, p} ; \eta\right) X_{i}^{p+1}
\]

for some \(\widetilde{x}_{i, p}\) between 0 and \(X_{i}\). We analyze the three terms associated with different terms of Taylor's expansion separately. We make use of Lemma A. 1 in each step.

First, using the Cauchy-Schwarz inequality, we obtain that

\[
\sup _{\eta \in \mathcal{T}_{n}}\left|m(0 ; \eta) \sum_{i \in I_{s}} w_{i, 0, p}(h)\right|=\sup _{\eta \in \mathcal{T}_{n}}|m(0 ; \eta)| O_{p}\left((n h)^{-1 / 2}\right)=O_{p}\left(r_{n}(n h)^{-1 / 2}\right) .
\]

Second, for \(j \in\{1, \ldots, p\}\), we have that

\[
\sup _{\eta \in \mathcal{T}_{n}}\left|\partial_{x}^{j} m(0 ; \eta) \sum_{i \in I_{s}} w_{i, 0, p}(h) X_{i}^{j}\right|=\sup _{\eta \in \mathcal{T}_{n}}\left|\partial_{x}^{j} m(0 ; \eta)\right| h^{j} O_{p}\left((n h)^{-1 / 2}\right)=O_{p}\left(h^{j}(n h)^{-1 / 2} v_{j, n}\right) .
\]

Third, we note that

\[
\sup _{\eta \in \mathcal{T}_{n}}\left|\sum_{i \in I_{s}} w_{i, 0, p}(h) \partial_{x}^{p+1} m\left(\widetilde{x}_{i} ; \eta\right) X_{i}^{p+1}\right| \leq \sum_{i \in I_{s}}\left|w_{i, 0, p}(h) X_{i}^{p+1}\right| \sup _{\eta \in \mathcal{T}_{n}}\left|\partial_{x}^{p+1} m\left(\widetilde{x}_{i} ; \eta\right)\right|=O_{p}\left(h^{p+1} v_{p+1, n}\right) .
\]

Next, we consider the conditional variance. It holds with probability approaching one that

\[
\begin{aligned}
\mathbb{V}\left[G_{s}(p) \mid \mathbb{X}_{n},\left(W_{j}\right)_{j \in I_{s}^{c}}\right] & =\sum_{i \in I_{s}} w_{i, 0, p}(h)^{2} \mathbb{V}\left[\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s}\left(Z_{i}\right) \mid \mathbb{X}_{n},\left(W_{j}\right)_{j \in I_{s}^{c}}\right] \\
& \leq \sup _{\eta \in \mathcal{T}_{n}} \sum_{i \in I_{s}} w_{i, 0, p}(h)^{2} \mathbb{E}\left[\left(\bar{\eta}\left(Z_{i}\right)-\eta\left(Z_{i}\right)\right)^{2} \mid X_{i}\right] \\
& \leq \sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \mathcal{X}_{h}} \mathbb{E}\left[\left(\bar{\eta}\left(Z_{i}\right)-\eta\left(Z_{i}\right)\right)^{2} \mid X_{i}=x\right] \sum_{i \in I_{s}} w_{i, 0, p}(h)^{2} \\
& =O_{p}\left(r_{n}^{2}(n h)^{-1}\right),
\end{aligned}
\]

where we use Lemma A. 1 and Assumption 1 in the last step. The conditional convergence then implies the unconditional one (see Chernozhukov et al., 2018, Lemma 6.1).

Theorem A.2. Suppose that the assumptions of Theorem A. 1 hold, Assumption 5 holds, and \(\mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\) is \(p+1\) times continuously differentiable with L-Lipschitz continuous \(p+1\) derivative bounded by \(C\). Then

\[
\sqrt{n h} V_{p}(\bar{\eta})^{-1 / 2}\left(\widehat{\tau}_{0, p}(h ; \widehat{\eta})-\tau-h^{p+1} B_{p}\right) \xrightarrow{d} \mathcal{N}(0,1),
\]

where, for some kernel constants \(\bar{\nu}_{p}\) and \(\bar{\kappa}_{p}\),

\[
\begin{aligned}
B_{p, n} & =\frac{\bar{\nu}_{p}}{2}\left(\left.\partial_{x}^{p+1} \mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\right|_{x=0^{+}}+\left.(-1)^{p} \partial_{x}^{p+1} \mathbb{E}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\right|_{x=0^{-}}\right)+o_{P}(1), \\
V_{p}(\bar{\eta}) & =\frac{\bar{\kappa}_{p}}{f_{X}(0)}\left(\mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}=0^{+}\right]+\mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}=0^{-}\right]\right) .
\end{aligned}
\]

Proof of Theorem A.2. By the conditional version of Lyapunov's CLT, we obtain that

\[
\operatorname{se}_{0, p}(h ; \bar{\eta})^{-1}\left(\widehat{\tau}_{0, p}(h ; \bar{\eta})-\mathbb{E}\left[\widehat{\tau}_{0, p}(h ; \bar{\eta}) \mid \mathbb{X}_{n}\right]\right) \rightarrow \mathcal{N}(0,1) .
\]

where \(\operatorname{se}_{0, p}^{2}(h ; \bar{\eta})=\sum_{i=1}^{n} w_{i, 0, p}(h)^{2} \mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}\right]\). By \(L\)-Ltipschis ictz continuity of \(\mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}=x\right]\) in \(x\), we obtain that

\[
\operatorname{se}_{0, p}^{2}(h ; \bar{\eta})=\sum_{i=1}^{n} w_{i, 0, p}^{-}(h)^{2} \mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}=0^{-}\right]+\sum_{i=1}^{n} w_{i, 0, p}^{+}(h)^{2} \mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}=0^{+}\right]+o_{p}\left((n h)^{-1}\right) .
\]

It then follows from standard kernel calculations that \(n h \mathrm{se}_{0, p}^{2}(h ; \bar{\eta})-V_{p}(\bar{\eta})=o_{P}(1)\) and \(\mathbb{E}\left[\widehat{\tau}_{0, p}(h ; \bar{\eta}) \mid \mathbb{X}_{n}\right]-\) \(\tau=B_{p} h^{p+1}+o_{p}\left(h^{p+1}\right)\) for some constant \(B_{p} . \quad\) Fan and Gijbels 1992\\
A.3. Proofs of Theorems 1-4. Theorems 1 and 2 follow directly from the general results in Theorems A. 1 and A. 2 with \(p=1\); and Theorem 4 follows from simple calculations. It remains to prove Theorem 3. For any \(\eta \in \mathcal{V}\), it holds that

\[
\frac{2 f_{X}(0)}{\bar{\kappa}} V(\eta)=\mathbb{V}\left[Y_{i}-\mu_{0}^{+}\left(Z_{i}\right) \mid X_{i}=0^{+}\right]+\mathbb{V}\left[Y_{i}-\mu_{0}^{-}\left(Z_{i}\right) \mid X_{i}=0^{-}\right]+R(\eta),
\]

where the first two terms on the right-hand side do not depend on \(\eta\), and

\[
R(\eta)=\mathbb{V}\left[\mu_{0}^{+}\left(Z_{i}\right)-\eta\left(Z_{i}\right) \mid X_{i}=0^{+}\right]+\mathbb{V}\left[\mu_{0}^{-}\left(Z_{i}\right)-\eta\left(Z_{i}\right) \mid X_{i}=0^{-}\right] .
\]

Further, it holds that

\[
\begin{aligned}
R(\eta)=R\left(\eta_{0}+\eta-\eta_{0}\right)= & \mathbb{V}\left[\left.\frac{1}{2}\left(\mu_{0}^{+}\left(Z_{i}\right)-\mu_{0}^{-}\left(Z_{i}\right)\right)-\left(\eta\left(Z_{i}\right)-\eta_{0}\left(Z_{i}\right)\right) \right\rvert\, X_{i}=0^{+}\right] \\
& +\mathbb{V}\left[\left.-\frac{1}{2}\left(\mu_{0}^{+}\left(Z_{i}\right)-\mu_{0}^{-}\left(Z_{i}\right)\right)-\left(\eta\left(Z_{i}\right)-\eta_{0}\left(Z_{i}\right)\right) \right\rvert\, X_{i}=0^{-}\right] \\
& =R\left(\eta_{0}\right)+2 \mathbb{V}\left[\eta\left(Z_{i}\right)-\eta_{0}\left(Z_{i}\right) \mid X_{i}=0\right],
\end{aligned}
\]

where in the last step we use the assumption on continuity of conditional covariances. The theorem follows from the above decomposition by taking the difference \(V\left(\eta^{(a)}\right)-V\left(\eta^{(b)}\right)\) for arbitrary \(\eta^{(a)}\) and \(\eta^{(b)}\) in \(\mathcal{V}\).\\
A.4. Proof of Proposition 1. We first note that

\[
\widehat{\theta}\left(h ; \widehat{\eta}_{Y}, \widehat{\eta}_{T}\right)-\widehat{\theta}\left(h ; \bar{\eta}_{Y}, \bar{\eta}_{T}\right)=O_{P}\left(\left(r_{n}(n h)^{-1 / 2}+v_{1, n} h(n h)^{-1 / 2}+v_{2, n} h^{2}\right)^{2}\right) .
\]

This equality is an immediate consequence of Theorem 1 and an application of the continuous mapping theorem as \(\left|\tau_{T}\right|>0\). Further, using a mean-value expansion, it follows that

\[
\widehat{\theta}\left(h ; \bar{\eta}_{Y}, \bar{\eta}_{T}\right)-\theta=\frac{1}{\tau_{T}}\left(\widehat{\tau}_{Y}\left(h ; \bar{\eta}_{Y}\right)-\tau_{Y}\right)-\frac{\tau_{Y}}{\tau_{T}^{2}}\left(\widehat{\tau}_{T}\left(h ; \bar{\eta}_{T}\right)-\tau_{T}\right)+\widehat{\rho}\left(\bar{\eta}_{T}, \bar{\eta}_{Y}\right)
\]

with

\[
\widehat{\rho}\left(\bar{\eta}_{T}, \bar{\eta}_{Y}\right)=\frac{\widehat{\tau}_{Y}\left(h ; \bar{\eta}_{Y}\right)\left(\widehat{\tau}_{T}\left(h ; \bar{\eta}_{T}\right)-\tau_{T}\right)^{2}}{2 \widehat{\tau}_{T}^{*}\left(h ; \bar{\eta}_{T}\right)^{3}}-\frac{\left(\widehat{\tau}_{Y}\left(h ; \bar{\eta}_{Y}\right)-\tau_{Y}\right)\left(\widehat{\tau}_{T}\left(h ; \bar{\eta}_{T}\right)-\tau_{T}\right)}{\tau_{T}^{2}},
\]

where \(\widehat{\tau}_{T}^{*}\left(h ; \eta_{T}\right)\) is some intermediate value between \(\tau_{T}\) and \(\widehat{\tau}_{T}\left(h ; \bar{\eta}_{T}\right)\). Given our assumptions, it follows that

\[
\widehat{\rho}\left(\widehat{\eta}_{T}, \widehat{\eta}_{Y}\right)=O_{P}\left(\left((n h)^{-1 / 2}+h^{2}\right)^{2}\right) .
\]

Part (i) follows analogously to Theorems 1 and 2 and Part (ii) follows from Theorem 3.

\section*{B. DETAILS ON SECTION 6.1}
In this section, we formally show that, under suitable assumptions, existing procedures for bandwidth selection and construction of confidence intervals devised for settings without covariates can be directly applied to the modified data \(\left\{\left(X_{i}, M_{i}(\widehat{\eta})\right)\right\}_{i \in[n]}\).\\
B.1. Standard Errors. We generalize the nearest-neighbors standard error from the main text to the local polynomial regression of an arbitrary order \(p\). Let

\[
\widehat{s e}_{v, p}^{2}(h ; \widehat{\eta})=\sum_{i=1}^{n} w_{i, v, p}^{2}(h) \widehat{\sigma}_{i}^{2}(\widehat{\eta}), \quad \widehat{\sigma}_{i}^{2}(\widehat{\eta})=\left(M_{i}\left(\widehat{\eta}_{s}(i)\right)-\frac{R}{R+1} \sum_{j \in \mathcal{R}_{i}} M_{j}\left(\widehat{\eta}_{s(j)}\right)\right)^{2},
\]

where \(\mathcal{R}_{i}\) is the set of \(R\) nearest neighbors of unit \(i\) in terms of their running variable realization on the respective side of the cutoff. Establishing consistency of this standard error requires the following technical assumption on the first stage estimator, which is implied by our main assumptions, for example, if \(M_{i}(\bar{\eta})\) is bounded.

Assumption B.1. For all \(s \in[S]\), it holds that \(\sum_{i \in[n]} w_{i, v, p}^{2}(h) \iota_{i}(\widehat{\eta})=o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\) for \(0 \leq v \leq p\), where

\[
\iota_{i}(\widehat{\eta})=\sum_{\substack{(j, l) \in \mathcal{R}_{i}^{2} \\(j, l) \notin I_{s(i)}^{2}}}\left(\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)-\left(\widehat{\eta}_{s(j)}\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right)\right)\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)
\]

Proposition B.1. Suppose that Assumptions 1-5 and B. 1 hold. Moreover, suppose that Assumption 1 also holds with \(\mathcal{X}_{h}\) replaced by \(\widetilde{\mathcal{X}}_{h}\) that is an open set s.t. \(\mathcal{X}_{h} \subset \widetilde{\mathcal{X}}_{h}\), and \(\sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \widetilde{\mathcal{X}}_{h}} \mathbb{E}\left[\left(M_{i}(\eta)-\right.\right.\) \(\left.\left.\mathbb{E}\left[M_{i}(\eta) \mid X_{i}\right]\right)^{4} \mid X_{i}=x\right]\) is bounded by \(B\) for all \(n \in \mathbb{N}\). Let further \(\mathbb{E}\left[\left(M_{i}(\bar{\eta}) \mid X_{i}=x\right]\right.\) and \(\mathbb{V}\left[\left(M_{i}(\bar{\eta}) \mid X_{i}=x\right]\right.\) be L-Lipschitz continuous. Then for all \(0 \leq v \leq p\), it holds that

\[
n h^{1+2 v}\left(\widehat{s e}_{v, p}^{2}(h ; \widehat{\eta})-s e_{v, p}^{2}(h ; \bar{\eta})\right)=o_{P}(1),
\]

where \(e_{v, p}^{2}(h ; \bar{\eta})=\sum_{i=1}^{n} w_{i, v, p}^{2}(h) \sigma_{i}^{2}(\bar{\eta})\) and \(\sigma_{i}^{2}(\bar{\eta})=\mathbb{V}\left[M_{i}(\bar{\eta}) \mid X_{i}\right]\).\\
We note that Assumption B. 1 could be dropped if we were to study a slight variation of \(\widehat{s e}_{v, p}^{2}(h ; \widehat{\eta})\) in which we take the \(R\) nearest neighbors of unit \(i\) in terms of running variable values among units in the same fold to compute \(\widehat{\sigma}_{i}^{2}(\widehat{\eta})\). However, proceeding like this would mean that existing software packages that compute nearest neighbor standard errors would have to be adapted, and could not be applied directly to the modified data \(\left\{\left(X_{i}, M_{i}(\widehat{\eta})\right)\right\}_{i \in[n]}\).\\
B.2. Confidence intervals. In this subsection, we discuss three types of confidence intervals for the RD parameter, based on undersmoothing, robust bias correction, and bias-aware critical values, respectively.\\
B.2.1. Undersmoothing. We first consider confidence intervals that are based on an undersmoothing bandwidth of order \(o\left(n^{-1 / 5}\right)\). This choice of bandwidth implies that the smoothing bias shrinks\\
to zero at a faster rate than the standard deviation and can hence be ignored when constructing confidence intervals. Let

\[
C I_{1-\alpha}^{u s}=\left[\widehat{\tau}(h ; \widehat{\eta}) \pm z_{\alpha} \widehat{\operatorname{se}}(h ; \widehat{\eta})\right]
\]

where \(z_{\alpha}\) is the \(1-\alpha / 2\) quantile of the standard normal distribution. Proposition B. 2 shows that \(C I_{1-\alpha}^{u s}\) is asymptotically valid.

Proposition B.2. Suppose that the assumptions of Proposition B. 1 hold for \(p=1\). If \(n h^{5}=o(1)\), then \(\mathbb{P}\left(\tau \in C I_{1-\alpha}^{u s}\right) \geq 1-\alpha+o_{p}(1)\).\\
B.2.2. Robust bias correction. We now adapt the robust bias corrections of Calonico et al. (2014) to our setting. To keep the exposition transparent, we focus on the important special case where the bandwidth used to obtain the bias correction is the same as the main bandwidth. In this case, the local linear estimator with a bias correction is numerically equal to the local quadratic estimator (with the same bandwidth), i.e. \(\widehat{\tau}_{0,2}(h ; \widehat{\eta})\). Let

\[
C I_{1-\alpha}^{r b c}=\left[\widehat{\tau}_{0,2}(h ; \widehat{\eta}) \pm z_{\alpha} \widehat{\mathrm{se}}_{0,2}(h ; \widehat{\eta})\right]
\]

Proposition B. 3 shows that \(C I_{1-\alpha}^{r b c}\) is asymptotically valid.\\
Proposition B.3. Suppose that the assumptions of Theorem A.2 and Proposition B. 1 hold for \(p=2\). If \(n h^{7}=o(1)\), then \(\mathbb{P}\left(\tau \in C I_{1-\alpha}^{r b c}\right) \geq 1-\alpha+o_{p}(1)\).\\
B.2.3. Bias-awareness. We consider a simplified version of the bias-aware approach of Armstrong and Kolesár (2018), which adjusts critical values to account for possible (asymptotic) bias. Suppose that the second derivative of the conditional expectation function of the outcome \(Y_{i}\) given the running variable \(X_{i}\) is bounded in absolute value by some constant \(B_{Y}\) on either side of the cutoff. Then it follows from the results of Armstrong and Kolesár (2020) and our Theorem 2 that the asymptotic bias of our covariate-adjusted RD estimator is bounded in absolute value by \(\bar{b}(h)+o_{P}\left(h^{2}\right)\), where

\[
\bar{b}(h)=-\frac{B_{Y}}{2} \sum_{i=1}^{n} w_{i}(h) X_{i}^{2} \operatorname{sign}\left(X_{i}\right)
\]

We note that this bound is independent of the chosen adjustment function. The proposed confidence interval is

\[
C I_{1-\alpha}^{b a}=\left[\widehat{\tau}(h ; \widehat{\eta}) \pm z_{\alpha}\left(\bar{b}(h) / \widehat{\operatorname{se}}_{0,1}(h ; \widehat{\eta})\right) \widehat{\operatorname{se}}_{0,1}(h ; \widehat{\eta})\right]
\]

where \(z_{\alpha}(r)\) is the \(1-\alpha / 2\) quantile of the absolute value of the normal distribution with mean \(r\) and variance one. Proposition B. 4 shows that \(C I_{1-\alpha}^{b a}\) is asymptotically valid.

Proposition B.4. Suppose that the assumptions of Proposition B. 1 hold for \(p=1\). If \(n h^{5}=O(1)\), then \(\mathbb{P}\left(\tau \in C I_{1-\alpha}^{b a}\right) \geq 1-\alpha+o_{p}(1)\).\\
B.3. Optimal bandwidth. In our Theorem 4, we show that the bandwidth that minimizes the Asymptotic Mean Squared Error (AMSE) of our proposed estimator is given by

\[
h_{A M S E}=\left(\frac{V(\bar{\eta})}{4 B_{\mathrm{base}}^{2}}\right)^{1 / 5} n^{-1 / 5}
\]

This optimal bandwidth can be consistently estimated by applying the procedure of Calonico et al. (2014, s.6) to the modified data \(\left\{\left(X_{i}, M_{i}\left(\widehat{\eta}_{s(i)}\right)\right)\right\}_{i \in[n]}\) using the following three steps. Step 0. Initial bandwidths.\\
(i) Let \(v_{n}\) be such that \(v_{n} \rightarrow 0\) and \(n v_{n} \rightarrow \infty\). In practice, set \(\widehat{v}_{n}=2.58 \min \left\{S_{X}, I Q R_{X} / 1.349\right\} n^{-1 / 5}\), where \(S_{X}^{2}\) and \(I Q R_{X}\) denote, respectively, the sample variance and interquartile range of \(\left\{X_{i}: 1 \leq i \leq n\right\}\).\\
(ii) Choose \(c_{n}\) such that \(c_{n} \rightarrow 0\) and \(n c_{n}^{7} \rightarrow \infty\). In practice, let

\[
\widehat{c}_{n}=\widehat{C}_{n}^{1 / 9} n^{-1 / 9}, \quad \widehat{C}_{n}=\frac{7 n v_{n}^{7} \widehat{s e}_{3,3}\left(v_{n} ; \widehat{\eta}\right)}{2 \mathcal{B}_{3,3}^{2}\left(\widehat{\gamma}_{4,4}^{+}(\widehat{\eta})-\widehat{\gamma}_{4,4}^{-}(\widehat{\eta})\right)^{2}}
\]

where \(\widehat{\gamma}_{4,4}^{\star}(\widehat{\eta})\) is the coefficient on \((1 / 4!) X_{i}^{4}\) in the fourth-order global polynomial regression of \(M_{i}\left(\widehat{\eta}_{s(i)}\right)\) on a constant, \(X_{i},(1 / 2!) X_{i}^{2},(1 / 3!) X_{i}^{3}\), and \((1 / 4!) X_{i}^{4}\), using the data on the respective side of the cutoff, and \(\mathcal{B}_{v, p}^{\star}\) for \(\star \in\{+,-\}\) is the kernel constant in the leading bias term of \(\widehat{\beta}_{v, p}^{\star}(h ; \widehat{\eta})\).

Step 1. Choose a pilot bandwidth \(b_{n}\) such that \(b_{n} \rightarrow 0\) and \(n b_{n}^{5} \rightarrow \infty\). In practice, use the following estimate of the bandwidth that minimizes the AMSE of the estimates of the second derivative terms in a local quadratic regression:

\[
\widehat{b}_{n}=\widehat{B}_{n}^{1 / 7} n^{-1 / 7}, \quad \widehat{B}_{n}=\frac{5 n v_{n}^{5} \widehat{s e}_{2,2}\left(v_{n} ; \widehat{\eta}\right)}{2 \mathcal{B}_{2,2}^{2}\left(\left(\widehat{\beta}_{3,3}^{+}\left(c_{n}, \widehat{\eta}\right)+\widehat{\beta}_{3,3}^{-}\left(c_{n} ; \widehat{\eta}\right)\right)^{2}+3 \widehat{s e}_{3,3}\left(c_{n} ; \widehat{\eta}\right)\right)}
\]

Step 2. Estimate \(h_{A M S E}\) by

\[
\widehat{h}_{n}=\widehat{H}_{n}^{1 / 5} n^{-1 / 5}, \quad \widehat{H}_{n}=\frac{n v_{n} \widehat{s e}_{0,1}\left(v_{n} ; \widehat{\eta}\right)}{4 \mathcal{B}_{0,1}^{2}\left(\left(\widehat{\beta}_{2,2}^{+}\left(b_{n} ; \widehat{\eta}\right)-\widehat{\beta}_{2,2}^{-}\left(b_{n} ; \widehat{\eta}\right)\right)^{2}+3 \widehat{s e}_{2,2}\left(b_{n} ; \widehat{\eta}\right)\right)}
\]

Proposition B.5. Suppose that the assumptions of Theorem A.2 and Proposition B. 1 hold for \(p=3\), \(\mathcal{X}\) is bounded, \(\mathbb{P}\left[1 / C \leq\left|\widehat{\gamma}_{4,4}^{+}(\bar{\eta})-\widehat{\gamma}_{4,4}^{-}(\bar{\eta})\right| \leq C\right] \rightarrow 1\) for some \(C>0\), and Assumption 1 holds with \(\mathcal{X}_{h}\) replaced by \(\mathcal{X}\). Suppose that \(\beta_{v}^{+}(\bar{\eta})-(-1)^{v+1} \beta_{v}^{-}(\bar{\eta})\) is bounded and bounded away from zero for \(v \in\{2,3\}\). Then \(\widehat{c}_{n} \xrightarrow{p} 0, n \widehat{c}_{n}^{7} \xrightarrow{p} \infty, \widehat{b}_{n} \xrightarrow{p} 0, n \widehat{b}_{n}^{5} \xrightarrow{p} \infty\), and \(\widehat{h}_{n} / h_{A M S E} \xrightarrow{p} 1\).

\section*{B.4. Proofs of Propositions B.1-B.5.}
B.4.1. Proof of Proposition B.1. To begin with, note that standard kernel calculations show that: (i) \(\sum_{i \in[n]} w_{i, v, p}(h)^{2}=O_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\) and (ii) \(\max _{i \in[n]} w_{i, v, p}(h)^{2}=o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\). The proof of Proposition B. 1 then requires showing that \(\widehat{\operatorname{se}}_{v, p}^{2}(h ; \widehat{\eta})\) is asymptotically equivalent to the following infeasible version of itself, which uses the deterministic function \(\bar{\eta}\) :

\[
\widehat{\operatorname{se}}_{v, p}^{2}(h ; \bar{\eta})=\sum_{i \in[n]} w_{i, v, p}^{2}(h)\left(M_{i}(\bar{\eta})-\frac{1}{R} \sum_{j \in \mathcal{R}_{i}} M_{j}(\bar{\eta})\right)^{2} .
\]

Using arguments as in the proof of Theorem 4 in Noack and Rothe (2024), one can show that \(\widehat{\mathrm{se}}_{v, p}^{2}(h ; \bar{\eta})-\mathrm{se}_{v, p}^{2}(h ; \bar{\eta})=o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\). It therefore remains to show that \(\widehat{\mathrm{se}}_{v, p}^{2}(h ; \widehat{\eta})-\widehat{\mathrm{se}}_{v, p}^{2}(h ; \bar{\eta})=\) \(o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\). We express this difference as the sum of terms that are linear in \(M_{i}\left(\widehat{\eta}_{s(i)}\right)-M_{i}(\bar{\eta})=\) \(\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s(i)}\left(Z_{i}\right)\) and a quadratic remainder:

\[
\begin{aligned}
& \widehat{\operatorname{se}}_{v, p}^{2}(h ; \widehat{\eta})-\widehat{\operatorname{se}}_{v, p}^{2}(h ; \bar{\eta}) \\
& =2 \sum_{i \in[n]} w_{i, v, p}^{2}(h)\left(M_{i}(\bar{\eta})-\frac{1}{R} \sum_{j \in \mathcal{R}_{i}} M_{j}(\bar{\eta})\right)\left(M_{i}\left(\widehat{\eta}_{s(i)}\right)-M_{i}(\bar{\eta})-\frac{1}{R} \sum_{j \in \mathcal{R}_{i}}\left(M_{j}\left(\widehat{\eta}_{s(j)}\right)-M_{j}(\bar{\eta})\right)\right) \\
& \quad+\sum_{i \in[n]} w_{i, v, p}^{2}(h)\left(M_{i}\left(\widehat{\eta}_{s(i)}\right)-M_{i}(\bar{\eta})-\frac{1}{R} \sum_{j \in \mathcal{R}_{i}}\left(M_{j}\left(\widehat{\eta}_{s(j)}\right)-M_{j}(\bar{\eta})\right)\right)^{2} \\
& \equiv A_{1}+2 A_{2} .
\end{aligned}
\]

We first consider \(A_{2}\). Let \(C\) denote a generic constant that might change from line to line. It holds that

\[
\begin{aligned}
\frac{1}{C} A_{2} & \leq \sum_{i=1}^{n} w_{i, v, p}^{2}(h)\left(\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2}+\frac{1}{R} \sum_{j \in \mathcal{R}_{i}}\left(\widehat{\eta}_{s(j)}\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right)^{2}\right) \\
& \leq \sum_{i=1}^{n}\left(w_{i, v, p}^{2}(h)+\frac{C}{R} \sum_{j: i \in R_{j}} w_{j, v, p}^{2}(h)\right)\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2} \\
& =\sum_{s \in[S]} \sum_{i \in I_{s}}\left(w_{i, v, p}^{2}(h)+\frac{C}{R} \sum_{j: i \in R_{j}} w_{j, v, p}^{2}(h)\right)\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2} \\
& \equiv \sum_{s \in[S]} A_{2, s} .
\end{aligned}
\]

For all \(s \in[S]\), it holds with probability approaching one that

\[
\begin{aligned}
\mathbb{E} & {\left[A_{2, s} \mid \mathbb{X}_{n},\left\{W_{i}\right\}_{i \in I_{s}^{c}}\right] } \\
& \leq \sum_{i \in I_{s}}\left(w_{i, v, p}^{2}(h)+\frac{C}{R} \sum_{j: i \in R_{j}} w_{j, v, p}^{2}(h)\right) \sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \mathcal{X}_{h}} \mathbb{E}\left[\left(\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2} \mid X_{i}=x\right] \\
& \leq C \sum_{i=1}^{n} w_{i, v, p}^{2}(h) \sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \mathcal{X}_{h}} \mathbb{E}\left[\left(\eta\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)^{2} \mid X_{i}=x\right]=O_{P}\left(\left(n h^{1+2 v}\right)^{-1} r_{n}^{2}\right) .
\end{aligned}
\]

As \(S\) is finite and \(A_{2, s}\) is a positive random variable, it follows that \(A_{2}=o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\).\\
To show that \(A_{1}\) is of order \(o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\), we separate the terms involving the nearest neighbors in the fold of unit \(i\) and those that involve at least one neighbor from a different fold. Specifically, we have that:

\[
\begin{aligned}
A_{1}= & \frac{1}{R^{2}} \sum_{i \in[n]} w_{i, v, p}^{2}(h)\left(\sum_{j, l \in \mathcal{R}_{i}}\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)\left(\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)-\left(\widehat{\eta}_{s(j)}\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right)\right)\right) \\
= & \frac{1}{R^{2}} \sum_{i \in[n]} w_{i, v, p}^{2}(h)\left(\sum_{\substack{(j, l) \in \mathcal{R}_{i}^{2} \\
(j, l) \notin I_{s(i)}^{2}}}\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)\left(\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)-\left(\widehat{\eta}_{s(j)}\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right)\right)\right) \\
& +\frac{1}{R^{2}} \sum_{s \in[S]} \sum_{i \in I_{s}} w_{i, v, p}^{2}(h)\left(\sum_{j, l \in \mathcal{R}_{i} \cap I_{s}}\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)\left(\left(\widehat{\eta}_{s(i)}\left(Z_{i}\right)-\bar{\eta}\left(Z_{i}\right)\right)-\left(\widehat{\eta}_{s(j)}\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right)\right)\right) \\
\equiv & A_{1,1}+\frac{1}{R^{2}} \sum_{s \in[S]} A_{1,2, s} .
\end{aligned}
\]

By Assumption B.1, it holds that \(A_{1,1}=o_{P}\left(\left(n h^{1+2 v}\right)^{-1}\right)\). For all \(s \in[S]\), it holds with probability approaching one that

\[
\begin{aligned}
& \mathbb{E}\left[\left|A_{1,2, s}\right| \mid \mathbb{X}_{n},\left\{W_{i}\right\}_{\left.i \in I_{s}^{c}\right]}\right. \\
& \quad \leq \sum_{i \in I_{s}} w_{i, v, p}^{2}(h) \sum_{j, l \in\left(\mathcal{R}_{i} \cap I_{s}\right) \cup\{i\}} \mathbb{E}\left[\mid\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)\left(\widehat{\eta}_{s(j)}\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right) \| \mathbb{X}_{n},\left\{W_{i}\right\}_{i \in I_{s}^{c}}\right] \\
& \quad \leq \sum_{i \in I_{s}} w_{i, v, p}^{2}(h) \sum_{j, l \in\left(\mathcal{R}_{i} \cap I_{s}\right) \cup\{i\}} \sup _{n \in \mathcal{T}_{n}} \mathbb{E}\left[\mid\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)\left(\eta\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right) \| \mathbb{X}_{n}\right] \\
& \quad \leq \sum_{i \in I_{s}} w_{i, v, p}^{2}(h) \sum_{j, l \in\left(\mathcal{R}_{i} \cap I_{s}\right) \cup\{i\}}\left(\mathbb{E}\left[\left(M_{i}(\bar{\eta})-M_{l}(\bar{\eta})\right)^{2} \mid \mathbb{X}_{n}\right] \sup _{\eta \in \mathcal{T}_{n}} \mathbb{E}\left[\left(\eta\left(Z_{j}\right)-\bar{\eta}\left(Z_{j}\right)\right)^{2} \mid \mathbb{X}_{n}\right]\right)^{1 / 2} \\
& \quad=O_{P}\left(\left(n h^{1+2 v}\right)^{-1} r_{n}\right),
\end{aligned}
\]

where the last equality follows from Assumption 1 and the assumption of bounded second moments. Hence, \(A_{1,2, s}=o_{p}\left(\left(n h^{1+2 v}\right)^{-1}\right)\), which concludes this proof.\\
B.4.2. Proof of Proposition B.2. The validity of the CI follows directly from the asymptotic normality of the local linear estimator established in Theorem A. 2 and the fact that the standard error is consistent.\\
B.4.3. Proof of Proposition B.3. Validity of the CI follows directly from asymptotic normality of the local quadratic estimator established in Theorem A. 2 and the fact that the standard error is consistent.\\
B.4.4. Proof of Proposition B.4. Validity of the CI follows directly from asymptotic normality of the local linear estimator established in Theorem A.2, the fact that the standard error is consistent, and that the asymptotic bias is bounded in absolute value by \(\bar{b}(h)+o_{P}\left(h^{2}\right)\).\\
B.4.5. Proof of Proposition B.5. The proposition follows, using the consistency of the standard error established in Proposition B.1, if the following claims hold:\\
(i) \(\widehat{\gamma}_{4,4}^{\star}(\widehat{\eta})-\widehat{\gamma}_{4,4}^{\star}(\bar{\eta})=o_{P}(1)\),\\
(ii) \(\widehat{\beta}_{3,3}^{+}\left(c_{n} ; \widehat{\eta}\right)+\widehat{\beta}_{3,3}^{-}\left(c_{n} ; \widehat{\eta}\right)=\beta_{3}^{+}(\bar{\eta})+\beta_{3}^{-}(\bar{\eta})+o_{P}(1)\),\\
(iii) \(\widehat{\beta}_{2,2}^{+}\left(b_{n} ; \widehat{\eta}\right)-\widehat{\beta}_{2,2}^{-}\left(b_{n} ; \widehat{\eta}\right)=\beta_{2}^{+}(\bar{\eta})-\beta_{2}^{-}(\bar{\eta})+o_{P}(1)\).

Part (i). First, note that

\[
\widehat{\gamma}_{4,4}^{\star}(\widehat{\eta})-\widehat{\gamma}_{4,4}^{\star}(\bar{\eta})=e_{4}^{\prime}\left(\sum_{i=1}^{n} \tilde{X}_{4, i}^{\star} \widetilde{X}_{4, i}^{\star^{\top}}\right)^{-1} \sum_{i=1}^{n} \tilde{X}_{4, i}^{\star}\left(\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s(i)}\left(Z_{i}\right)\right)
\]

where \(\widetilde{X}_{4, i}^{+}=\widetilde{X}_{4, i} \mathbf{1}\left\{X_{i} \geq 0\right\}\) and \(\widetilde{X}_{4, i}^{-}=\widetilde{X}_{4, i} \mathbf{1}\left\{X_{i}<0\right\}\). Further, for \(s \in[S]\), we have that

\[
\left|\frac{S}{n} \sum_{i \in I_{s}} X_{i}^{j}\left(\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s}\left(Z_{i}\right)\right)\right| \leq \sqrt{\frac{S}{n} \sum_{i \in I_{s}} X_{i}^{2 j}} \sqrt{\frac{S}{n} \sum_{i \in I_{s}}\left(\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s}\left(Z_{i}\right)\right)^{2}}
\]

Note that, with probability approaching one,

\[
\begin{aligned}
\mathbb{E}\left[\left.\frac{S}{n} \sum_{i \in I_{s}}\left(\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s}\left(Z_{i}\right)\right)^{2} \right\rvert\, \mathbb{X}_{n},\left(W_{j}\right)_{j \in I_{s}^{c}}\right] & \leq \sup _{\eta \in \mathcal{T}_{n}} \mathbb{E}\left[\left.\frac{S}{n} \sum_{i \in I_{s}}\left(\bar{\eta}\left(Z_{i}\right)-\eta\left(Z_{i}\right)\right)^{2} \right\rvert\, \mathbb{X}_{n}\right] \\
& \leq \sup _{\eta \in \mathcal{T}_{n}} \sup _{x \in \mathcal{X}} \mathbb{E}\left[\left(\bar{\eta}\left(Z_{i}\right)-\eta\left(Z_{i}\right)\right)^{2} \mid X_{i}=x\right]=o(1)
\end{aligned}
\]

It follows that \(\left|\frac{S}{n} \sum_{i \in I_{s}} X_{i}^{j}\left(\bar{\eta}\left(Z_{i}\right)-\widehat{\eta}_{s(i)}\left(Z_{i}\right)\right)\right|=o_{p}(1)\). Since \(\mathcal{X}\) is bounded, the claim follows.\\
Part (ii) and (iii). Using steps as in the proof of Theorem A.1, for \(p \in\{2,3\}\), we obtain that \(\widehat{\beta}_{p, p}^{\star}(h ; \widehat{\eta})-\widehat{\beta}_{p, p}^{\star}(h, \bar{\eta})=o_{P}(1)\). Moreover, under the assumptions made, \(\widehat{\beta}_{p, p}^{\star}(h, \bar{\eta})-\beta_{p}^{\star}(\bar{\eta})=\) \(O_{P}\left(h+\left(n h^{1+2 p}\right)^{-1 / 2}\right)\). The claims follow using the conditions on \(b_{n}\) and \(c_{n}\).

\section*{C. DETAILS ON THE LITERATURE REANALYSIS}
In this section, we provide additional details on the practical performance described in Section 4.\\
C.1. Data Collection. We conducted an extensive literature search in order to document how covariates are used in empirical RD designs and to collect data sets on which to compare our proposed method to the existing approaches. We focused on the publications in AER, AER Insights, AEJ: Applied Economics, AEJ: Economic Policy, and AEA Papers and Proceedings between 2018 and 2023. Starting from a Google Scholar search for the keywords "regression discontinuity", \({ }^{17}\) we first identified 74 articles that appeared to fit into our theoretical framework, \({ }^{18}\) and then retained those 16 papers for which the journal's replication package contained all the data used in the empirical analysis. In 14 of these papers, covariates were used in at least one of the reported RD regressions, while in two papers the available covariates were used only for balance checks but could in principle have been used in the RD regressions, too. For each paper, we identified the main specification (or a version thereof) that includes covariates. These specifications often involve multiple outcomes or running variables, which yielded a total of 56 specifications. The details on all of them are given in Table S2 in the Online Supplement. In our reanalysis of these papers, we focus on these main specifications. In the two cases where only a no covariates RD analysis is reported, we included the covariates that were used for covariates balance checks.\\
C.2. Implementation Details. We apply our flexible adjustment RD estimator proposed in Section 3, and we contrast it with the no covariates, conventional linear, and cross-fitted localized linear adjustment RD estimators. In general, the flexible adjustment is implemented as an ensemble of eight learners listed in Section 3.4 and we use \(B=25\) data splits. For three specifications with more than 100,000 observations, we speed up the computations by considering only the local versions of machine learning methods and using \(B=5\) data splits. For one specification where the number of observations times covariates exceeds \(500,000,000\), we use only the local version of random forest as our flexible adjustment and consider \(B=1\) data split. For the bias-aware approach, we calibrated smoothness constants via the rule of thumb of Armstrong and Kolesár (2020). This choice was dictated by practical considerations, as it would not be possible to separately discuss the choice of smoothness bound for each of the 56 specifications. While one can argue whether the resulting smoothness bound is always appropriate, the qualitative conclusions about the relative reductions in the confidence interval length are not too sensitive to the choice of the smoothness bound.

\section*{REFERENCES}
Andrews, D. (1994): "Asymptotics for semiparametric econometric models via stochastic equicontinuity," Econometrica, 62, 43-72.

\footnotetext{\({ }^{17}\) A majority of papers found through the Google Scholar search did not conduct an original RD analysis, but only cited other RD papers, and were hence excluded.\\
\({ }^{18}\) We excluded geographic RD designs where boundary fixed effects were included as part of the identification strategy, and a small number of other nonstandard RD analyses where the outcome variable was measured at a higher level of aggregation than the running variable or a donut design was used.
}Arai, Y., T. Otsu, and M. H. Seo (2024): "Regression Discontinuity Design with Potentially Many Covariates," arXiv preprint arXiv:2109.08351.

Armstrong, T. B. and M. Kolesír (2018): "Optimal inference in a class of regression models," Econometrica, 86, 655-683.\\
-\_ (2020): "Simple and honest confidence intervals in nonparametric regression," Quantitative Economics, 11, 1-39.

Belloni, A., V. Chernozhukov, I. Fernández-Val, and C. Hansen (2017): "Program Evaluation and Causal Inference With High-Dimensional Data," Econometrica, 85, 233-298.

Calonico, S., M. D. Cattaneo, M. H. Farrell, and R. Titiunik (2019): "Regression Discontinuity Designs Using Covariates," Review of Economics and Statistics, 101, 442-451.

Calonico, S., M. D. Cattaneo, and R. Titiunik (2014): "Robust nonparametric confidence intervals for regression-discontinuity designs," Econometrica, 82, 2295-2326.

Cattaneo, M. D., N. Idrobo, and R. Titiunik (2019): A practical introduction to regression discontinuity designs: Foundations, Cambridge University Press.

Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins (2018): "Double/debiased machine learning for treatment and structural parameters," Econometrics Journal, 21, C1-C68.

Chernozhukov, V., W. Newey, J. Robins, and R. Singh (2019): "Double/de-biased machine learning of global and local parameters using regularized Riesz representers," Working Paper.

Colangelo, K. and Y.-Y. Lee (2022): "Double debiased machine learning nonparametric inference with continuous treatments," Working Paper.

Dong, Y. (2018): "Alternative Assumptions to Identify LATE in Fuzzy Regression Discontinuity Designs," Oxford Bulletin of Economics and Statistics, 80, 1020-1027.

Fan, J. and I. Gijbels (1996): Local polynomial modelling and its applications, Chapman \& Hall/CRC.

Fan, Q., Y.-C. Hsu, R. P. Lieli, and Y. Zhang (2020): "Estimation of Conditional Average Treatment Effects With High-Dimensional Data," Journal of Business \& Economic Statistics, 0, 1-15.

Frölich, M. and M. Huber (2019): "Including Covariates in the Regression Discontinuity Design," Journal of Business छ Economic Statistics, 37, 736-748.

Gerard, F., M. Rokkanen, and C. Rothe (2020): "Bounds on treatment effects in regression discontinuity designs with a manipulated running variable," Quantitative Economics, 11, 839-870.

Hahn, J. (1998): "On the role of the propensity score in efficient semiparametric estimation of average treatment effects," Econometrica, 66, 315-331.

Hahn, J., P. Todd, and W. Van der Klaauw (2001): "Identification and Estimation of Treatment Effects with a Regression-Discontinuity Design," Econometrica, 69, 201-209.

Imbens, G. and K. Kalyanaraman (2012): "Optimal bandwidth choice for the regression discontinuity estimator," Review of Economic Studies, 79, 933-959.

Imbens, G. W. and T. Lemieux (2008): "Regression discontinuity designs: A guide to practice," Journal of Econometrics, 142, 615-635.

Kennedy, E. H. (2020): "Optimal doubly robust estimation of heterogeneous causal effects," arXiv preprint arXiv:2004.14497.

Kennedy, E. H., Z. Ma, M. D. McHugh, and D. S. Small (2017): "Nonparametric methods for doubly robust estimation of continuous treatment effects," Journal of the Royal Statistical Society. Series B, Statistical Methodology, 79, 1229.

Kreiss, A. and C. Rothe (2023): "Inference in regression discontinuity designs with highdimensional covariates," Econometrics Journal.

Lee, D. S. and T. Lemieux (2010): "Regression discontinuity designs in economics," Journal of Economic Literature, 48, 281-355.

Londoño-Vélez, J., C. Rodríguez, and F. Sánchez (2020): "Upstream and downstream impacts of college merit-based financial aid for low-income students: Ser Pilo Paga in Colombia," American Economic Journal: Economic Policy, 12, 193-227.

McCrary, J. (2008): "Manipulation of the running variable in the regression discontinuity design: A density test," Journal of Econometrics, 142, 698-714.

Newey, W. (1994): "The Asymptotic Variance of Semiparametric Estimators," Econometrica, 62, 1349-1382.

Noack, C. and C. Rothe (2024): "Bias-aware inference in fuzzy regression discontinuity designs," Econometrica, 92, 687-711.

Robins, J. M. and A. Rotnitzky (2001): "Comment on "Inference for semiparametric models: some questions and an answer" by P. Bickel and J. Kwon," Statistica Sinica, 11, 920-936.

Su, L., T. Ura, and Y. Zhang (2019): "Non-separable models with high-dimensional data," Journal of Econometrics, 212, 646-677.

Van der Laan, M. J., E. C. Polley, and A. E. Hubbard (2007): "Super Learner," Statistical applications in genetics and molecular biology, 6.

Wager, S., W. Du, J. Taylor, and R. J. Tibshirani (2016): "High-dimensional regression adjustments in randomized experiments," Proceedings of the National Academy of Sciences, 113, 12673-12678.

\section*{Flexible Covariate Adjustments in Regression Discontinuity Designs \\
 Online Supplement }
July 10, 2024

\begin{abstract}
This Online Supplement contains additional empirical and simulation results.
\end{abstract}

\section*{A. ADDITIONAL EMPIRICAL RESULTS}
In Figure S1, we present the full results of our empirical analysis for bias-aware inference and robust bias correction. The first two graphs in Panel A are discussed in the main text. The third graph concerns the cross-fitted localized linear adjustment. We note that this adjustment can lead to wider confidence intervals than the no covariates confidence intervals in settings where the number of covariates is large relative to the effective sample size, as discussed in Simulation II. The fourth graph shows that the conventional linear adjustment yields substantially shorter confidence intervals than its cross-fitted counterpart. However, most of the gains are in settings where the effective sample size is small relative to the number of covariates. As discussed in Simulation II, the conventional standard errors might be unreliable in such settings. The results with the robust bias correction in Panel B follow broadly similar patterns. We note that in the fourth graph features a small number of applications where the conventional linear adjustment leads to an increase in the robust confidence interval length. This happens due to the selected bandwidth being smaller than the no covariates bandwidth in some cases. Such a pattern should not occur asymptotically but can be present in finite samples.

In 13 out of 16 papers in our literature analysis, the standard errors were clustered. To address that, in Figure S2, we present the results of our empirical analysis with clustered standard errors. In

\footnotetext{First version: July 16, 2021. This version: July 10, 2024. The authors gratefully acknowledge financial support by the European Research Council (ERC) through grant SH1-77202. The second author also gratefully acknowledges support from the European Research Council through Starting Grant No. 852332. Author contact information: Claudia Noack, Department of Economics, University of Bonn. E-Mail: \href{mailto:claudia.noack@uni-bonn.de}{claudia.noack@uni-bonn.de}. Website: \href{https://claudianoack.github.io}{https://claudianoack.github.io}. Tomasz Olma, Department of Statistics, Ludwig Maximilian University of Munich. E-Mail: \href{mailto:t.olma@lmu.de}{t.olma@lmu.de}. Website: \href{https://tomaszolma.github.io}{https://tomaszolma.github.io}. Christoph Rothe, Department of Economics, University of Mannheim. E-Mail: \href{mailto:rothe@vwl.uni-mannheim.de}{rothe@vwl.uni-mannheim.de}. Website: \href{http://www}{http://www}. \href{http://christophrothe.net}{christophrothe.net}.
}
our second-stage RD regression, we cluster based on the same variable as in the original application. Additionally, we adapt the data splitting procedure such that all observations within a cluster belong to the same fold. Clustering substantially increases the length of all confidence intervals, but the relative patterns displayed in Figure S2 are remarkably similar to those in Figure S1.

\section*{B. ADDITIONAL SIMULATION RESULTS}
In this section, we provide more details and additional results for the simulation studies in Section 7.\\
B.1. Scope for Efficiency Gains. To gauge the scope for efficiency gains due to covariate adjustments in this simulation setting, in Table S1, we present RD estimates at the placebo cutoff using all the observations in the restricted data set of Londoño-Vélez et al. (2020) described in Section 7.1. As in in the main text, we consider the original outcome and age as the dependent variables, and now we employ the robust bias correction approach in addition to the bias-aware inference. In Panel A, the results are very similar in all rows, which indicates that the covariates have virtually no explanatory power for the outcome and so the covariate adjustments do not lead to meaningful changes in the length of confidence intervals. When considering the age as the dependent variable, the machine learning adjustments improve upon the no covariates and linear adjustment RD estimators, with our proposed flexible adjustment leading to the shortest confidence intervals.\\
B.2. Additional Results for Simulation I. Table S 3 extends the results in Table 1 from the main text and displays the results for all individual methods considered in our flexible adjustment. For all methods that employ cross-fitting, we consider their oracle versions obtained on the restricted data set. The observations about the performance of the flexible adjustment discussed in the main text apply here too. The confidence intervals are slightly conservative, the average standard error is very close to the standard deviation in all cases, and the changes in the bias across different adjustments are minimal relative to the standard deviation. The feasible and infeasible, oracle versions of the estimators perform very similarly. The flexible adjustment consistently leads to the shortest confidence intervals among all the adjustments employing cross-fitting. The results in Table S 4 are based on the robust bias correction but are otherwise analogous to the results in Table S3.

Figures S3 and S4 illustrate the asymptotic equivalence result in Theorem 1 of age as the dependent variable and bias-aware inference. \({ }^{1}\) Specifically, they show the difference between the simulated RD estimates based on the feasible adjustments and oracle adjustments for sample sizes of 2000 and 5000 . As a reference point, we also displayed the full distribution of the no covariates RD estimates. As predicted by our theory, RD estimates based on feasible and oracle adjustments

\footnotetext{\({ }^{1}\) The oracle and the feasible estimates are even more similar when using the original outcome as dependent variable, as the covariates do not have much explanatory power in this case. The results are also very similar when conducting inference based on robust bias correction.
}Table S1: Estimation results for the full restricted sample in the simulation setting of Section 7.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow[b]{2}{*}{Adjustment Method} & \multicolumn{5}{|c|}{Bias-Aware Inference} & \multicolumn{5}{|c|}{Robust Bias Correction} \\
\hline
 & \( \begin{aligned} & \text { Est } \\ & \text { x100 } \end{aligned} \) & \( \begin{aligned} & \text { SE } \\ & \text { x100 } \end{aligned} \) & Bandwidth & \begin{tabular}{l}
CI \\
Length x100 \\
\end{tabular} & \begin{tabular}{l}
CI \\
Length \% Red. \\
\end{tabular} & \( \begin{aligned} & \text { Est } \\ & \text { x100 } \end{aligned} \) & \( \begin{aligned} & \text { SE } \\ & \text { x100 } \end{aligned} \) & Bandwidth & \begin{tabular}{l}
CI \\
Length x100 \\
\end{tabular} & \begin{tabular}{l}
CI \\
Length \% Red. \\
\end{tabular} \\
\hline
\multicolumn{11}{|l|}{Panel A - Original Outcome} \\
\hline
No Covariates & 0.83 & 0.54 & 10.30 & 2.36 & 0.00 & 0.41 & 0.37 & 21.49 & 1.75 & 0.00 \\
\hline
Conventional Linear & 0.96 & 0.53 & 10.22 & 2.32 & 1.87 & 0.56 & 0.38 & 19.30 & 1.80 & -2.58 \\
\hline
Localized Linear & 0.98 & 0.53 & 10.22 & 2.32 & 1.84 & 0.57 & 0.39 & 19.16 & 1.80 & -2.92 \\
\hline
Global Linear & 0.98 & 0.53 & 10.22 & 2.32 & 1.83 & 0.59 & 0.39 & 19.14 & 1.80 & -2.89 \\
\hline
Localized Random Forest & 0.86 & 0.53 & 10.21 & 2.32 & 1.99 & 0.50 & 0.38 & 19.31 & 1.79 & -2.38 \\
\hline
Global Random Forest & 0.87 & 0.53 & 10.22 & 2.31 & 2.05 & 0.50 & 0.38 & 19.29 & 1.79 & -2.43 \\
\hline
Localized Boosted Trees & 0.88 & 0.53 & 10.21 & 2.32 & 2.01 & 0.48 & 0.38 & 19.75 & 1.77 & -1.31 \\
\hline
Global Boosted Tree & 0.87 & 0.53 & 10.22 & 2.32 & 1.96 & 0.49 & 0.38 & 19.83 & 1.77 & -1.18 \\
\hline
Localized Post-Lasso & 0.94 & 0.53 & 10.22 & 2.32 & 1.96 & 0.55 & 0.38 & 19.56 & 1.78 & -1.76 \\
\hline
Global Post-Lasso & 0.94 & 0.53 & 10.22 & 2.32 & 1.91 & 0.54 & 0.38 & 19.98 & 1.77 & -0.87 \\
\hline
Flexible & 0.89 & 0.53 & 10.21 & 2.31 & 2.07 & 0.50 & 0.38 & 19.55 & 1.78 & -1.71 \\
\hline
\multicolumn{11}{|l|}{Panel B - Age} \\
\hline
No Covariates & -9.89 & 8.36 & 6.60 & 38.04 & 0.00 & -7.42 & 5.24 & 15.52 & 23.46 & 0.00 \\
\hline
Conventional Linear & -10.71 & 7.46 & 6.24 & 33.97 & 10.69 & -5.77 & 4.49 & 15.69 & 20.23 & 13.77 \\
\hline
Localized Linear & -10.75 & 7.47 & 6.25 & 34.01 & 10.60 & -5.81 & 4.53 & 15.43 & 20.36 & 13.23 \\
\hline
Global Linear & -10.66 & 7.47 & 6.25 & 34.02 & 10.57 & -5.31 & 4.50 & 15.72 & 20.28 & 13.57 \\
\hline
Localized Random Forest & -9.69 & 7.36 & 6.19 & 33.40 & 12.19 & -5.10 & 4.29 & 16.14 & 19.25 & 17.96 \\
\hline
Global Random Forest & -9.22 & 7.30 & 6.19 & 33.20 & 12.73 & -4.51 & 4.25 & 16.36 & 19.06 & 18.73 \\
\hline
Localized Boosted Trees & -11.06 & 7.40 & 6.21 & 33.60 & 11.66 & -5.57 & 4.39 & 15.96 & 19.74 & 15.87 \\
\hline
Global Boosted Tree & -10.82 & 7.37 & 6.22 & 33.55 & 11.80 & -5.05 & 4.36 & 16.14 & 19.66 & 16.21 \\
\hline
Localized Post-Lasso & -10.74 & 7.47 & 6.25 & 34.00 & 10.61 & -5.91 & 4.52 & 15.49 & 20.32 & 13.38 \\
\hline
Global Post-Lasso & -10.71 & 7.47 & 6.25 & 34.02 & 10.57 & -5.36 & 4.50 & 15.71 & 20.28 & 13.54 \\
\hline
Flexible & -9.25 & 7.30 & 6.19 & 33.19 & 12.76 & -4.49 & 4.25 & 16.33 & 19.09 & 18.63 \\
\hline
\end{tabular}
\end{center}

Notes: Results are based on the restricted dataset of Londoño-Vélez et al. (2020) described in Section 7. Sample size is \(n=259,419\). The columns show the estimate (Est), the standard error (SE), the bandwidth (Bandwidth), the length of confidence intervals with \(95 \%\) nominal coverage (CI Length), and the percentage reduction in CI length relative to the no covariates CI length (CI Length \% Red.). Estimators are described in Section 3.\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-44(1)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-44(2)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-44(3)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-44(4)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-44(6)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-44(5)}\\
Figure S1: Full empirical results without clustering.\\
\includegraphics[max width=\textwidth]{2025_01_31_485d36515c76dfd75a4dg-44} data and Section 3 for more details on the estimator.\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-45}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-45(3)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-45(4)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-45(5)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-45(2)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-45(1)}\\
Figure S2: Full empirical results with clustering.\\
Notes: Results of our empirical literature reanalysis for bias-aware inference and robust bias correction with clustered standard errors. See Section 4 and Appendix C for details on the data and Section 3 for more details on the estimator.

Table S2: Overview of the papers of the literature analysis.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
 & Outcome & Running variable & Covariates & \( \begin{aligned} & \# \text { Covs } \\ & (\operatorname{not} 0 / 1) \end{aligned} \) & \#Obs & Eff \#Obs & \( \begin{aligned} & \text { Eff \#Obs } \\ & \text { /\#Covs } \end{aligned} \) & \#Clus & Covs in RD \\
\hline
\multicolumn{10}{|l|}{Akhtari et al. (AER 2022)} \\
\hline
\multicolumn{10}{|l|}{Table 3: "Political Turnover and Fourth Grade and Eighth-grade Test Scores"} \\
\hline
1 & Fourth-grade test scores & Incumbent's vote & Baseline school-level average & 14 (1) & 1,088,553 & 325,554 & 23,254 & 3737 & Yes \\
\hline
2 & Eighth grade test scores & margin & test scores; school- and individual-level controls; election-cycle indicator & 14 (1) & 446,451 & 234,629 & 17,545 & 2368 & Yes \\
\hline
\multicolumn{10}{|l|}{Altindag et al. (AEJAE 2022)} \\
\hline
\multicolumn{10}{|l|}{Table 4: "Effects of Curfew on Mental Health Outcomes"} \\
\hline
3 & Mental distress & \multirow[b]{2}{*}{number of months} & month, province, and & 175 (0) & 1868 & 475 & 2.7 & 144 & Yes \\
\hline
 & Somatic symptoms of distress &  & surveyor fixed effects, & 175 (0) & 1868 & 503 & 2.8 & 144 & Yes \\
\hline
5 & Nonsomatic symptoms of distress & month & indicator variables for education levels, ethnicity, & 175 (0) & 1868 & 478 & 2.7 & 144 & Yes \\
\hline
 & Sum of "Yes" answers in SRQ-20 &  & and gender & 175 (0) & 1868 & 475 & 2.7 & 144 & Yes \\
\hline
\multicolumn{10}{|l|}{Notes: The authors show results for different bandwidths. The reported effective sample sizes correspond to the optimal bandwidth calculated by Calonico et al. (2014) algorithm. In the main specification, month fixed effects are included and the standard error is clustered on the running variable. In our reanalysis, we do not cluster the standard errors on the running variable, and we do not include covariates that are a deterministic function of the running variable} \\
\hline
\end{tabular}
\end{center}

\section*{Ambrus et al. (AER 2020)}
Table 3: "Boundary Effects of Rental Prices"

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
7 & Log rental prices, 1853 &  & Determinants of rental & 14 (12) & 1738 & 469 & 34 & 179 & Yes \\
\hline
8 & Log rental prices, 1864 & Distance to & values, distance to various & 14 (12) & 1738 & 510 & 36 & 179 & Yes \\
\hline
9 & Log rental prices, 1894 & boundary & amenities, distance to & 5 (5) & 1879 & 363 & 73 & 179 & Yes \\
\hline
10 & Log rental prices, 1936 &  & presumed plague pit, and sewer access & 6 (6) & 793 & 221 & 37 & 90 & Yes \\
\hline
\end{tabular}
\end{center}

Table S2: Overview of the papers of the literature analysis.

\begin{center}
\begin{tabular}{ccclll}
\hline
Outcome & Running variable & Covariates & \begin{tabular}{l}
\#Covs \\
\((\) not \(0 / 1)\) \\
\end{tabular} & \begin{tabular}{l}
\#Obs \\
\end{tabular} & \begin{tabular}{l}
Eff \\
\#Obs \\
\end{tabular} \\
\hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\multicolumn{9}{|l|}{Asher and Novosad (AER 2020)} \\
\hline
\multicolumn{9}{|l|}{Table 3: "Impact of New Road on Indices of Major Outcomes"} \\
\hline
11 Transportation &  &  & 225 (8) & 11432 & 11432 & 51 & - & Yes \\
\hline
12 Occupation &  & controls for amenities and & 225 (8) & 11432 & 11432 & 51 & - & Yes \\
\hline
13 Firms & Village population & \includegraphics[max width=\textwidth]{2025_01_31_485d36515c76dfd75a4dg-47(2)}
 & 225 (8) & 10678 & 10678 & 48 & - & Yes \\
\hline
14 Production &  &  & 225 (8) & 11432 & 11432 & 51 & - & Yes \\
\hline
15 Consumption &  & district-cutoff fixed effects & 225 (8) & 11432 & 11432 & 51 & - & Yes \\
\hline
\end{tabular}
\end{center}

\section*{Avis et al. (AEJAE 2022)}
Table 4: "Effects of Campaign Spending Limits on Candidate Entry"

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
\# of candidates &  &  & 5 (5) & 5562 & 3080 & 616 & - & Yes \\
\hline
Eff. \# of candidates &  &  & 5 (5) & 5558 & 3052 & 610 & - & Yes \\
\hline
Small party &  &  & 5 (5) & 5562 & 3116 & 623 & - & Yes \\
\hline
Small party w/o incumbent &  &  & 5 (5) & 5562 & 2804 & 561 & - & Yes \\
\hline
Party's ideology index & maximum amount & Municipal controls: GDP & 5 (5) & 5562 & 2783 & 557 & - & Yes \\
\hline
Candidate's prop. to win & \includegraphics[max width=\textwidth]{2025_01_31_485d36515c76dfd75a4dg-47}
 & per capita, illiteracy, share & 5 (5) & 5459 & 3074 & 615 & - & Yes \\
\hline
Candidate's wealth & \includegraphics[max width=\textwidth]{2025_01_31_485d36515c76dfd75a4dg-47(1)}
 & urban, Gini coefficient, & 5 (5) & 5562 & 3218 & 644 & - & Yes \\
\hline
Candidate's political experience &  & population & 5 (5) & 5562 & 2849 & 570 & - & Yes \\
\hline
Candidate's gender &  &  & 5 (5) & 5562 & 3080 & 616 & - & Yes \\
\hline
Candidate's age &  &  & 5 (5) & 5562 & 3259 & 652 & - & Yes \\
\hline
Candidate's college degree &  &  & 5 (5) & 5562 & 2881 & 576 & - & Yes \\
\hline
Candidate: white &  &  & 5 (5) & 5562 & 2668 & 534 & - & Yes \\
\hline
\end{tabular}
\end{center}

\section*{Baskaran and Hessami (AEJEP 2018)}
Table 2: "Baseline Results: Rank Improvement of Female Candidates"\\
28 Rank improvement vote margin municipality characteristics - \(\quad\) No

Notes: We use 24 (24 non-binary) covariates from the robustness check in Table A.4.

Table S2: Overview of the papers of the literature analysis.

\begin{center}
\begin{tabular}{ccccll}
\hline
Outcome & Running variable & Covariates & \begin{tabular}{l}
\#Covs \\
\((\) not \(0 / 1)\) \\
\end{tabular} & \begin{tabular}{l}
\#Obs \\
\end{tabular} & \begin{tabular}{l}
Eff \\
\#Obs \\
\end{tabular} \\
\hline
\end{tabular}
\end{center}

\section*{Becker et al. (AER 2020)}
Table A.10: "Border Sample from the Diagnoza Survey"\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-48}

Notes: All RD results are in the appendix.

Chin (AEJAE 2023)\\
Table 2: "Effect on the Geographic Concentration of Voters" "and Table 4 Panel C in the Appendix\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-48(1)}

Notes: Additionally to the covariates used in the main text, we use all covariates that were used in Table 4 Panel C in the Appendix. As the number of observations of the original data set is very large and its distribution is very skewed around the cutoff, we restricted the sample to lie within three times of the bandwidth used in the main analysis around the cutoff. In the main specification, the author include the density of the population as a control, but we don't do this.

\section*{Curto-Grau et al. (AEJAE 2018)}
Table 1 A: "Average Effect of Partisan Alignment on Capital Transfers"

\begin{center}
\begin{tabular}{|c|}
\hline
\multirow[t]{2}{*}{37} \\
\hline
 \\
\hline
\end{tabular}
\end{center}

Table S2: Overview of the papers of the literature analysis.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Outcome & Running variable & Covariates & \( \begin{aligned} & \# \text { Covs } \\ & (\operatorname{not} 0 / 1) \end{aligned} \) & \#Obs & \begin{tabular}{l}
Eff \\
\#Obs \\
\end{tabular} & Eff \#Obs /\#Covs & \( \begin{aligned} & \text { \#Clusters Covs } \\ & \text { in RD } \end{aligned} \) \\
\hline
\end{tabular}
\end{center}

Notes: In their main specification, they include 14 fixed effects. We do not use them in our no-covariates RD estimator. For our RD estimators that use covariates, we also include all covariates that are used for from the falsification check of Figure A.10. This gives us a total of 25 ( 10 non-binary) covariates.

\section*{Granzier et al. (AEJAE 2023)}
Table 2: "Impact on Running in the Second Round and Winning"and Table C4

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
38 & Running & Vote Margin 1 vs 2 &  & 23 (8) & 45064 & 24544 & 1067 & 8970 & OA \\
\hline
39 & Winning & Vote Margin 1 vs 2 &  & 23 (8) & 45064 & 16054 & 698 & 8970 & OA \\
\hline
40 & Running & Vote Margin 2 vs 3 & gender, characteristics of & 23 (8) & 17730 & 10694 & 465 & 4810 & OA \\
\hline
41 & Winning & Vote Margin 2 vs 3 & previous election and party, & 23 (8) & 17730 & 8796 & 382 & 4810 & OA \\
\hline
42 & Running & Vote Margin 3 vs 4 & incumbent, strength & 23 (8) & 3956 & 2338 & 102 & 1243 & OA \\
\hline
43 & Winning & Vote Margin 3 vs 4 &  & 23 (8) & 3956 & 2232 & 97 & 1243 & OA \\
\hline
\end{tabular}
\end{center}

\section*{0 Greenstone et al. (AER Insigts 2022)}
Table 1: "Automating Air Quality Monitoring System and Reported \(\mathrm{PM}_{10}\) ", Column 2

\begin{center}
\begin{tabular}{llllllllllllllllll}
44 & \(\mathrm{PM}_{10}\) concentration & Days to automation & weather controls, and station & \(670(4)\) & \(1,049,325\) & 49,843 & 74 & 123 & Yes \\
\end{tabular}
\end{center} and month fixed effects

Notes: We do not include covariates that are determined based on the running variable and therefore exclude month fixed effects from our analysis.

\section*{Johnson (AER 2020)}
Table 2: "Instrumental Variables (IV) Estimate of the General Deterrence Effect of a Press Release on Compliance of Other Facilities "and Table A. 1

\begin{center}
\begin{tabular}{llllllllll}
45 & Number of Violations & Focal penalty &  & \(2(0)\) & 60,416 & 3302 & 1651 & 2746 & Yes \\
46 & Number of Violations & Focal penalty & construction, programmed & \(1(0)\) & 39,058 & 10,873 & 10,873 & 2455 & Yes \\
\end{tabular}
\end{center}

Notes: Second specification excludes inspections initiated by a serious accident worker complaint, or referral.\\
We further consider 30 covariates that are from Table A. 1 (, (press release, cfr, union, \# inspection prior tc, total violations prior tc)\\
Tuttle (AEJEP 2019)\\
Table 3: "Main Results: Effect of the SNAP Ban on Recidivism"

Table S2: Overview of the papers of the literature analysis.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
 & Outcome & Running variable & Covariates & \( \begin{aligned} & \text { \#Covs } \\ & (\operatorname{not} 0 / 1) \end{aligned} \) & \#Obs & \begin{tabular}{l}
Eff \\
\#Obs \\
\end{tabular} & Eff \#Obs /\#Covs & \( \# \mathrm{Cl}_{1} \) & Covs in RD \\
\hline
47 & Recidivism &  &  & 14 (4) & 18850 & 790 & 56 & 5385 & OA \\
\hline
48 & Financially motivated recidivism & Date & offender characteristics & 14 (4) & 18850 & 936 & 67 & 5385 & OA \\
\hline
49 & Non-financially motivated recidi- &  &  & 14 (4) & 18850 & 980 & 70 & 5385 & OA \\
\hline
\end{tabular}
\end{center}

Notes: In the main specification, dummies for weekdays are included and the standard error is clustered on the running variable. We do not cluster the standard errors on the running variable and we do not include covariates that are a deterministic functions of the running variable

\section*{Del Valle et al. (AEJAE 2020)}
Table 2: "Impact of Fonden on Night Lights"

50 difference in night lights \(\quad\) heavy rainfall index \begin{tabular}{ll}
 & characteristics of dwellings -2708 \\
 & quality, health care system, \\
 & education system, municipal \\
 & indicators, night lights, loca- \\
 & tion indicators, historic mean \\
 & annual rainfall \\
\end{tabular}

Notes: We use 24 ( 24 non-binary) covariates from the robustness check of Figure 4.

Londoño-Vélez et al. (AEJEP 2020)\\
Table 2: "Immediate Enrollment in any Postsecondary Education, by Type of Institution"and Table A. 4

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
\hline
 & Immediate Enrollment in Any Postsecondary Education & SABER 11 test score & indicators for gender, age, ethic, employment status, & 21 (2) & 273361 & 37882 & 1804 & - & OA \\
\hline
52 & Immediate Enrollment in Any Postsecondary Education & SISBEN wealth index & family size, parent's education, household residential stratum, high school schedule, and private high school & 21 (2) & 21071 & 8201 & 391 & - & OA \\
\hline
\end{tabular}
\end{center}

Table S2: Overview of the papers of the literature analysis.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|}
\hline
Outcome & Running variable & Covariates & \( \begin{aligned} & \text { \#Covs } \\ & (\operatorname{not} 0 / 1) \end{aligned} \) & \#Obs & \begin{tabular}{l}
Eff \\
\#Obs \\
\end{tabular} & Eff \#Obs /\#Covs & \( \begin{aligned} & \text { \#Clusters Covs } \\ & \text { in RD } \end{aligned} \) \\
\hline
\end{tabular}
\end{center}

\section*{Wasserman (AEJ-P\&P 2021)}
Table 2: "The Effect of Losing, by Gender"

\begin{center}
\begin{tabular}{lllllllllll}
53 & Probability Run again - Female & Margin of victory & fixed effects for state, & \(92(0)\) & 13092 & 3652 & 121 & 50 & Yes \\
54 & Probability Run again and Win & Margin of victory & \begin{tabular}{ll}
election-year, political party, & \(92(0)\) \\
\end{tabular} & 13092 & 3512 & 121 & 50 & Yes &  \\
 & - Female &  & and legislative chamber &  &  &  &  &  &  \\
\end{tabular}
\end{center}

Notes: The parameter of interest is the difference of the RD estimands of female and male candidates. Here, we consider these as two separate RD regressions.

Notes: The table shows description of the respective variables ("Outcome", "Running variable", "Covariates"); the number of covariates with the number of nonbinary covariates in parentheses ("\#Covs (not \(0 / 1\) )"); the total sample size ("\#Obs"); the number of observations within the bandwidth of the respective specification ("Eff \#Obs"); the effective sample size relative to the number of covariates ("Eff \#Obs / \#Covs"); the number of clusters ( "\#Clusters" ); and whether the covariates were used in the RD regression; "Yes" if they were used in the main text, "OA" if they were only used in the online appendix and "No" if they were not used ("Covs in RD").

Table S3: Full results for Simulation I with bias-aware inference.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow[b]{2}{*}{Adjustment method} & \multicolumn{8}{|c|}{Original Outcome} & \multicolumn{8}{|c|}{Age} \\
\hline
 & Mean SE x100 & \( \begin{aligned} & \text { SD } \\ & \text { x100 } \end{aligned} \) & \( \begin{aligned} & \text { Bias } \\ & \text { x100 } \end{aligned} \) & \( \begin{aligned} & \text { RMSE } \\ & \text { x100 } \end{aligned} \) & Mean Bandwidth & CI Cov in \% & \begin{tabular}{l}
Mean \\
CI \\
Length \\
x100 \\
\end{tabular} & \begin{tabular}{l}
Mean \\
CI \\
Length \% Red. \\
\end{tabular} & Mean SE x100 & \( \begin{aligned} & \text { SD } \\ & \text { x100 } \end{aligned} \) & \( \begin{gathered} \text { Bias } \\ \text { x100 } \end{gathered} \) & \( \begin{aligned} & \text { RMSE } \\ & \text { x100 } \end{aligned} \) & Mean Bandwidth & CI Cov in \% & \begin{tabular}{l}
Mean \\
CI \\
Length \\
x100 \\
\end{tabular} & \begin{tabular}{l}
Mean \\
CI \\
Length \% Red. \\
\end{tabular} \\
\hline
No Covariates & 2.57 & 2.56 & 0.38 & 2.59 & 23.31 & 97.10 & 11.25 & 0.00 & 38.39 & 38.79 & -7.53 & 39.51 & 14.66 & 97.85 & 173.82 & 0.00 \\
\hline
Conventional Linear & 2.50 & 2.52 & 0.44 & 2.56 & 23.16 & 96.95 & 10.97 & 2.49 & 33.41 & 34.19 & -6.66 & 34.83 & 13.92 & 97.54 & 152.55 & 12.24 \\
\hline
\multicolumn{17}{|l|}{Linear Regression} \\
\hline
Localized Feasible & 2.55 & 2.53 & 0.47 & 2.57 & 23.21 & 96.98 & 11.13 & 1.04 & 34.47 & 34.52 & -6.64 & 35.15 & 13.96 & 97.86 & 156.46 & 9.99 \\
\hline
Oracle & 2.53 & 2.51 & 0.47 & 2.56 & 23.15 & 96.90 & 11.06 & 1.68 & 34.04 & 34.10 & -6.64 & 34.74 & 13.91 & 97.85 & 154.66 & 11.02 \\
\hline
Global Feasible & 2.53 & 2.52 & 0.48 & 2.57 & 23.17 & 96.85 & 11.08 & 1.45 & 34.21 & 34.22 & -6.25 & 34.79 & 13.92 & 97.87 & 155.32 & 10.64 \\
\hline
Oracle & 2.53 & 2.51 & 0.48 & 2.56 & 23.15 & 96.91 & 11.06 & 1.68 & 34.11 & 34.10 & -6.25 & 34.67 & 13.90 & 97.88 & 154.88 & 10.90 \\
\hline
\multicolumn{17}{|l|}{Random Forest} \\
\hline
Localized Feasible & 2.56 & 2.55 & 0.48 & 2.59 & 23.28 & 97.02 & 11.20 & 0.42 & 34.27 & 34.43 & -6.05 & 34.95 & 13.93 & 97.86 & 155.58 & 10.50 \\
\hline
Oracle & 2.52 & 2.51 & 0.42 & 2.54 & 23.13 & 97.05 & 11.04 & 1.84 & 33.28 & 33.44 & -5.97 & 33.97 & 13.75 & 98.01 & 151.27 & 12.98 \\
\hline
Global Feasible & 2.55 & 2.54 & 0.47 & 2.58 & 23.22 & 96.97 & 11.15 & 0.86 & 33.77 & 33.90 & -5.86 & 34.40 & 13.84 & 97.97 & 153.41 & 11.74 \\
\hline
Oracle & 2.52 & 2.51 & 0.42 & 2.54 & 23.12 & 97.06 & 11.03 & 1.91 & 33.07 & 33.20 & -5.64 & 33.67 & 13.71 & 97.98 & 150.34 & 13.51 \\
\hline
\multicolumn{17}{|l|}{Boosted Trees} \\
\hline
Localized Feasible & 2.55 & 2.54 & 0.39 & 2.57 & 23.23 & 97.20 & 11.16 & 0.79 & 34.24 & 34.36 & -6.47 & 34.96 & 13.94 & 97.89 & 155.54 & 10.52 \\
\hline
Oracle & 2.52 & 2.51 & 0.41 & 2.54 & 23.13 & 97.13 & 11.04 & 1.87 & 33.57 & 33.70 & -6.59 & 34.33 & 13.82 & 97.91 & 152.64 & 12.18 \\
\hline
\multirow[t]{2}{*}{Global \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.53 & 2.52 & 0.42 & 2.56 & 23.16 & 97.01 & 11.08 & 1.48 & 33.85 & 33.99 & -6.34 & 34.57 & 13.87 & 97.90 & 153.85 & 11.49 \\
\hline
 & 2.52 & 2.51 & 0.42 & 2.55 & 23.13 & 97.06 & 11.04 & 1.84 & 33.56 & 33.63 & -6.46 & 34.25 & 13.81 & 97.90 & 152.54 & 12.24 \\
\hline
\multicolumn{17}{|l|}{Post-lasso} \\
\hline
\multirow[t]{2}{*}{Localized \(\begin{aligned} & \text { Feasible } \\ & \\ & \text { Oracle }\end{aligned}\)} & 2.55 & 2.53 & 0.41 & 2.56 & 23.22 & 97.06 & 11.14 & 0.94 & 34.37 & 34.54 & -6.83 & 35.20 & 13.96 & 97.87 & 156.12 & 10.19 \\
\hline
 & 2.52 & 2.51 & 0.45 & 2.55 & 23.14 & 97.02 & 11.04 & 1.81 & 34.04 & 34.11 & -6.78 & 34.77 & 13.91 & 97.85 & 154.68 & 11.01 \\
\hline
\multirow[t]{2}{*}{Global \(\begin{aligned} & \text { Feasible } \\ & \\ & \text { Oracle }\end{aligned}\)} & 2.54 & 2.52 & 0.42 & 2.56 & 23.18 & 96.95 & 11.09 & 1.38 & 34.22 & 34.25 & -6.66 & 34.89 & 13.93 & 97.91 & 155.40 & 10.60 \\
\hline
 & 2.53 & 2.51 & 0.46 & 2.55 & 23.14 & 96.99 & 11.04 & 1.79 & 34.11 & 34.10 & -6.27 & 34.67 & 13.91 & 97.91 & 154.89 & 10.89 \\
\hline
\multicolumn{17}{|l|}{Flexible} \\
\hline
Feasible & 2.53 & 2.52 & 0.44 & 2.56 & 23.15 & 96.98 & 11.05 & 1.71 & 33.52 & 33.69 & -6.25 & 34.26 & 13.85 & 97.93 & 152.58 & 12.22 \\
\hline
Oracle & 2.52 & 2.51 & 0.42 & 2.54 & 23.12 & 97.07 & 11.03 & 1.93 & 33.06 & 33.18 & -5.66 & 33.66 & 13.71 & 98.00 & 150.30 & 13.53 \\
\hline
\end{tabular}
\end{center}

Notes: Results are based on 10,000 Monte Carlo draws. The left panel shows the results for the original outcome and the right panel for age as the outcome based on Londoño-Vélez et al. (2020) and a sample size of \(n=5000\) (see Section 7 for details). The bandwidth is chosen and the confidence sets are constructed based on bias-aware inference. The columns show the simulated mean standard error (Mean SE), standard deviation (SD); simulated bias (Bias); root mean squared error (RMSE); average bandwidth (Mean Bandwidth), coverage of confidence intervals with 95\% nominal level (CI Cov); the average confidence interval length (Mean CI Length); and the reduction in mean CI length relative to the no covariates CI length (Mean CI Length \% Red.). The estimators are described in Section 3.

Table S4: Full results for Simulation I with robust bias correction.

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\multirow[b]{2}{*}{Adjustment method} & \multicolumn{8}{|c|}{Original Outcome} & \multicolumn{8}{|c|}{Age} \\
\hline
 & \begin{tabular}{l}
Mean SE \\
x100 \\
\end{tabular} & \( \begin{aligned} \hline & \text { SD } \\ & \text { x100 } \end{aligned} \) & \( \begin{aligned} & \text { Bias } \\ & \text { x100 } \end{aligned} \) & \( \begin{aligned} & \text { RMSE } \\ & \text { x100 } \end{aligned} \) & Mean Bandwidth & CI Cov in \% & \begin{tabular}{l}
Mean \\
CI \\
Length \\
x100 \\
\end{tabular} & \begin{tabular}{l}
Mean \\
CI \\
Length \\
\% Red. \\
\end{tabular} & Mean SE x100 & \( \begin{aligned} & \mathrm{SD} \\ & \mathrm{x} 100 \end{aligned} \) & \( \begin{gathered} \text { Bias } \\ \mathrm{x} 100 \end{gathered} \) & \( \begin{aligned} & \text { RMSE } \\ & \text { x100 } \end{aligned} \) & Mean Bandwidth & \( \begin{aligned} \hline & \mathrm{CI} \\ & \mathrm{Cov} \\ & \text { in \% } \end{aligned} \) & \begin{tabular}{l}
Mean \\
CI \\
Length \\
x100 \\
\end{tabular} & \begin{tabular}{l}
Mean \\
CI \\
Length \\
\% Red. \\
\end{tabular} \\
\hline
No Covariates & 2.72 & 2.94 & 0.49 & 2.98 & 21.30 & 94.50 & 12.72 & 0.00 & 32.28 & 34.90 & -5.45 & 35.32 & 21.08 & 94.80 & 150.42 & 0.00 \\
\hline
Conventional Linear & 2.64 & 2.89 & 0.58 & 2.95 & 21.15 & 93.97 & 12.35 & 2.91 & 27.68 & 29.96 & -4.08 & 30.24 & 20.79 & 94.39 & 129.04 & 14.22 \\
\hline
\multicolumn{17}{|l|}{Linear Regression} \\
\hline
Localized Feasible & 2.69 & 2.90 & 0.61 & 2.96 & 21.32 & 94.09 & 12.56 & 1.27 & 28.16 & 30.12 & -4.06 & 30.39 & 21.04 & 94.78 & 131.26 & 12.73 \\
\hline
Oracle & 2.67 & 2.87 & 0.60 & 2.94 & 21.30 & 94.21 & 12.45 & 2.13 & 27.87 & 29.80 & -4.01 & 30.07 & 21.01 & 94.83 & 129.92 & 13.63 \\
\hline
\multirow[t]{2}{*}{Global \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.67 & 2.89 & 0.62 & 2.95 & 21.31 & 94.08 & 12.48 & 1.86 & 28.03 & 29.92 & -3.75 & 30.16 & 21.05 & 94.95 & 130.70 & 13.11 \\
\hline
 & 2.67 & 2.88 & 0.61 & 2.94 & 21.30 & 94.22 & 12.45 & 2.11 & 27.93 & 29.81 & -3.70 & 30.03 & 21.04 & 94.91 & 130.21 & 13.44 \\
\hline
\multicolumn{17}{|l|}{Random Forest} \\
\hline
\multirow[t]{2}{*}{Localized \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.71 & 2.92 & 0.61 & 2.99 & 21.33 & 94.30 & 12.65 & 0.55 & 27.91 & 29.92 & -3.64 & 30.14 & 21.03 & 94.99 & 130.11 & 13.50 \\
\hline
 & 2.66 & 2.86 & 0.53 & 2.91 & 21.29 & 94.23 & 12.42 & 2.35 & 26.96 & 28.87 & -3.97 & 29.14 & 21.01 & 94.82 & 125.65 &  \\
\hline
\multirow[t]{2}{*}{Global \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.69 & 2.90 & 0.60 & 2.96 & 21.33 & 94.25 & 12.57 & 1.15 & 27.59 & 29.58 & -3.48 & 29.78 & 21.05 & 94.80 & 128.62 & 14.50 \\
\hline
 & 2.66 & 2.87 & 0.53 & 2.91 & 21.29 & 94.25 & 12.42 & 2.36 & 26.89 & 28.77 & -3.58 & 28.99 & 21.03 & 94.82 & 125.33 & 16.68 \\
\hline
\multicolumn{17}{|l|}{Boosted Trees} \\
\hline
\multirow[t]{2}{*}{Localized \(\begin{gathered}\text { Feasibl } \\ \text { Oracle }\end{gathered}\)} & 2.70 & 2.92 & 0.51 & 2.96 & 21.28 & 94.43 & 12.61 & 0.87 & 27.90 & 29.84 & -4.04 & 30.11 & 21.04 & 94.93 & 130.07 & 13.53 \\
\hline
 & 2.66 & 2.87 & 0.52 & 2.92 & 21.29 & 94.23 & 12.41 & 2.37 & 27.41 & 29.31 & -4.13 & 29.59 & 21.03 & 94.98 & 127.77 & 15.06 \\
\hline
\multirow[t]{2}{*}{Global \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.67 & 2.88 & 0.54 & 2.93 & 21.30 & 94.34 & 12.48 & 1.88 & 27.69 & 29.67 & -3.76 & 29.90 & 21.05 & 94.84 & 129.08 & 14.18 \\
\hline
 & 2.66 & 2.87 & 0.54 & 2.92 & 21.29 & 94.20 & 12.42 & 2.30 & 27.40 & 29.25 & -3.75 & 29.48 & 21.03 & 94.97 & 127.75 & 15.07 \\
\hline
\multicolumn{17}{|l|}{Post-lasso} \\
\hline
\multirow[t]{2}{*}{Localized \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.69 & 2.91 & 0.52 & 2.96 & 21.31 & 94.57 & 12.58 & 1.08 & 28.12 & 30.16 & -4.26 & 30.46 & 21.00 & 94.79 & 131.06 & 12.87 \\
\hline
 & 2.66 & 2.87 & 0.58 & 2.93 & 21.30 & 94.28 & 12.43 & 2.29 & 27.88 & 29.79 & -4.21 & 30.09 & 21.01 & 94.82 & 129.95 & 13.61 \\
\hline
\multirow[t]{2}{*}{Global \(\begin{aligned} & \text { Feasible } \\ & \text { Oracle }\end{aligned}\)} & 2.68 & 2.89 & 0.54 & 2.93 & 21.30 & 94.41 & 12.49 & 1.76 & 28.06 & 29.99 & -4.14 & 30.27 & 21.03 & 94.80 & 130.83 & 13.02 \\
\hline
 & 2.66 & 2.87 & 0.58 & 2.93 & 21.30 & 94.20 & 12.43 & 2.24 & 27.93 & 29.81 & -3.75 & 30.04 & 21.04 & 94.94 & 130.22 & 13.43 \\
\hline
\multicolumn{17}{|l|}{Flexible} \\
\hline
Feasile & 0.56 & 2.88 & 0.56 & 2.93 & 21.29 & 94.25 & 12.44 & 2.16 & 27.44 & 29.39 & -3.76 & 29.63 & 21.02 & 94.92 & 127.91 & 14.96 \\
\hline
Oracle & 2.66 & 2.87 & 0.54 & 2.92 & 21.29 & 94.24 & 12.41 & 2.42 & 26.89 & 28.77 & -3.53 & 28.99 & 21.02 & 94.88 & 125.34 & 16.67 \\
\hline
\end{tabular}
\end{center}

Notes: Results are based on 10,000 Monte Carlo draws. The left panel shows results for the original outcome and the right panel for age as outcome based on Londoño-Vélez et al. (2020) and a sample size of \(n=5000\) (see Section 7 for details). The bandwidth is chosen and the confidence sets are constructed based on robust bias correction. The columns show the simulated mean standard error (Mean SE), standard deviation (SD); simulated bias (Bias); root mean squared error (RMSE); average bandwidth (Mean Bandwidth), coverage of confidence intervals with 95\% nominal level (CI Cov); the average confidence interval length (Mean CI Length); and the reduction in mean CI length relative to the no covariates CI length (Mean CI Length \% Red.). The estimators are described in Section 3.\\
are very close to each other especially compared to the distribution of no covariates RD estimates. They even become more similar when the sample size increases.\\
B.3. Additional Results for Simulation II. Figure S 5 extends the results presented in Figures 2 and 3. It presents further simulation results for bias-aware inference for age as the dependent variable and for robust bias correction for both the original outcome and age as the dependent variables. The qualitative conclusions about the bias, the standard error, and the validity of confidence intervals are very similar to the ones discussed in Section 7.3. In all cases, the simulated bias is insensitive to including many covariates, the inference based on the cross-fitted methods is valid, while the conventional linear adjustment leads to severely downward-biased standard errors and invalid confidence intervals as the number of covariates increases. The patterns in the standard deviations are different for the original outcome and age because of the different explanatory power of the covariates for these two dependent variables.\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-55(1)}

Figure S3: Difference between cross-fitted feasible and oracle estimators for \(n=2000\).\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-55}

Figure S4: Difference between cross-fitted feasible and oracle estimators for \(n=5000\).\\
Notes: In each figure, the first box plot shows the distribution of the no covariates RD estimator and the other ones the difference of the cross-fitted feasible covariate-adjusted RD estimates and their respective oracle counterpart based on the respective adjustment methods. Simulations are based on Londoño-Vélez et al. (2020) and age is the dependent variable. See details for a description of the estimators in Section 3 for details. Results are based on 10,000 Monte Carlo draws.\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(3)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(5)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(6)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(4)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(7)}\\
No covariates Conventional Linear \(\triangle\) Localized Linear \(\leqslant\) Localized Random Forest\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(1)}\\
\includegraphics[max width=\textwidth, center]{2025_01_31_485d36515c76dfd75a4dg-56(2)}\\
\includegraphics[max width=\textwidth]{2025_01_31_485d36515c76dfd75a4dg-56} mean standard error of the respective estimator relative to its standard deviation. The last graph shows the simulated coverage of the confidence interval with \(95 \%\) nominal level. See Section 3.2 and 3.3 for details of the estimators. Results are based on 10,000 Monte Carlo draws.

\section*{REFERENCES}
Akhtari, M., D. Moreira, and L. Trucco (2022): "Political turnover, bureaucratic turnover, and the quality of public services," American Economic Review, 112, 442-493.

Altindag, O., B. Erten, and P. Keskin (2022): "Mental health costs of lockdowns: Evidence from age-specific curfews in Turkey," American Economic Journal: Applied Economics, 14, 320-343.

Ambrus, A., E. Field, and R. Gonzalez (2020): "Loss in the time of cholera: Long-run impact of a disease epidemic on the urban landscape," American Economic Review, 110, 475-525.

Asher, S. and P. Novosad (2020): "Rural roads and local economic development," American economic review, 110, 797-823.

Avis, E., C. Ferraz, F. Finan, and C. Varjão (2022): "Money and politics: The effects of campaign spending limits on political entry and competition," American Economic Journal: Applied Economics, 14, 167-199.

Baskaran, T. and Z. Hessami (2018): "Does the election of a female leader clear the way for more women in politics?" American Economic Journal: Economic Policy, 10, 95-121.

Becker, S. O., I. Grosfeld, P. Grosjean, N. Voigtländer, and E. Zhuravskaya (2020): "Forced migration and human capital: Evidence from post-WWII population transfers," American Economic Review, 110, 1430-1463.

Calonico, S., M. D. Cattaneo, and R. Titiunik (2014): "Robust nonparametric confidence intervals for regression-discontinuity designs," Econometrica, 82, 2295-2326.

Chin, M. (2023): "When do politicians appeal broadly? The economic consequences of electoral rules in Brazil," American Economic Journal: Applied Economics, 15, 183-209.

Curto-Grau, M., A. Solé-Ollé, and P. Sorribas-Navarro (2018): "Does electoral competition curb party favoritism?" American Economic Journal: Applied Economics, 10, 378-407.

Del Valle, A., A. de Janvry, and E. Sadoulet (2020): "Rules for recovery: Impact of indexed disaster funds on shock coping in Mexico," American Economic Journal: Applied Economics, 12, 164-195.

Granzier, R., V. Pons, and C. Tricaud (2023): "Coordination and bandwagon effects: How past rankings shape the behavior of voters and candidates," American Economic Journal: Applied Economics, 15, 177-217.

Greenstone, M., G. He, R. Jia, and T. Liu (2022): "Can technology solve the principal-agent problem? Evidence from China's war on air pollution," American Economic Review: Insights, 4, 54-70.

Johnson, M. S. (2020): "Regulation by shaming: Deterrence effects of publicizing violations of workplace safety and health laws," American economic review, 110, 1866-1904.

Londoño-Vélez, J., C. Rodríguez, and F. Sánchez (2020): "Upstream and downstream impacts of college merit-based financial aid for low-income students: Ser Pilo Paga in Colombia," American Economic Journal: Economic Policy, 12, 193-227.

Tuttle, C. (2019): "Snapping Back: Food Stamp Bans and Criminal Recidivism," American Economic Journal: Economic Policy, 11, 301-27.

Wasserman, M. (2021): "Up the political ladder: Gender parity in the effects of electoral defeats," AEA Papers and Proceedings, 111, 169-173.


\end{document}